{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/EthanHsu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000\n",
    "#to download json_lines: python -m pip install json-lines\n",
    "#pip install textstat\n",
    "#pip install --upgrade gensim\n",
    "from pathlib import Path\n",
    "import json_lines, json, copy\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from nltk.util import ngrams \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from gensim import corpora\n",
    "\n",
    "import csv\n",
    "from textstat.textstat import textstat\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "#TODO combine lists into csv\n",
    "\n",
    "#toy example\n",
    "#rows = zip(list1,list2,list3,list4,list5)\n",
    "#with open(newfilePath, \"w\") as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    for row in rows:\n",
    "#        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lg = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_core_lg = nlp_en = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assorted cleaning\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "def stem_tokens(tokens, ps):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(ps.stem(item))\n",
    "    return stemmed\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lemmatize_tokens(tokens, wnl):\n",
    "    lemmatized = []\n",
    "    for item in tokens:\n",
    "        lemmatized.append(wnl.lemmatize(item))\n",
    "    return lemmatized\n",
    "wnl = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_instance_filePath = Path(\"../clickbait_dataset/instances_train.jsonl\")\n",
    "raw_instance_dir = Path(\"../clickbait_dataset\")\n",
    "raw_truth_filePath = Path(\"../clickbait_dataset/truth_train.jsonl\")\n",
    "raw_truth_dir = Path(\"../clickbait_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'postMedia': [],\n",
       " 'postText': [\"Apple's iOS 9 'App thinning' feature will give your phone's storage a boost\"],\n",
       " 'postTimestamp': 'Tue Jun 09 16:31:10 +0000 2015',\n",
       " 'targetCaptions': [\"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs to run on individual handsets. It 'slices' the app into 'app variants' that only need to access the specific files on that specific device\",\n",
       "  \"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs to run on individual handsets. It 'slices' the app into 'app variants' that only need to access the specific files on that specific device\",\n",
       "  \"The guidelines also discuss so-called 'on-demand resources.' This allows developers to omit features from an app until they are opened or requested by the user. The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0This will also increase how quickly an app downloads\",\n",
       "  \"The guidelines also discuss so-called 'on-demand resources.' This allows developers to omit features from an app until they are opened or requested by the user. The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0This will also increase how quickly an app downloads\",\n",
       "  \"Apple said it will then 'purge on-demand resources when they are no longer needed and disk space is low' (Apple's storage menu is pictured)\",\n",
       "  \"Apple said it will then 'purge on-demand resources when they are no longer needed and disk space is low' (Apple's storage menu is pictured)\",\n",
       "  'A 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included. A drop of 8GB, leaving 87.5 % of storage free.\\xa0Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54% and 79% of free space (illustrated)',\n",
       "  'A 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included. A drop of 8GB, leaving 87.5 % of storage free.\\xa0Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54% and 79% of free space (illustrated)',\n",
       "  \"Earlier this year, a pair of disgruntled Apple users filed a lawsuit in Miami accusing the tech giant of 'concealing, omitting and failing to disclose' that on 16GB versions of iPhones, more than 20% of the advertised space isn't available. This graph reveals the capacity available and unavailable to the user\",\n",
       "  \"Earlier this year, a pair of disgruntled Apple users filed a lawsuit in Miami accusing the tech giant of 'concealing, omitting and failing to disclose' that on 16GB versions of iPhones, more than 20% of the advertised space isn't available. This graph reveals the capacity available and unavailable to the user\"],\n",
       " 'targetDescription': \"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space by 'slicing' it to work on individual handsets (illustrated).\",\n",
       " 'targetKeywords': 'Apple,gives,gigabytes,iOS,9,app,thinning,feature,finally,phone,s,storage,boost',\n",
       " 'targetParagraphs': [\"Paying for a 64GB phone only to discover that this is significantly reduced by system files and bloatware is the bane of many smartphone owner's lives.\\xa0\",\n",
       "  'And the issue became so serious earlier this year that some Apple users even sued the company over it.\\xa0',\n",
       "  \"But with the launch of iOS 9, Apple is hoping to address storage concerns by introducing a feature known as 'app thinning.'\",\n",
       "  'It has been explained on the watchOS Developer Library site and is aimed at developers looking to optimise their apps to work on iOS and the watchOS.\\xa0',\n",
       "  'It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs run on the particular handset it is being installed onto.',\n",
       "  \"It 'slices' the app into 'app variants' that only need to access the specific files on that specific handset.\\xa0\",\n",
       "  \"XperiaBlog recently spotted that the 8GB version of Sony's mid-range M4 Aqua has just 1.26GB of space for users.\\xa0\",\n",
       "  'This means that firmware, pre-installed apps and Android software take up a staggering 84.25 per cent.\\xa0',\n",
       "  \"Sony does let users increase storage space using a microSD card, but as XperiaBlog explained: 'Sony should never have launched an 8GB version of the Xperia M4 Aqua.\\xa0\",\n",
       "  \"'If you are thinking about purchasing this model, be aware of what you are buying into.'\",\n",
       "  \"Previously, apps would need to be able to run on all handsets and account for the varying files, chipsets and power so contained sections that weren't always relevant to the phone it was being installed on.\",\n",
       "  'This made them larger than they needed to be.\\xa0',\n",
       "  'Under the new plans, when a phone is downloaded from the App Store, the app recognises which phone it is being installed onto and only pulls in the files and code it needs to work on that particular device.\\xa0',\n",
       "  'For iOS, sliced apps are supported on the latest iTunes and on devices running iOS 9.0 and later.\\xa0',\n",
       "  \"In all other cases, the App Store will deliver the previous 'universal apps' to customers.\",\n",
       "  \"The guidelines also discuss so-called 'on-demand resources.'\\xa0This allows developers to omit features from an app until they are opened or requested by the user.\\xa0\",\n",
       "  'The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0',\n",
       "  'This will also increase how quickly an app downloads.\\xa0',\n",
       "  'An example given by Apple is a game app that may divide resources into game levels and request the next level of resources only when the app anticipates the user has completed the previous level.',\n",
       "  'Similarly, the app can request In-App Purchase resources only when the user buys a corresponding in-app purchase.',\n",
       "  \"Apple explained the operating system will then 'purge on-demand resources when they are no longer needed and disk space is low', removing them until they are needed again.\",\n",
       "  'And the whole iOS 9 software has been designed to be thinner during updates, namely from 4.6GB to 1.3GB, to free up space.\\xa0',\n",
       "  'This app thinning applies to third-party apps created by developers.\\xa0',\n",
       "  \"Apple doesn't say if it will apply to the apps Apple pre-installed on devices, such as Stocks, Weather and Safari - but it is likely that it will in order to make iOS 9 smaller.\\xa0\",\n",
       "  'As an example of storage space on Apple devices, a 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included.\\xa0',\n",
       "  'A drop of 8GB, leaving 87.5 per cent of storage free.\\xa0',\n",
       "  \"By comparison, Samsung's 64GB S6 Edge has 53.42GB of available space, and of this 9GB is listed as system memory.\\xa0\",\n",
       "  'Although this is a total drop of almost 11GB, it equates to 83 per cent of space free.\\xa0',\n",
       "  'By comparison, on a 32GB S6 MailOnline found 23.86GB of space was available, with 6.62GB attributed to system memory.',\n",
       "  'This is a drop of just over 8GB and leaves 75 per cent free.',\n",
       "  'Samsung said it, too, had addressed complaints about bloatware and storage space with its S6 range. \\xa0',\n",
       "  'Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54 per cent and 79 per cent of free space.',\n",
       "  '\\xa0',\n",
       "  \"Businessman 'killed his best friend when he crashed jet-powered dinghy into his £1million yacht while showing off' as his wife filmed them\"],\n",
       " 'targetTitle': \"Apple gives back gigabytes: iOS 9 'app thinning' feature will finally give your phone's storage a boost\"}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open with json - raw_instance_train\n",
    "raw_instance_train=[]\n",
    "with open('../clickbait_dataset/instances_train.jsonl','rb') as raw_instance_file: #opening file with binary(rb) mode.\n",
    "    for x in raw_instance_file:\n",
    "        raw_instance_train.append(json.loads(x))\n",
    "raw_instance_train[0]\n",
    "\n",
    "### Fields in instances.jsonl:\n",
    "##{\n",
    "#\"id\" : \"<instance id>\",\n",
    "#\"postTimestamp\" : \"<weekday> <month> <day> <hour>:<minute>:<second><time_offset> <year>\",\n",
    "#\"postText\" : [\"<text of the tweet post with links removed>\"],\n",
    "#\"postMedia\" : [\"<path to a file in the media archive>\"],\n",
    "#\"targetTitle\" : \"<title of target news article>\",\n",
    "#\"targetDescription\": \"<description tag of target news article>\",\n",
    "#\"targetKeywords\" : \"<keywords tag of target news article>\",\n",
    "#\"targetParagraphs\" : [\"<text of the i-th paragraph in the target article>\"],\n",
    "#\"targetCaptions\" : [\"<caption of the i-th image in the target article>\"]}\n",
    "### Fields in truth.jsonl:\n",
    "#{\n",
    "#\"id\" : \"<instance id>\",\n",
    "#\"truthJudgments\" : [<number in [0,1]>],\n",
    "#\"truthMean\" : <number in [0,1]>,\n",
    "#\"truthMedian\" : <number in [0,1]>,\n",
    "#\"truthMode\" : <number in [0,1]>,\n",
    "#truthClass\" : \"clickbait | no-clickbait\"\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17581\n"
     ]
    }
   ],
   "source": [
    "_instance_train = []\n",
    "print(len(raw_instance_train))\n",
    "for id_, instance in enumerate(raw_instance_train):\n",
    "    if instance['targetParagraphs'] != []:\n",
    "        _instance_train.append(instance)\n",
    "raw_instance_train = _instance_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7',\n",
       " 'truthClass': 'no-clickbait',\n",
       " 'truthJudgments': [0.33333334, 0.0, 0.33333334, 0.33333334, 0.0],\n",
       " 'truthMean': 0.2,\n",
       " 'truthMedian': 0.33333334,\n",
       " 'truthMode': 0.33333334}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open with json - raw_truth_train\n",
    "\n",
    "raw_truth_train=[]\n",
    "with open('../clickbait_dataset/truth_train.jsonl','rb') as raw_truth_file: #opening file with binary(rb) mode.\n",
    "    for x in raw_truth_file:\n",
    "        raw_truth_train.append(json.loads(x))\n",
    "raw_truth_train[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "17581\n",
      "Check Dict. Indexes for instance_train: dict_keys(['targetCaptions', 'targetDescription', 'postText', 'postTimestamp', 'postMedia', 'targetParagraphs', 'targetTitle', 'id', 'targetKeywords'])\n",
      "Check Dict. Indexes for truth_train: dict_keys(['truthMedian', 'truthMean', 'truthMode', 'truthClass', 'truthJudgments', 'id'])\n"
     ]
    }
   ],
   "source": [
    "#check the properties\n",
    "print(type(raw_instance_train))    #type for whole data: list\n",
    "print(type(raw_instance_train[0]))  #type for each element of the liast: dict\n",
    "print(len(raw_instance_train))\n",
    "print('Check Dict. Indexes for instance_train: ' + str(raw_instance_train[0].keys()))\n",
    "print('Check Dict. Indexes for truth_train: ' + str(raw_truth_train[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_instance_train = [json.loads(line) for line in open(\"../clickbait_dataset/instances_train.jsonl\")]\n",
    "raw_truth_train = [json.loads(line) for line in open(\"../clickbait_dataset/truth_train.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separate into two list (clickbait/ non_clickbait)\n",
    "clickbait_train = []\n",
    "non_clickbait_train = []\n",
    "for x in range(0, len(raw_instance_train)):\n",
    "    if raw_truth_train[x]['truthClass'] == 'clickbait':\n",
    "        temp_dict = copy.deepcopy(raw_instance_train[x])\n",
    "        temp_dict.update(raw_truth_train[x])\n",
    "        clickbait_train.append(temp_dict)\n",
    "    else:\n",
    "        raw_instance_train[x].update(raw_truth_train[x])\n",
    "        non_clickbait_train.append(raw_instance_train[x])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'postMedia': [],\n",
       " 'postText': ['U.S. Soccer should start answering tough questions about Hope Solo, @eric_adelson writes.'],\n",
       " 'postTimestamp': 'Fri Jun 12 23:36:05 +0000 2015',\n",
       " 'targetCaptions': ['US to vote for Ali in FIFA election and not Blatter',\n",
       "  'US to vote for Ali in FIFA election and not Blatter',\n",
       "  \"FILE - This Oct. 10, 2014, file photo shows Sunil Gulati, president of the United States Soccer Federation, during a press conference in Bristol, Conn. The United States says it will vote for Jordan's Prince Ali bin Al-Hussein for FIFA president Friday, May 29, 2015 and not for incumbent Sepp Blatter. (AP Photo/Elise Amendola, File)\"],\n",
       " 'targetDescription': \"A U.S. Senator's scathing letter questioned U.S. Soccer's inadequate handling of Solo's domestic violence charges. It's time for Sunil Gulati to respond.\",\n",
       " 'targetKeywords': '',\n",
       " 'targetParagraphs': [\"WINNIPEG, Manitoba – The bubble U.S. Soccer is putting around Hope Solo isn't working to calm anyone's concerns about the star goalkeeper.\",\n",
       "  \"The latest lament comes from no less than a U.S. Senator, who into Solo's domestic violence incident of last year and offer a detailed explanation of why Solo is on the field. She is expected to be the starting goalkeeper when the USA plays Sweden in its second group game at the Women's World Cup on Friday.\",\n",
       "  '[FC Yahoo: ]',\n",
       "  'U.S. Senator Richard Blumenthal of Connecticut penned a lengthy complaint about the near-silence the organization has given on Solo, especially in the wake of ESPN\\'s \"Outside the Lines\" report on Sunday. Blumenthal wrote that if the report is accurate \"U.S. Soccer\\'s approach to domestic violence and family violence is at best superficial and at worst dangerously neglectful and self-serving.\"',\n",
       "  'This situation is well beyond Solo now. U.S. Soccer has made this a referendum about its own ability to represent the values of the nation. \"As boys and girls tune into Friday\\'s game, watching the women on TV as role models,\" Blumenthal wrote, \"what is the message of starting Hope Solo at goal?\"',\n",
       "  \"[Women's World Cup: | | | ]\",\n",
       "  \"U.S. Soccer is not only avoiding difficult questions, it is also avoiding an account of all its actions. Even NFL commissioner Roger Goodell has, to some extent, owned up to his failures on the Ray Rice case, yet Gulati has not even decried Solo's poor decisions.\",\n",
       "  \"Last September, three months after Solo's domestic violence charges (which were later ), Gulati released this vague statement on the matter:\",\n",
       "  '\"U.S. Soccer takes the issue of domestic violence very seriously. From the beginning, we considered the information available and have taken a deliberate and thoughtful approach regarding Hope Solo\\'s status with the national team. Based on that information, U.S. Soccer stands by our decision to allow her to participate with the team as the legal process unfolds. If new information becomes available we will carefully consider it.\"',\n",
       "  \"A lot of this would be solved if Gulati and Solo held a press conference and claimed some accountability. It's clear from Monday's dominant performance in a 3-1 tournament-opening win over Australia that Solo is not distracted by the national discussion of her past, so a short appearance – even without reporters' questions – probably won't ruin the U.S.'s chances for a trophy. And claiming that Solo has a match to focus on isn't credible as it's basically an admission that a single game is more important than a discussion of domestic violence.\",\n",
       "  \"For Gulati, there is little excuse. The silence, the lack of punishment and then the decision to allow head coach Jill Ellis to discuss (or not discuss) the situation here, combines to make the top official of American soccer look like he doesn't prioritize this issue.\",\n",
       "  '\"In the wake of this violent incident, U.S. Soccer offered no comment publicly for three months,\" Blumenthal wrote. \"It finally issued a statement that was purportedly the result of a \\'deliberate and thoughtful approach\\' to consider the incident and determine Hope Solo\\'s status with the team, but it neglected to include an effort to contact the alleged victims.\"',\n",
       "  'The more U.S. Soccer tries to shift focus to the field, the less it accomplishes that. This is the Super Bowl of women\\'s soccer, and decrying this as \"old news\" doesn\\'t work because the entire country is watching now. Countless Americans are debating whether to root for Solo or not, and her protectors are effectively convincing a lot of people to remain skeptical of her.',\n",
       "  \"It doesn't have to be this way. A better explanation of what Gulati has done on this topic – and a better explanation of what Solo has done to work on her problems – would go a long way toward moving on, especially the way U.S. Soccer clearly wants. Instead, there is opacity where there should be transparency.\",\n",
       "  'The NFL has come under a lot of scrutiny for its efforts to \"protect the shield,\" but U.S. Soccer\\'s shield stands for a lot more than just a sport. That shield shouldn\\'t only be used to defend a player.',\n",
       "  '735'],\n",
       " 'targetTitle': 'U.S. Soccer should start answering tough questions about Hope Solo',\n",
       " 'truthClass': 'clickbait',\n",
       " 'truthJudgments': [0.33333334, 0.6666667, 1.0, 0.0, 0.6666667],\n",
       " 'truthMean': 0.53333336,\n",
       " 'truthMedian': 0.6666667,\n",
       " 'truthMode': 0.6666667}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clickbait_train\n",
    "clickbait_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13',\n",
       " 'postMedia': ['media/609998126737395712.jpg'],\n",
       " 'postText': ['Britain forced to withdraw spies after Russia and China access files stolen by Snowden'],\n",
       " 'postTimestamp': 'Sun Jun 14 08:17:41 +0000 2015',\n",
       " 'targetCaptions': ['GCHQ-AFPGetty.jpg',\n",
       "  'GettyImages-74018446.jpg',\n",
       "  '4-Edward-Snowden-AP.jpg',\n",
       "  'GettyImages-2428443.jpg'],\n",
       " 'targetDescription': 'The news that Russia and China have gained access to secret information in files leaked by whistleblower Edward Snowden has been called a \"huge catastrophe\" by a former CIA field officer.',\n",
       " 'targetKeywords': ', Home News, UK, News',\n",
       " 'targetParagraphs': [\"UK security chiefs have warned that the former NSA contractor's actions have compromised anti-terror activity\",\n",
       "  'The news that Russia and China have gained access to secret information in files leaked by whistleblower Edward Snowden has been called a \"huge catastrophe\" by a former CIA field officer.',\n",
       "  'Speaking on BBC Radio 5 Live this morning, Robert Baer, an author and former CIA officer, said the apparent discovery of secret files that has led to British intelligence agents being withdrawn from operations is \"the worst espionage setback the west has suffered since forever.\"',\n",
       "  'Baer warned that foreign countries could have cracked US and UK encryption codes, which would mean that they could gain access to secret communications from the military and intelligence services.',\n",
       "  'Speaking about the impact that the reported withdrawal of intelligence agents might have on international relations, he said: \"Russia gets more hostile by the day, and if we\\'re closing stations and pulling people back, I can\\'t tell you where this is going to go... I can\\'t tell you how bad this is.\"',\n",
       "  'He also criticsed the NSA for allowing Snowden to leak the documents in the first place.',\n",
       "  '\"[What Snowden did] was impossible when I worked for the government, it was absolutely impossible to compromise this much stuff. And I don\\'t think he knows himself how much damage he has done.\"',\n",
       "  'Chalking the leak up partly to \"the sheer incompetence of the NSA,\" he said \"I just can\\'t believe nobody\\'s been fired.\"',\n",
       "  'Baer worked for the CIA for 21 years, mostly assigned in the Middle East. Some say he was the best field officer in the region during the years he was active. Since he left the CIA, he has become an author and commentator, writing books based on his experiences and knowledge of working in the CIA.',\n",
       "  'Citing senior government officials, the Sunday Times reported today that secret information in the leaked files has finally been \"cracked\" by other countries, despite Snowden\\'s assurance that the most crucial data in his leaks could not be accessed by foreign powers.',\n",
       "  'In what was described by former GCHQ director Sir David Omand as a “huge strategic setback” for the West, it was reported that information that exposed intelligence-gathering techniques and identified individual spies has been revealed.',\n",
       "  'A Downing Street source told the Sunday Times: “It is the case that Russians and Chinese have information. It has meant agents have had to be moved and that knowledge of how we operate has stopped us getting vital information.”',\n",
       "  'However, despite a senior government official was quoted by the paper as saying that Snowden had \"blood on his hands\", Downing Street confirmed that there was “no evidence of anyone being harmed” as a result of his leaks.',\n",
       "  \"The newspaper quoted a senior Home Office source as saying: “Putin didn't give him asylum for nothing. His documents were encrypted but they weren't completely secure and we have now seen our agents and assets being targeted.”\",\n",
       "  'A British intelligence source was quoted as saying: “We know Russia and China have access to Snowden\\'s material and will be going through it for years to come, searching for clues to identify potential targets.\"',\n",
       "  '“Snowden has done incalculable damage. In some cases the agencies have been forced to intervene and lift their agents from operations to prevent them from being identified and killed.”',\n",
       "  \"However, Glenn Greenwald, a journalist who Snowden first approached with the secret documents, criticised the legitimacy of the government's claims.\",\n",
       "  'Writing on Twitter, he said: \"If you\\'re someone who believes anonymously voiced self-serving government claims, you\\'re dumb. If you\\'re a journalist who prints it, you\\'re worse.\"',\n",
       "  \"If you're someone who believes anonymously voiced self-serving govt claims, you're dumb. If you're a journalist who prints it, you're worse\",\n",
       "  'Greenwald has also published a lengthy post on The Intercept, in which he says the government\\'s claims that anything has been \"cracked\" by the Chinese of Russians is a lie. He also criticises the Sunday Times for reporting the story.',\n",
       "  'In 2013, Snowden leaked tens of thousands of documents to about NSA and GCHQ intelligence-gathering techniques.',\n",
       "  \"He fled to Hong Kong where he met journalists to co-ordinate a series of articles that exposed mass surveillance programmes such as the NSA's Prism and GCHQ's Tempora, which involve “hoovering up” vast volumes of private communications.\",\n",
       "  'He the intended to fly to Ecuador to claim asylum, but his route involved stopping at Moscow Sheremetyevo airport. Once he landed, his passport was cancelled by the USA, leaving him stranded in Russia.',\n",
       "  'He has received asylum in Russia, and currently lives in Moscow. He remains wanted by the US government.'],\n",
       " 'targetTitle': \"'Huge catastrophe' as government claims Britain is forced to withdraw spies after Russia and China access files stolen by US whistleblower Edward Snowden\",\n",
       " 'truthClass': 'no-clickbait',\n",
       " 'truthJudgments': [0.0, 0.6666667, 0.33333334, 0.0, 0.33333334],\n",
       " 'truthMean': 0.26666668,\n",
       " 'truthMedian': 0.33333334,\n",
       " 'truthMode': 0.0}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non_clickbait_train\n",
    "non_clickbait_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Post Text tokenizing and word count manually\n",
    "postTextTokens = []\n",
    "postTextTokensNoPun = []\n",
    "postTextCountTokens = []\n",
    "\n",
    "for x in range(0, len(raw_instance_train)):\n",
    "    tokens = word_tokenize(raw_instance_train[x]['postText'][0])\n",
    "    postTextCountTokens.append([x for x in tokens if x != []])\n",
    "    postTextTokensNoPun.append([c for c in tokens if c not in punctuation])\n",
    "#print(postTextTokens[0])\n",
    "#print(len(postTextTokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_parser(document):\n",
    "    #print(document)\n",
    "    #print(porter_stemmer.stem(document))\n",
    "    #print(stem_doc)\n",
    "    tokens = word_tokenize(document)\n",
    "    non_empty_tokens = [x for x in tokens if x != []]\n",
    "    clean_tokens = [c for c in non_empty_tokens if c not in punctuation and stop_words]\n",
    "    #non_apos_tokens = [c.replace(\"'\", \"\") for c in non_empty_tokens]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_punc_parser(document):\n",
    "    #print(document)\n",
    "    #print(porter_stemmer.stem(document))\n",
    "    #print(stem_doc)\n",
    "    tokens = word_tokenize(document)\n",
    "    non_empty_tokens = [x for x in tokens if x != []]\n",
    "    #clean_tokens = [c for c in non_empty_tokens if c not in punctuation and stop_words]\n",
    "    #non_apos_tokens = [c.replace(\"'\", \"\") for c in non_empty_tokens]\n",
    "    return non_empty_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "def documentParser(document):  \n",
    "    #print(punctuation)\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    tokens = [word for word in document.lower().split() if word not in stoplist and punctuation]\n",
    "    #print(tokens)\n",
    "    #stem_tokens = [porter_stemmer.stem(a) for a in tokens]\n",
    "    #print(\"stem\",stem_tokens)\n",
    "    non_apos_tokens = [c.replace(\"'\", \"\").replace(\":\", \"\").replace(\".\", \"\").replace(\",\", \"\") for c in tokens]\n",
    "    return non_apos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def sklearn_TfidfVectorizer(document):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    #print(X.shape)\n",
    "    return vectorizer.get_feature_names()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_Tokenizer(document):\n",
    "    tknzr = TweetTokenizer()\n",
    "    #tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    return [c for c in tknzr.tokenize(document) if c not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Apple's iOS 9 'App thinning' feature will give your phone's storage a boost\"]\n",
      "---------------------\n",
      "postTextCountTokens:  ['Apple', \"'s\", 'iOS', '9', \"'App\", 'thinning', \"'\", 'feature', 'will', 'give', 'your', 'phone', \"'s\", 'storage', 'a', 'boost'] \n",
      "\n",
      "postTextTokensNoPun:  ['Apple', \"'s\", 'iOS', '9', \"'App\", 'thinning', 'feature', 'will', 'give', 'your', 'phone', \"'s\", 'storage', 'a', 'boost'] \n",
      "\n",
      "doc_parser():  ['Apple', \"'s\", 'iOS', '9', \"'App\", 'thinning', 'feature', 'will', 'give', 'your', 'phone', \"'s\", 'storage', 'a', 'boost'] \n",
      "\n",
      "doc_punc_parser():  ['Apple', \"'s\", 'iOS', '9', \"'App\", 'thinning', \"'\", 'feature', 'will', 'give', 'your', 'phone', \"'s\", 'storage', 'a', 'boost'] \n",
      "\n",
      "documentParser():  ['apples', 'ios', '9', 'app', 'thinning', 'feature', 'will', 'give', 'your', 'phones', 'storage', 'boost'] \n",
      "\n",
      "sklearn_TfidfVectorizer():  ['app', 'apple', 'boost', 'feature', 'give', 'ios', 'phone', 'storage', 'thinning', 'will', 'your'] \n",
      "\n",
      "Tweet_Tokenizer():  [\"Apple's\", 'iOS', '9', 'App', 'thinning', 'feature', 'will', 'give', 'your', \"phone's\", 'storage', 'a', 'boost'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_instance_train[0]['postText'])\n",
    "print(\"---------------------\")\n",
    "print(\"postTextCountTokens: \", postTextCountTokens[0], \"\\n\")\n",
    "print(\"postTextTokensNoPun: \", postTextTokensNoPun[0], \"\\n\")\n",
    "print(\"doc_parser(): \",doc_parser(raw_instance_train[0]['postText'][0]), \"\\n\")\n",
    "print(\"doc_punc_parser(): \", doc_punc_parser(raw_instance_train[0]['postText'][0]), \"\\n\")\n",
    "print(\"documentParser(): \", documentParser(raw_instance_train[0]['postText'][0]), \"\\n\")\n",
    "print(\"sklearn_TfidfVectorizer(): \",sklearn_TfidfVectorizer(raw_instance_train[0]['postText']), \"\\n\")\n",
    "print(\"Tweet_Tokenizer(): \", tweet_Tokenizer(raw_instance_train[0]['postText'][0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple gives back gigabytes: iOS 9 'app thinning' feature will finally give your phone's storage a boost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'gives',\n",
       " 'back',\n",
       " 'gigabytes',\n",
       " 'ios',\n",
       " '9',\n",
       " 'app',\n",
       " 'thinning',\n",
       " 'feature',\n",
       " 'will',\n",
       " 'finally',\n",
       " 'give',\n",
       " 'your',\n",
       " 'phones',\n",
       " 'storage',\n",
       " 'boost']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_instance_train[0]['targetTitle'])\n",
    "documentParser(raw_instance_train[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "countLongestWord = []\n",
    "\n",
    "def count_longest_word(word_list):  \n",
    "    longest_word =  max(word_list , key=len)\n",
    "    return len(longest_word)\n",
    "\n",
    "for x in range(0, 1751):\n",
    "    countLongestWord.append(count_longest_word(postTextTokensNoPun[x]))\n",
    "\n",
    "print(max(countLongestWord))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fury', 'as', 'men', 'throw', 'a', 'cat', 'into', 'a', 'crocodile-infested', 'pond']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(postTextTokensNoPun[915])\n",
    "count_longest_word(postTextTokensNoPun[915])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lexicon (Word) Count\n",
    "targetTitleCountWords = []\n",
    "for x in range(0, len(raw_instance_train)):\n",
    "    targetTitleCountWords.append(textstat.lexicon_count(raw_instance_train[x]['targetTitle'][0], removepunct=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grade Level reading difficulty\n",
    "#Coleman-Liau index, Gunning FOG Formula, Linsear Write Formula\n",
    "targetTitle_coleman_liau_index = []\n",
    "targetTitle_gunning_fog = []\n",
    "targetTitle_linsear_write_formula = []\n",
    "for x in range(0, len(raw_instance_train)):\n",
    "    targetTitle_coleman_liau_index.append(textstat.coleman_liau_index(raw_instance_train[x]['targetTitle']))\n",
    "    targetTitle_gunning_fog.append(textstat.gunning_fog(raw_instance_train[x]['targetTitle']))\n",
    "    targetTitle_linsear_write_formula.append(textstat.linsear_write_formula(raw_instance_train[x]['targetTitle']))\n",
    "targetTitle_coleman_liau_index = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple gives back gigabytes: iOS 9 'app thinning' feature will finally give your phone's storage a boost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.12"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_instance_train[0]['targetTitle'])\n",
    "textstat.coleman_liau_index(raw_instance_train[0]['targetTitle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple gives back gigabytes: iOS 9 'app thinning' feature will finally give your phone's storage a boost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.564705882352943"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_instance_train[0]['targetTitle'])\n",
    "textstat.gunning_fog(raw_instance_train[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple gives back gigabytes: iOS 9 'app thinning' feature will finally give your phone's storage a boost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_instance_train[0]['targetTitle'])\n",
    "textstat.linsear_write_formula(raw_instance_train[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_sim(title, body):\n",
    "    title = api(title)\n",
    "    return np.mean(map(lambda x: title.similarity(api(x)), body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating similarity between titles and body.\n",
    "import spacy\n",
    "#nlp = spacy.load('../GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "def avg_doc_sim(nlp, title, body):\n",
    "#    similarity = word_vectors.n_similarity(title, body)\n",
    "#    return \"{:.4f}\".format(similarity)\n",
    "    print(title)\n",
    "    title = nlp(title)\n",
    "    print(title)\n",
    "    \n",
    "    return np.mean(map(lambda x: title.similarity(nlp(x)), body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = nlp_en(u\"'Obama speaks to the media in Illinois'\")\n",
    "doc2 = nlp_en(u\"The president greets the press in Chicago\")\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use this one\n",
    "def spacy_acg_sim(title, body):\n",
    "    body_list = []\n",
    "    title = nlp_en(title)\n",
    "    for b in body:\n",
    "        body_list.append(title.similarity(nlp_en(b)))        \n",
    "    return np.mean(body_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45000000000000001"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0, 0.1, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'#.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'#.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85945738934493399"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_doc1 = nlp_en(raw_instance_train[0]['targetTitle'])\n",
    "paragraph_doc2 = nlp_en(raw_instance_train[0]['targetParagraphs'][0])\n",
    "title_doc1.similarity(paragraph_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6058277647979724"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#avg_similairty_result\n",
    "spacy_acg_sim(clickbait_train[10]['targetTitle'],clickbait_train[10]['targetParagraphs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76707725013332617"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list = []\n",
    "train_list_n = []\n",
    "print(len(raw_instance_train))\n",
    "for x in range(0, 100):\n",
    "    #TODO BLOCK NULL\n",
    "    train_list.append(spacy_acg_sim(clickbait_train[x]['targetTitle'],clickbait_train[x]['targetParagraphs']))\n",
    "for x in range(0, 100):\n",
    "    #TODO BLOCK NULL\n",
    "    train_list_n.append(spacy_acg_sim(non_clickbait_train[x]['targetTitle'],non_clickbait_train[x]['targetParagraphs']))\n",
    "np.mean(train_list)\n",
    "np.mean(train_list_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757950531111\n",
      "0.767077250133\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_list))\n",
    "print(np.mean(train_list_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple gives back gigabytes: iOS 9 'app thinning' feature will finally give your phone's storage a boost\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Word2VecKeyedVectors' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-678053de40e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#checking glove-wiki-gigaword-100 and ../GoogleNews-vectors-negative300.bin performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(\"glove-wiki-gigaword-100:\",avg_doc_sim(word_vectors,sentence_obama,sentence_president))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GoogleNews:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_doc_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_instance_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targetTitle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_instance_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targetParagraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#raw_instance_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-530f89ad2b18>\u001b[0m in \u001b[0;36mavg_doc_sim\u001b[0;34m(nlp, title, body)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#    return \"{:.4f}\".format(similarity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2VecKeyedVectors' object is not callable"
     ]
    }
   ],
   "source": [
    "#checking glove-wiki-gigaword-100 and ../GoogleNews-vectors-negative300.bin performance\n",
    "#print(\"glove-wiki-gigaword-100:\",avg_doc_sim(word_vectors,sentence_obama,sentence_president))\n",
    "print(\"GoogleNews:\", avg_doc_sim(model,raw_instance_train[0]['targetTitle'], raw_instance_train[0]['targetParagraphs']))\n",
    "#raw_instance_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-wiki-gigaword-100: 0.7067\n",
      "GoogleNews: 0.5984\n"
     ]
    }
   ],
   "source": [
    "print(\"glove-wiki-gigaword-100:\", avg_doc_sim(word_vectors,['sushi', 'shop'], ['japanese', 'restaurant']))\n",
    "print(\"GoogleNews:\", avg_doc_sim(model,['sushi', 'shop'], ['japanese', 'restaurant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml_(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    print(raw_html.replace(u'\\xa0',u''))\n",
    "    return raw_html.replace(u'\\xa0',u'')\n",
    "    cleantext_ = re.sub(cleanr, '', clean_html)\n",
    "    #return (cleantext_)\n",
    "    return cleantext_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str1 = \"lives.\\xa0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lives.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lives.'"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanhtml_(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parsing targetTitle and targetParagraphs\n",
    "#print([raw_instance_train[0]['targetTitle']])\n",
    "c_target_title_ = tweet_Tokenizer(raw_instance_train[0]['targetTitle'])\n",
    "c_target_paragraph_ = []\n",
    "for n in raw_instance_train[0]['targetParagraphs']:\n",
    "    n = n.replace(u'\\xa0',u'').replace(':', ' ')\n",
    "    if(n != \"\"):\n",
    "        #n = [n]\n",
    "        c_target_paragraph_ += tweet_Tokenizer(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'gives', 'back', 'gigabytes', 'iOS', '9', 'app', 'thinning', 'feature', 'will', 'finally', 'give', 'your', \"phone's\", 'storage', 'a', 'boost']\n",
      "------\n",
      "['Paying', 'for', 'a', '64GB', 'phone', 'only', 'to', 'discover', 'that', 'this', 'is', 'significantly', 'reduced', 'by', 'system', 'files', 'and', 'bloatware', 'is', 'the', 'bane', 'of', 'many', 'smartphone', \"owner's\", 'lives', 'And', 'the', 'issue', 'became', 'so', 'serious', 'earlier', 'this', 'year', 'that', 'some', 'Apple', 'users', 'even', 'sued', 'the', 'company', 'over', 'it', 'But', 'with', 'the', 'launch', 'of', 'iOS', '9', 'Apple', 'is', 'hoping', 'to', 'address', 'storage', 'concerns', 'by', 'introducing', 'a', 'feature', 'known', 'as', 'app', 'thinning', 'It', 'has', 'been', 'explained', 'on', 'the', 'watchOS', 'Developer', 'Library', 'site', 'and', 'is', 'aimed', 'at', 'developers', 'looking', 'to', 'optimise', 'their', 'apps', 'to', 'work', 'on', 'iOS', 'and', 'the', 'watchOS', 'It', 'ensures', 'apps', 'use', 'the', 'lowest', 'amount', 'of', 'storage', 'space', 'on', 'a', 'device', 'by', 'only', 'downloading', 'the', 'parts', 'it', 'needs', 'run', 'on', 'the', 'particular', 'handset', 'it', 'is', 'being', 'installed', 'onto', 'It', 'slices', 'the', 'app', 'into', 'app', 'variants', 'that', 'only', 'need', 'to', 'access', 'the', 'specific', 'files', 'on', 'that', 'specific', 'handset', 'XperiaBlog', 'recently', 'spotted', 'that', 'the', '8GB', 'version', 'of', \"Sony's\", 'mid-range', 'M4', 'Aqua', 'has', 'just', '1.26', 'GB', 'of', 'space', 'for', 'users', 'This', 'means', 'that', 'firmware', 'pre-installed', 'apps', 'and', 'Android', 'software', 'take', 'up', 'a', 'staggering', '84.25', 'per', 'cent', 'Sony', 'does', 'let', 'users', 'increase', 'storage', 'space', 'using', 'a', 'microSD', 'card', 'but', 'as', 'XperiaBlog', 'explained', 'Sony', 'should', 'never', 'have', 'launched', 'an', '8GB', 'version', 'of', 'the', 'Xperia', 'M4', 'Aqua', 'If', 'you', 'are', 'thinking', 'about', 'purchasing', 'this', 'model', 'be', 'aware', 'of', 'what', 'you', 'are', 'buying', 'into', 'Previously', 'apps', 'would', 'need', 'to', 'be', 'able', 'to', 'run', 'on', 'all', 'handsets', 'and', 'account', 'for', 'the', 'varying', 'files', 'chipsets', 'and', 'power', 'so', 'contained', 'sections', 'that', \"weren't\", 'always', 'relevant', 'to', 'the', 'phone', 'it', 'was', 'being', 'installed', 'on', 'This', 'made', 'them', 'larger', 'than', 'they', 'needed', 'to', 'be', 'Under', 'the', 'new', 'plans', 'when', 'a', 'phone', 'is', 'downloaded', 'from', 'the', 'App', 'Store', 'the', 'app', 'recognises', 'which', 'phone', 'it', 'is', 'being', 'installed', 'onto', 'and', 'only', 'pulls', 'in', 'the', 'files', 'and', 'code', 'it', 'needs', 'to', 'work', 'on', 'that', 'particular', 'device', 'For', 'iOS', 'sliced', 'apps', 'are', 'supported', 'on', 'the', 'latest', 'iTunes', 'and', 'on', 'devices', 'running', 'iOS', '9.0', 'and', 'later', 'In', 'all', 'other', 'cases', 'the', 'App', 'Store', 'will', 'deliver', 'the', 'previous', 'universal', 'apps', 'to', 'customers', 'The', 'guidelines', 'also', 'discuss', 'so-called', 'on-demand', 'resources', 'This', 'allows', 'developers', 'to', 'omit', 'features', 'from', 'an', 'app', 'until', 'they', 'are', 'opened', 'or', 'requested', 'by', 'the', 'user', 'The', 'App', 'Store', 'hosts', 'these', 'resources', 'on', 'Apple', 'servers', 'and', 'manages', 'the', 'downloads', 'for', 'the', 'developer', 'and', 'user', 'This', 'will', 'also', 'increase', 'how', 'quickly', 'an', 'app', 'downloads', 'An', 'example', 'given', 'by', 'Apple', 'is', 'a', 'game', 'app', 'that', 'may', 'divide', 'resources', 'into', 'game', 'levels', 'and', 'request', 'the', 'next', 'level', 'of', 'resources', 'only', 'when', 'the', 'app', 'anticipates', 'the', 'user', 'has', 'completed', 'the', 'previous', 'level', 'Similarly', 'the', 'app', 'can', 'request', 'In-App', 'Purchase', 'resources', 'only', 'when', 'the', 'user', 'buys', 'a', 'corresponding', 'in-app', 'purchase', 'Apple', 'explained', 'the', 'operating', 'system', 'will', 'then', 'purge', 'on-demand', 'resources', 'when', 'they', 'are', 'no', 'longer', 'needed', 'and', 'disk', 'space', 'is', 'low', 'removing', 'them', 'until', 'they', 'are', 'needed', 'again', 'And', 'the', 'whole', 'iOS', '9', 'software', 'has', 'been', 'designed', 'to', 'be', 'thinner', 'during', 'updates', 'namely', 'from', '4.6', 'GB', 'to', '1.3', 'GB', 'to', 'free', 'up', 'space', 'This', 'app', 'thinning', 'applies', 'to', 'third-party', 'apps', 'created', 'by', 'developers', 'Apple', \"doesn't\", 'say', 'if', 'it', 'will', 'apply', 'to', 'the', 'apps', 'Apple', 'pre-installed', 'on', 'devices', 'such', 'as', 'Stocks', 'Weather', 'and', 'Safari', 'but', 'it', 'is', 'likely', 'that', 'it', 'will', 'in', 'order', 'to', 'make', 'iOS', '9', 'smaller', 'As', 'an', 'example', 'of', 'storage', 'space', 'on', 'Apple', 'devices', 'a', '64GB', 'Apple', 'iPhone', '6', 'is', 'typically', 'left', 'with', '56GB', 'of', 'free', 'space', 'after', 'pre-installed', 'apps', 'system', 'files', 'and', 'software', 'is', 'included', 'A', 'drop', 'of', '8GB', 'leaving', '87.5', 'per', 'cent', 'of', 'storage', 'free', 'By', 'comparison', \"Samsung's\", '64GB', 'S6', 'Edge', 'has', '53.42', 'GB', 'of', 'available', 'space', 'and', 'of', 'this', '9GB', 'is', 'listed', 'as', 'system', 'memory', 'Although', 'this', 'is', 'a', 'total', 'drop', 'of', 'almost', '11GB', 'it', 'equates', 'to', '83', 'per', 'cent', 'of', 'space', 'free', 'By', 'comparison', 'on', 'a', '32GB', 'S6', 'MailOnline', 'found', '23.86', 'GB', 'of', 'space', 'was', 'available', 'with', '6.62', 'GB', 'attributed', 'to', 'system', 'memory', 'This', 'is', 'a', 'drop', 'of', 'just', 'over', '8GB', 'and', 'leaves', '75', 'per', 'cent', 'free', 'Samsung', 'said', 'it', 'too', 'had', 'addressed', 'complaints', 'about', 'bloatware', 'and', 'storage', 'space', 'with', 'its', 'S6', 'range', 'Previous', 'handsets', 'including', 'the', 'Samsung', 'Galaxy', 'S4', 'and', 'Apple', 'iPhone', '5C', 'typically', 'ranged', 'from', 'between', '54', 'per', 'cent', 'and', '79', 'per', 'cent', 'of', 'free', 'space', 'Businessman', 'killed', 'his', 'best', 'friend', 'when', 'he', 'crashed', 'jet-powered', 'dinghy', 'into', 'his', '£', '1million', 'yacht', 'while', 'showing', 'off', 'as', 'his', 'wife', 'filmed', 'them']\n"
     ]
    }
   ],
   "source": [
    "#review parsing result for targetTitle and targetParagraphs\n",
    "print(c_target_title_) #target title\n",
    "print(\"------\")\n",
    "print(c_target_paragraph_) # target paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'phone's' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-817-af663dd0fc24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#measure title and body's similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#error \"word 'watchos' not in vocabulary\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mavg_doc_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_target_title_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_target_paragraph_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-568-1e9bab263768>\u001b[0m in \u001b[0;36mavg_doc_sim\u001b[0;34m(word_vectors, title, body)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavg_doc_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"{:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mn_similarity\u001b[0;34m(self, ws1, ws2)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'At least one of the passed list is empty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'At least one of the passed list is empty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'phone's' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#measure title and body's similarity\n",
    "#error \"word 'watchos' not in vocabulary\"\n",
    "avg_doc_sim(model,c_target_title_, c_target_paragraph_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs to run on individual handsets. It 'slices' the app into 'app variants' that only need to access the specific files on that specific device\",\n",
       " \"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs to run on individual handsets. It 'slices' the app into 'app variants' that only need to access the specific files on that specific device\",\n",
       " \"The guidelines also discuss so-called 'on-demand resources.' This allows developers to omit features from an app until they are opened or requested by the user. The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0This will also increase how quickly an app downloads\",\n",
       " \"The guidelines also discuss so-called 'on-demand resources.' This allows developers to omit features from an app until they are opened or requested by the user. The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0This will also increase how quickly an app downloads\",\n",
       " \"Apple said it will then 'purge on-demand resources when they are no longer needed and disk space is low' (Apple's storage menu is pictured)\",\n",
       " \"Apple said it will then 'purge on-demand resources when they are no longer needed and disk space is low' (Apple's storage menu is pictured)\",\n",
       " 'A 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included. A drop of 8GB, leaving 87.5 % of storage free.\\xa0Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54% and 79% of free space (illustrated)',\n",
       " 'A 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included. A drop of 8GB, leaving 87.5 % of storage free.\\xa0Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54% and 79% of free space (illustrated)',\n",
       " \"Earlier this year, a pair of disgruntled Apple users filed a lawsuit in Miami accusing the tech giant of 'concealing, omitting and failing to disclose' that on 16GB versions of iPhones, more than 20% of the advertised space isn't available. This graph reveals the capacity available and unavailable to the user\",\n",
       " \"Earlier this year, a pair of disgruntled Apple users filed a lawsuit in Miami accusing the tech giant of 'concealing, omitting and failing to disclose' that on 16GB versions of iPhones, more than 20% of the advertised space isn't available. This graph reveals the capacity available and unavailable to the user\"]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#targetCaptions\n",
    "targetCaptions_documents = raw_instance_train[0]['targetCaptions']\n",
    "targetCaptions_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find common words and tokenize\n",
    "def pre_word_count(documents):\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "             for document in documents]\n",
    "    # remove words that appear only once\n",
    "    from collections import defaultdict\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    texts = [[token for token in text if frequency[token] > 1]\n",
    "             for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count each word in a document\n",
    "def word_Count(parsed_document):\n",
    "    dictionary = corpora.Dictionary(parsed_document)\n",
    "    return dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'app\",\n",
       "  \"thinning'\",\n",
       "  'will',\n",
       "  'be',\n",
       "  'supported',\n",
       "  'on',\n",
       "  \"apple's\",\n",
       "  'ios',\n",
       "  '9',\n",
       "  'later',\n",
       "  'models.',\n",
       "  'it',\n",
       "  'ensures',\n",
       "  'apps',\n",
       "  'use',\n",
       "  'lowest',\n",
       "  'amount',\n",
       "  'storage',\n",
       "  'space',\n",
       "  'on',\n",
       "  'device',\n",
       "  'by',\n",
       "  'only',\n",
       "  'downloading',\n",
       "  'parts',\n",
       "  'it',\n",
       "  'needs',\n",
       "  'run',\n",
       "  'on',\n",
       "  'individual',\n",
       "  'handsets.',\n",
       "  'it',\n",
       "  \"'slices'\",\n",
       "  'app',\n",
       "  'into',\n",
       "  \"'app\",\n",
       "  \"variants'\",\n",
       "  'that',\n",
       "  'only',\n",
       "  'need',\n",
       "  'access',\n",
       "  'specific',\n",
       "  'files',\n",
       "  'on',\n",
       "  'that',\n",
       "  'specific',\n",
       "  'device'],\n",
       " [\"'app\",\n",
       "  \"thinning'\",\n",
       "  'will',\n",
       "  'be',\n",
       "  'supported',\n",
       "  'on',\n",
       "  \"apple's\",\n",
       "  'ios',\n",
       "  '9',\n",
       "  'later',\n",
       "  'models.',\n",
       "  'it',\n",
       "  'ensures',\n",
       "  'apps',\n",
       "  'use',\n",
       "  'lowest',\n",
       "  'amount',\n",
       "  'storage',\n",
       "  'space',\n",
       "  'on',\n",
       "  'device',\n",
       "  'by',\n",
       "  'only',\n",
       "  'downloading',\n",
       "  'parts',\n",
       "  'it',\n",
       "  'needs',\n",
       "  'run',\n",
       "  'on',\n",
       "  'individual',\n",
       "  'handsets.',\n",
       "  'it',\n",
       "  \"'slices'\",\n",
       "  'app',\n",
       "  'into',\n",
       "  \"'app\",\n",
       "  \"variants'\",\n",
       "  'that',\n",
       "  'only',\n",
       "  'need',\n",
       "  'access',\n",
       "  'specific',\n",
       "  'files',\n",
       "  'on',\n",
       "  'that',\n",
       "  'specific',\n",
       "  'device'],\n",
       " ['guidelines',\n",
       "  'also',\n",
       "  'discuss',\n",
       "  'so-called',\n",
       "  \"'on-demand\",\n",
       "  \"resources.'\",\n",
       "  'this',\n",
       "  'allows',\n",
       "  'developers',\n",
       "  'omit',\n",
       "  'features',\n",
       "  'from',\n",
       "  'an',\n",
       "  'app',\n",
       "  'until',\n",
       "  'they',\n",
       "  'are',\n",
       "  'opened',\n",
       "  'or',\n",
       "  'requested',\n",
       "  'by',\n",
       "  'user.',\n",
       "  'app',\n",
       "  'store',\n",
       "  'hosts',\n",
       "  'these',\n",
       "  'resources',\n",
       "  'on',\n",
       "  'apple',\n",
       "  'servers',\n",
       "  'manages',\n",
       "  'downloads',\n",
       "  'developer',\n",
       "  'user.',\n",
       "  'this',\n",
       "  'will',\n",
       "  'also',\n",
       "  'increase',\n",
       "  'how',\n",
       "  'quickly',\n",
       "  'an',\n",
       "  'app',\n",
       "  'downloads'],\n",
       " ['guidelines',\n",
       "  'also',\n",
       "  'discuss',\n",
       "  'so-called',\n",
       "  \"'on-demand\",\n",
       "  \"resources.'\",\n",
       "  'this',\n",
       "  'allows',\n",
       "  'developers',\n",
       "  'omit',\n",
       "  'features',\n",
       "  'from',\n",
       "  'an',\n",
       "  'app',\n",
       "  'until',\n",
       "  'they',\n",
       "  'are',\n",
       "  'opened',\n",
       "  'or',\n",
       "  'requested',\n",
       "  'by',\n",
       "  'user.',\n",
       "  'app',\n",
       "  'store',\n",
       "  'hosts',\n",
       "  'these',\n",
       "  'resources',\n",
       "  'on',\n",
       "  'apple',\n",
       "  'servers',\n",
       "  'manages',\n",
       "  'downloads',\n",
       "  'developer',\n",
       "  'user.',\n",
       "  'this',\n",
       "  'will',\n",
       "  'also',\n",
       "  'increase',\n",
       "  'how',\n",
       "  'quickly',\n",
       "  'an',\n",
       "  'app',\n",
       "  'downloads'],\n",
       " ['apple',\n",
       "  'said',\n",
       "  'it',\n",
       "  'will',\n",
       "  'then',\n",
       "  \"'purge\",\n",
       "  'on-demand',\n",
       "  'resources',\n",
       "  'when',\n",
       "  'they',\n",
       "  'are',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'needed',\n",
       "  'disk',\n",
       "  'space',\n",
       "  'is',\n",
       "  \"low'\",\n",
       "  \"(apple's\",\n",
       "  'storage',\n",
       "  'menu',\n",
       "  'is',\n",
       "  'pictured)'],\n",
       " ['apple',\n",
       "  'said',\n",
       "  'it',\n",
       "  'will',\n",
       "  'then',\n",
       "  \"'purge\",\n",
       "  'on-demand',\n",
       "  'resources',\n",
       "  'when',\n",
       "  'they',\n",
       "  'are',\n",
       "  'no',\n",
       "  'longer',\n",
       "  'needed',\n",
       "  'disk',\n",
       "  'space',\n",
       "  'is',\n",
       "  \"low'\",\n",
       "  \"(apple's\",\n",
       "  'storage',\n",
       "  'menu',\n",
       "  'is',\n",
       "  'pictured)'],\n",
       " ['64gb',\n",
       "  'apple',\n",
       "  'iphone',\n",
       "  '6',\n",
       "  'is',\n",
       "  'typically',\n",
       "  'left',\n",
       "  'with',\n",
       "  '56gb',\n",
       "  'free',\n",
       "  'space',\n",
       "  'after',\n",
       "  'pre-installed',\n",
       "  'apps,',\n",
       "  'system',\n",
       "  'files',\n",
       "  'software',\n",
       "  'is',\n",
       "  'included.',\n",
       "  'drop',\n",
       "  '8gb,',\n",
       "  'leaving',\n",
       "  '87.5',\n",
       "  '%',\n",
       "  'storage',\n",
       "  'free.',\n",
       "  'previous',\n",
       "  'handsets,',\n",
       "  'including',\n",
       "  'samsung',\n",
       "  'galaxy',\n",
       "  's4',\n",
       "  'apple',\n",
       "  'iphone',\n",
       "  '5c',\n",
       "  'typically',\n",
       "  'ranged',\n",
       "  'from',\n",
       "  'between',\n",
       "  '54%',\n",
       "  '79%',\n",
       "  'free',\n",
       "  'space',\n",
       "  '(illustrated)'],\n",
       " ['64gb',\n",
       "  'apple',\n",
       "  'iphone',\n",
       "  '6',\n",
       "  'is',\n",
       "  'typically',\n",
       "  'left',\n",
       "  'with',\n",
       "  '56gb',\n",
       "  'free',\n",
       "  'space',\n",
       "  'after',\n",
       "  'pre-installed',\n",
       "  'apps,',\n",
       "  'system',\n",
       "  'files',\n",
       "  'software',\n",
       "  'is',\n",
       "  'included.',\n",
       "  'drop',\n",
       "  '8gb,',\n",
       "  'leaving',\n",
       "  '87.5',\n",
       "  '%',\n",
       "  'storage',\n",
       "  'free.',\n",
       "  'previous',\n",
       "  'handsets,',\n",
       "  'including',\n",
       "  'samsung',\n",
       "  'galaxy',\n",
       "  's4',\n",
       "  'apple',\n",
       "  'iphone',\n",
       "  '5c',\n",
       "  'typically',\n",
       "  'ranged',\n",
       "  'from',\n",
       "  'between',\n",
       "  '54%',\n",
       "  '79%',\n",
       "  'free',\n",
       "  'space',\n",
       "  '(illustrated)'],\n",
       " ['earlier',\n",
       "  'this',\n",
       "  'year,',\n",
       "  'pair',\n",
       "  'disgruntled',\n",
       "  'apple',\n",
       "  'users',\n",
       "  'filed',\n",
       "  'lawsuit',\n",
       "  'miami',\n",
       "  'accusing',\n",
       "  'tech',\n",
       "  'giant',\n",
       "  \"'concealing,\",\n",
       "  'omitting',\n",
       "  'failing',\n",
       "  \"disclose'\",\n",
       "  'that',\n",
       "  'on',\n",
       "  '16gb',\n",
       "  'versions',\n",
       "  'iphones,',\n",
       "  'more',\n",
       "  'than',\n",
       "  '20%',\n",
       "  'advertised',\n",
       "  'space',\n",
       "  \"isn't\",\n",
       "  'available.',\n",
       "  'this',\n",
       "  'graph',\n",
       "  'reveals',\n",
       "  'capacity',\n",
       "  'available',\n",
       "  'unavailable',\n",
       "  'user'],\n",
       " ['earlier',\n",
       "  'this',\n",
       "  'year,',\n",
       "  'pair',\n",
       "  'disgruntled',\n",
       "  'apple',\n",
       "  'users',\n",
       "  'filed',\n",
       "  'lawsuit',\n",
       "  'miami',\n",
       "  'accusing',\n",
       "  'tech',\n",
       "  'giant',\n",
       "  \"'concealing,\",\n",
       "  'omitting',\n",
       "  'failing',\n",
       "  \"disclose'\",\n",
       "  'that',\n",
       "  'on',\n",
       "  '16gb',\n",
       "  'versions',\n",
       "  'iphones,',\n",
       "  'more',\n",
       "  'than',\n",
       "  '20%',\n",
       "  'advertised',\n",
       "  'space',\n",
       "  \"isn't\",\n",
       "  'available.',\n",
       "  'this',\n",
       "  'graph',\n",
       "  'reveals',\n",
       "  'capacity',\n",
       "  'available',\n",
       "  'unavailable',\n",
       "  'user']]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding common words\n",
    "targetCaptions_parsed_document = pre_word_count(targetCaptions_documents)\n",
    "targetCaptions_parsed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'%': 83,\n",
       " \"'app\": 0,\n",
       " \"'concealing,\": 115,\n",
       " \"'on-demand\": 37,\n",
       " \"'purge\": 69,\n",
       " \"'slices'\": 1,\n",
       " \"(apple's\": 70,\n",
       " '(illustrated)': 84,\n",
       " '16gb': 116,\n",
       " '20%': 117,\n",
       " '54%': 85,\n",
       " '56gb': 86,\n",
       " '5c': 87,\n",
       " '6': 88,\n",
       " '64gb': 89,\n",
       " '79%': 90,\n",
       " '87.5': 91,\n",
       " '8gb,': 92,\n",
       " '9': 2,\n",
       " 'access': 3,\n",
       " 'accusing': 118,\n",
       " 'advertised': 119,\n",
       " 'after': 93,\n",
       " 'allows': 38,\n",
       " 'also': 39,\n",
       " 'amount': 4,\n",
       " 'an': 40,\n",
       " 'app': 5,\n",
       " 'apple': 41,\n",
       " \"apple's\": 6,\n",
       " 'apps': 7,\n",
       " 'apps,': 94,\n",
       " 'are': 42,\n",
       " 'available': 120,\n",
       " 'available.': 121,\n",
       " 'be': 8,\n",
       " 'between': 95,\n",
       " 'by': 9,\n",
       " 'capacity': 122,\n",
       " 'developer': 43,\n",
       " 'developers': 44,\n",
       " 'device': 10,\n",
       " \"disclose'\": 123,\n",
       " 'discuss': 45,\n",
       " 'disgruntled': 124,\n",
       " 'disk': 71,\n",
       " 'downloading': 11,\n",
       " 'downloads': 46,\n",
       " 'drop': 96,\n",
       " 'earlier': 125,\n",
       " 'ensures': 12,\n",
       " 'failing': 126,\n",
       " 'features': 47,\n",
       " 'filed': 127,\n",
       " 'files': 13,\n",
       " 'free': 97,\n",
       " 'free.': 98,\n",
       " 'from': 48,\n",
       " 'galaxy': 99,\n",
       " 'giant': 128,\n",
       " 'graph': 129,\n",
       " 'guidelines': 49,\n",
       " 'handsets,': 100,\n",
       " 'handsets.': 14,\n",
       " 'hosts': 50,\n",
       " 'how': 51,\n",
       " 'included.': 101,\n",
       " 'including': 102,\n",
       " 'increase': 52,\n",
       " 'individual': 15,\n",
       " 'into': 16,\n",
       " 'ios': 17,\n",
       " 'iphone': 103,\n",
       " 'iphones,': 130,\n",
       " 'is': 72,\n",
       " \"isn't\": 131,\n",
       " 'it': 18,\n",
       " 'later': 19,\n",
       " 'lawsuit': 132,\n",
       " 'leaving': 104,\n",
       " 'left': 105,\n",
       " 'longer': 73,\n",
       " \"low'\": 74,\n",
       " 'lowest': 20,\n",
       " 'manages': 53,\n",
       " 'menu': 75,\n",
       " 'miami': 133,\n",
       " 'models.': 21,\n",
       " 'more': 134,\n",
       " 'need': 22,\n",
       " 'needed': 76,\n",
       " 'needs': 23,\n",
       " 'no': 77,\n",
       " 'omit': 54,\n",
       " 'omitting': 135,\n",
       " 'on': 24,\n",
       " 'on-demand': 78,\n",
       " 'only': 25,\n",
       " 'opened': 55,\n",
       " 'or': 56,\n",
       " 'pair': 136,\n",
       " 'parts': 26,\n",
       " 'pictured)': 79,\n",
       " 'pre-installed': 106,\n",
       " 'previous': 107,\n",
       " 'quickly': 57,\n",
       " 'ranged': 108,\n",
       " 'requested': 58,\n",
       " 'resources': 59,\n",
       " \"resources.'\": 60,\n",
       " 'reveals': 137,\n",
       " 'run': 27,\n",
       " 's4': 109,\n",
       " 'said': 80,\n",
       " 'samsung': 110,\n",
       " 'servers': 61,\n",
       " 'so-called': 62,\n",
       " 'software': 111,\n",
       " 'space': 28,\n",
       " 'specific': 29,\n",
       " 'storage': 30,\n",
       " 'store': 63,\n",
       " 'supported': 31,\n",
       " 'system': 112,\n",
       " 'tech': 138,\n",
       " 'than': 139,\n",
       " 'that': 32,\n",
       " 'then': 81,\n",
       " 'these': 64,\n",
       " 'they': 65,\n",
       " \"thinning'\": 33,\n",
       " 'this': 66,\n",
       " 'typically': 113,\n",
       " 'unavailable': 140,\n",
       " 'until': 67,\n",
       " 'use': 34,\n",
       " 'user': 141,\n",
       " 'user.': 68,\n",
       " 'users': 142,\n",
       " \"variants'\": 35,\n",
       " 'versions': 143,\n",
       " 'when': 82,\n",
       " 'will': 36,\n",
       " 'with': 114,\n",
       " 'year,': 144}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word count\n",
    "word_Count(targetCaptions_parsed_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clickbait_train, emoji\n",
    "def pos_1gram(data, title):\n",
    "    postag_words = word_tokenize(data.get(title)[0])\n",
    "    return nltk.pos_tag(postag_words)\n",
    "\n",
    "pos_tag_list = []\n",
    "#for single_data in clickbait_train:\n",
    "    #print(pos_2gram(single_data, \"postText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.', 'NNP'),\n",
       " ('Soccer', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " ('start', 'VB'),\n",
       " ('answering', 'VBG'),\n",
       " ('tough', 'JJ'),\n",
       " ('questions', 'NNS'),\n",
       " ('about', 'IN'),\n",
       " ('Hope', 'NNP'),\n",
       " ('Solo', 'NNP'),\n",
       " (',', ','),\n",
       " ('@', 'NNP'),\n",
       " ('eric_adelson', 'NN'),\n",
       " ('writes', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_1gram(clickbait_train[0], \"postText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate tags in a data\n",
    "def cal_pos_1grams_tag(data, title, tag):\n",
    "    i = 0\n",
    "    pos_list = []\n",
    "    postag_words = word_tokenize(data.get(title)[0])\n",
    "    pos_list = nltk.pos_tag(postag_words)\n",
    "    for tags in pos_list:\n",
    "        print(\"tags: \", tags)\n",
    "        if tags[1] == tag:\n",
    "            i+=1\n",
    "    return i   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags:  ('U.S.', 'NNP')\n",
      "tags:  ('Soccer', 'NNP')\n",
      "tags:  ('should', 'MD')\n",
      "tags:  ('start', 'VB')\n",
      "tags:  ('answering', 'VBG')\n",
      "tags:  ('tough', 'JJ')\n",
      "tags:  ('questions', 'NNS')\n",
      "tags:  ('about', 'IN')\n",
      "tags:  ('Hope', 'NNP')\n",
      "tags:  ('Solo', 'NNP')\n",
      "tags:  (',', ',')\n",
      "tags:  ('@', 'NNP')\n",
      "tags:  ('eric_adelson', 'NN')\n",
      "tags:  ('writes', 'NNS')\n",
      "tags:  ('.', '.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_pos_1grams_tag(clickbait_train[0], \"postText\", \"NNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yutarochan's code\n",
    "def pos_2gram(title, p1, p2):\n",
    "    pos_list = pos_tag(word_tokenize(title))\n",
    "    #print(\"-1: \", pos_list[:-1])\n",
    "    #print(\"1: \", pos_list[1:])\n",
    "    #print(\"zip: \", list(zip(pos_list[:-1], pos_list[1:])))\n",
    "    #print(list(zip(pos_list[:-1], pos_list[1:]))[0][1])\n",
    "    #print(list(zip(pos_list[:-1], pos_list[1:]))[1][1])\n",
    "    return sum(map(lambda x: x[0][1] == p1 and x[1][1] == p2, zip(pos_list[:-1], pos_list[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U.S. Soccer should start answering tough questions about Hope Solo'"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickbait_train[0]['targetTitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_2gram(clickbait_train[0]['targetTitle'], 'NNP', 'NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#revised pos_2grams\n",
    "def pos_2gram_sum(title, p1, p2):\n",
    "    pos_2g_num = 0\n",
    "    pos_list = pos_tag(word_tokenize(title))\n",
    "    pos_ziplist = zip(pos_list[:-1], pos_list[1:])\n",
    "    for pos_z in pos_ziplist:\n",
    "        if pos_z[0][1] == p1 and pos_z[1][1] == p2:\n",
    "            pos_2g_num += 1\n",
    "    return pos_2g_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_2gram_sum(clickbait_train[0]['targetTitle'], 'NNP', 'NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check pos_2grams\n",
    "def pos_2gram_check(title, p1, p2):\n",
    "    pos_2gram_list = []\n",
    "    pos_list = pos_tag(word_tokenize(title))\n",
    "    pos_ziplist = zip(pos_list[:-1], pos_list[1:])\n",
    "    for pos_z in pos_ziplist:\n",
    "        if pos_z[0][1] == p1 and pos_z[1][1] == p2:\n",
    "            pos_2gram_list.append([pos_z[0], pos_z[1]])\n",
    "    return pos_2gram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('U.S.', 'NNP'), ('Soccer', 'NNP')], [('Hope', 'NNP'), ('Solo', 'NNP')]]\n"
     ]
    }
   ],
   "source": [
    "print(pos_2gram_check(clickbait_train[0]['targetTitle'], 'NNP', 'NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#revised pos_3grams\n",
    "def pos_3gram_sum(title, p1, p2, p3):\n",
    "    pos_3g_num = 0\n",
    "    pos_list = pos_tag(word_tokenize(title))\n",
    "    pos_ziplist = zip(pos_list[:-2], pos_list[1:-1], pos_list[2:])\n",
    "    for pos_z in pos_ziplist:\n",
    "        if pos_z[0][1] == p1 and pos_z[1][1] == p2 and pos_z[2][1] == p3:\n",
    "            pos_3g_num += 1\n",
    "    return pos_3g_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(pos_3gram_sum(clickbait_train[0]['targetTitle'], 'NNP', 'NNP', 'NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check pos_3grams\n",
    "def pos_3gram_check(title, p1, p2, p3):\n",
    "    pos_3gram_list = []\n",
    "    pos_list = pos_tag(word_tokenize(title))\n",
    "    pos_ziplist = zip(pos_list[:-2], pos_list[1:-1], pos_list[2:])\n",
    "    for pos_z in pos_ziplist:\n",
    "        if pos_z[0][1] == p1 and pos_z[1][1] == p2 and pos_z[2][1] == p3:\n",
    "            pos_3gram_list.append([pos_z[0], pos_z[1],pos_z[2]])\n",
    "    return pos_3gram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(pos_3gram_check(clickbait_train[0]['targetTitle'], 'NNP', 'NNP', 'NNP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/EthanHsu/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just downloading some nltk samples\n",
    "nltk.download('brown')\n",
    "nltk.corpus.brown.tagged_words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just downloading some nltk samples\n",
    "nltk_list = list(nltk.corpus.brown.tagged_words(categories='news'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_para_list = []\n",
    "for n in clickbait_train[0]['targetParagraphs']:\n",
    "    target_para_ = doc_punc_parser(n)\n",
    "    #print(type(target_para_list))\n",
    "    #print(target_para_)\n",
    "    target_para_list = target_para_list + target_para_\n",
    "    #print(n)\n",
    "    #print(nltk.pos_tag(n))\n",
    "\n",
    "target_para_tags = nltk.pos_tag(target_para_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tags (tags_list):\n",
    "    tag_fd = nltk.FreqDist(tag for (word, tag) in tags_list)\n",
    "    return (tag_fd.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 115), ('NNP', 90), ('DT', 88), ('IN', 87), ('JJ', 44), ('VBZ', 41), ('RB', 40), ('.', 31), ('VB', 30), (',', 27), ('NNS', 24), ('TO', 23), ('CC', 20), ('POS', 16), ('VBD', 15), ('VBG', 14), ('PRP', 13), ('VBN', 12), ('PRP$', 10), (\"''\", 10), ('``', 9), ('VBP', 6), ('MD', 6), ('JJR', 5), ('WP', 4), ('WRB', 3), (':', 3), ('CD', 3), ('EX', 3), ('JJS', 2), ('(', 2), ('WDT', 2), (')', 2), ('RBR', 2), ('RBS', 1), ('RP', 1), ('NNPS', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(count_tags(target_para_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findtags(tag_prefix, tagged_text):\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    return dict((tag, cfd[tag].most_common(5)) for tag in cfd.conditions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': [('violence', 6),\n",
       "  ('lot', 4),\n",
       "  ('incident', 3),\n",
       "  ('explanation', 3),\n",
       "  ('game', 3)],\n",
       " 'NNP': [('Solo', 15),\n",
       "  ('U.S.', 11),\n",
       "  ('Soccer', 10),\n",
       "  ('Gulati', 5),\n",
       "  ('Hope', 4)],\n",
       " 'NNPS': [('Americans', 1)],\n",
       " 'NNS': [('women', 2),\n",
       "  ('questions', 2),\n",
       "  ('months', 2),\n",
       "  ('concerns', 1),\n",
       "  ('values', 1)]}"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findtags(\"NN\",target_para_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['Apple', 'gives', 'back', 'gigabytes', ':', 'iOS', '9', \"'app\", 'thinning', \"'\", 'feature', 'will', 'finally', 'give', 'your', 'phone', \"'s\", 'storage', 'a', 'boost']\n",
      "['A']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Whether 'target Title' Start with 5W1H.\n",
    "wh_start = lambda x: word_tokenize(raw_instance_train[x]['targetTitle'].lower())[0] in ['who', 'when', 'where', 'how', 'why','what']\n",
    "#Ethan's edit, I delete the [0] after raw_instance_train[x]['targetTitle']. It's a string already\n",
    "wh_sample = lambda x: word_tokenize('What is it'.lower())[0] in ['who', 'when', 'where', 'how', 'why','what']\n",
    "print((wh_sample(0)))\n",
    "print(word_tokenize(raw_instance_train[0]['targetTitle']))\n",
    "print(word_tokenize(raw_instance_train[0]['targetTitle'][0]))\n",
    "print(wh_start(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['apple', 'gives', 'back', 'gigabytes', ':', 'ios', '9', \"'app\", 'thinning', \"'\", 'feature', 'will', 'finally', 'give', 'your', 'phone', \"'s\", 'storage', 'a', 'boost']\n"
     ]
    }
   ],
   "source": [
    "#Foward reference: Whether 'target Title' has Demonstratives(this, that, those, these) \n",
    "hasDemonstrative = lambda y: sum(map(lambda x: str(x) in ['this', 'that', 'those','these'], \n",
    "                                     word_tokenize(raw_instance_train[y]['targetTitle'].lower()))) > 0\n",
    "print(hasDemonstrative(0))\n",
    "print(word_tokenize(raw_instance_train[0]['targetTitle'].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Foward reference: Whether 'target Title' has definite article('the')\n",
    "hasDefinite = lambda y: sum(map(lambda x: str(x) == 'the', word_tokenize(raw_instance_train[y]\n",
    "                                                                         ['targetTitle'].lower()))) > 0\n",
    "hasDefinite(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Foward reference: Whether 'target Title' has 3rd person pronoun('he','she','his','her','him')\n",
    "hasThirdPronoun = lambda y: sum(map(lambda x: str(x) in ['he','she','his','her','him'], word_tokenize(raw_instance_train[y]['targetTitle'].lower()))) > 0\n",
    "hasThirdPronoun(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Foward reference: Whether 'target Title' Start with adverbs.\n",
    "adverb_start = lambda x: pos_tag(word_tokenize(raw_instance_train[x]['targetTitle']))[0][1] == 'ADV'\n",
    "# print(pos_tag(word_tokenize(raw_instance_train[0]['targetTitle']))[0][1])\n",
    "print(adverb_start(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U.S. Soccer should start answering tough questions about Hope Solo'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickbait_train[0]['targetTitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"WINNIPEG, Manitoba – The bubble U.S. Soccer is putting around Hope Solo isn't working to calm anyone's concerns about the star goalkeeper.\",\n",
       " \"The latest lament comes from no less than a U.S. Senator, who into Solo's domestic violence incident of last year and offer a detailed explanation of why Solo is on the field. She is expected to be the starting goalkeeper when the USA plays Sweden in its second group game at the Women's World Cup on Friday.\",\n",
       " '[FC Yahoo: ]',\n",
       " 'U.S. Senator Richard Blumenthal of Connecticut penned a lengthy complaint about the near-silence the organization has given on Solo, especially in the wake of ESPN\\'s \"Outside the Lines\" report on Sunday. Blumenthal wrote that if the report is accurate \"U.S. Soccer\\'s approach to domestic violence and family violence is at best superficial and at worst dangerously neglectful and self-serving.\"',\n",
       " 'This situation is well beyond Solo now. U.S. Soccer has made this a referendum about its own ability to represent the values of the nation. \"As boys and girls tune into Friday\\'s game, watching the women on TV as role models,\" Blumenthal wrote, \"what is the message of starting Hope Solo at goal?\"',\n",
       " \"[Women's World Cup: | | | ]\",\n",
       " \"U.S. Soccer is not only avoiding difficult questions, it is also avoiding an account of all its actions. Even NFL commissioner Roger Goodell has, to some extent, owned up to his failures on the Ray Rice case, yet Gulati has not even decried Solo's poor decisions.\",\n",
       " \"Last September, three months after Solo's domestic violence charges (which were later ), Gulati released this vague statement on the matter:\",\n",
       " '\"U.S. Soccer takes the issue of domestic violence very seriously. From the beginning, we considered the information available and have taken a deliberate and thoughtful approach regarding Hope Solo\\'s status with the national team. Based on that information, U.S. Soccer stands by our decision to allow her to participate with the team as the legal process unfolds. If new information becomes available we will carefully consider it.\"',\n",
       " \"A lot of this would be solved if Gulati and Solo held a press conference and claimed some accountability. It's clear from Monday's dominant performance in a 3-1 tournament-opening win over Australia that Solo is not distracted by the national discussion of her past, so a short appearance – even without reporters' questions – probably won't ruin the U.S.'s chances for a trophy. And claiming that Solo has a match to focus on isn't credible as it's basically an admission that a single game is more important than a discussion of domestic violence.\",\n",
       " \"For Gulati, there is little excuse. The silence, the lack of punishment and then the decision to allow head coach Jill Ellis to discuss (or not discuss) the situation here, combines to make the top official of American soccer look like he doesn't prioritize this issue.\",\n",
       " '\"In the wake of this violent incident, U.S. Soccer offered no comment publicly for three months,\" Blumenthal wrote. \"It finally issued a statement that was purportedly the result of a \\'deliberate and thoughtful approach\\' to consider the incident and determine Hope Solo\\'s status with the team, but it neglected to include an effort to contact the alleged victims.\"',\n",
       " 'The more U.S. Soccer tries to shift focus to the field, the less it accomplishes that. This is the Super Bowl of women\\'s soccer, and decrying this as \"old news\" doesn\\'t work because the entire country is watching now. Countless Americans are debating whether to root for Solo or not, and her protectors are effectively convincing a lot of people to remain skeptical of her.',\n",
       " \"It doesn't have to be this way. A better explanation of what Gulati has done on this topic – and a better explanation of what Solo has done to work on her problems – would go a long way toward moving on, especially the way U.S. Soccer clearly wants. Instead, there is opacity where there should be transparency.\",\n",
       " 'The NFL has come under a lot of scrutiny for its efforts to \"protect the shield,\" but U.S. Soccer\\'s shield stands for a lot more than just a sport. That shield shouldn\\'t only be used to defend a player.',\n",
       " '735']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickbait_train[0]['targetParagraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_acg_sim_lg(title, body):\n",
    "    body_list = []\n",
    "    title = nlp_lg(title)\n",
    "    for b in body:\n",
    "        body_list.append(title.similarity(nlp_lg(b)))        \n",
    "    return np.mean(body_list) \n",
    "\n",
    "def spacy_acg_sim_core_lg(title, body):\n",
    "    body_list = []\n",
    "    title = nlp_core_lg(title)\n",
    "    for b in body:\n",
    "        body_list.append(title.similarity(nlp_core_lg(b)))        \n",
    "    return np.mean(body_list) \n",
    "\n",
    "def spacy_acg_sim(title, body):\n",
    "    body_list = []\n",
    "    title = nlp_en(title)\n",
    "    for b in body:\n",
    "        body_list.append(title.similarity(nlp_en(b)))        \n",
    "    return np.mean(body_list) \n",
    "\n",
    "def spacy_acg_sim_en(title, body):\n",
    "    body_list = []\n",
    "    title = nlp(title)\n",
    "    for b in body:\n",
    "        body_list.append(title.similarity(nlp(b)))        \n",
    "    return np.mean(body_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.713268983008\n",
      "0.713268983008\n",
      "0.713268983008\n",
      "0.566875402212\n"
     ]
    }
   ],
   "source": [
    "print(spacy_acg_sim(clickbait_train[0]['targetTitle'], clickbait_train[0]['targetParagraphs']))\n",
    "print(spacy_acg_sim_lg(clickbait_train[0]['targetTitle'], clickbait_train[0]['targetParagraphs']))\n",
    "print(spacy_acg_sim_core_lg(clickbait_train[0]['targetTitle'], clickbait_train[0]['targetParagraphs']))\n",
    "print(spacy_acg_sim_en(clickbait_train[0]['targetTitle'], clickbait_train[0]['targetParagraphs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696579747105\n",
      "0.696579747105\n",
      "0.696579747105\n"
     ]
    }
   ],
   "source": [
    "print(spacy_acg_sim(clickbait_train[1]['targetTitle'], clickbait_train[1]['targetParagraphs']))\n",
    "print(spacy_acg_sim_lg(clickbait_train[1]['targetTitle'], clickbait_train[1]['targetParagraphs']))\n",
    "print(spacy_acg_sim_core_lg(clickbait_train[1]['targetTitle'], clickbait_train[1]['targetParagraphs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805061736424\n",
      "0.805061736424\n",
      "0.805061736424\n"
     ]
    }
   ],
   "source": [
    "print(spacy_acg_sim(non_clickbait_train[0]['targetTitle'], non_clickbait_train[0]['targetParagraphs']))\n",
    "print(spacy_acg_sim_lg(non_clickbait_train[0]['targetTitle'], non_clickbait_train[0]['targetParagraphs']))\n",
    "print(spacy_acg_sim_core_lg(non_clickbait_train[0]['targetTitle'], non_clickbait_train[0]['targetParagraphs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56687540221182187, 0.50985086969995952, 0.47911665660326125, 0.5603878601403921, 0.59881734957342114, 0.51712832714141144, 0.37089290114414508, 0.61882848343541907, 0.23625808029908985, 0.59168612024112233, 0.36493104801633119, 0.50633185574348405, 0.57726160156698081, 0.14982355632069932, 0.47559553618131384, 0.47980012819828349, 0.50439637931267156, 0.31338355078344143, 0.67130023539476247, 0.65597724463824225, 0.58835304069642147, 0.74504834685644739, 0.58044116675746593, 0.43465435359670701, 0.42675142208514855, 0.64152274653532293, 0.48751852812505048, 0.53094540055102335, 0.63605935285367388, 0.72131931845542552, 0.54126572071383949, 0.58710082084990378, 0.59526088798802401, 0.45283390276903968, 0.38646000554808257, 0.27874326885828604, 0.59489120874645107, 0.46349992281546937, 0.49155166787859311, 0.6550014953349137, 0.64515822731164896, 0.140578053900936, 0.5281877175407933, 0.64100202964790143, 0.49362951167069574, 0.65735084106997144, 0.67395831716178023, 0.71091628582935507, 0.35521771332615021, 0.54709387571882506, 0.62016076025212175, 0.51934008924069763, 0.55788854188854831, 0.36587526494672773, 0.66130264636238645, 0.61933433173890817, 0.53475610253898853, 0.66806086284879229, 0.50509587364207775, 0.4475623038726832, 0.70791646411839348, 0.32115894116965243, 0.39982138580031046, 0.45060372106285823, 0.52439717668499097, 0.73061786580245969, 0.49379314373026856, 0.62773109310134045, 0.42100597603798207, 0.71260656736535821, 0.64335803749138198, 0.63448461650027133, 0.66429189259415677, 0.46357217862286709, 0.69156430031703797, 0.5270233562745541, 0.52882839336024479, 0.36089913441925786, 0.76957641355265105, 0.63287973723944935, 0.40097477283306271, 0.56458600313307306, 0.58491128630963474, 0.36493104801633119, 0.50235598936943526, 0.6992424725919999, 0.66677587237591018, 0.63004489116983686, 0.54890484110325932, 0.56776918344286553, 0.41017990903345064, 0.48182924158878815, 0.59499243403591362, 0.74352053414325492, 0.72000535290923162, 0.4211429990475099, 0.46675478949112581, 0.66943736801381437, 0.33491296182304575, 0.72106329614647935]\n",
      "0.53880800757\n",
      "[0.57787673420185448, 0.37884481869148107, 0.68763759341449371, 0.42332516942841059, 0.69576916488393381, 0.24447487062508758, 0.67131586003040289, 0.56608977508556257, 0.59697071625856557, 0.36325155278438925, 0.62454756737637307, 0.63469286076186282, 0.56165514403673222, 0.56624037943982564, 0.62185224201365386, 0.55183048155385295, 0.65856661941752237, 0.55082589637742074, 0.5457032290244791, 0.51286796563124037, 0.37025234477266339, 0.46530367751093227, 0.38141951252489303, 0.58250766379166485, 0.64752092545289464, 0.44332082344408485, 0.53623342553601827, 0.68196672811248449, 0.71268651315175136, 0.735741683802841, 0.45217086200467543, 0.66049239110149272, 0.48903688836153758, 0.67488194995438311, 0.38570949617514294, 0.3442501837572941, 0.74604422694114936, 0.53137308617624535, 0.61387138884050152, 0.59878807596539041, 0.45803250212136271, 0.57149793910080027, 0.21178105469341732, 0.72965148044971928, 0.51081774718548345, 0.24466577242252618, 0.50240730835272163, 0.61765733064995754, 0.424604094107632, 0.59923062603221178, 0.60533444139743331, 0.51171227217034099, 0.57266176856025175, 0.62298972050584678, 0.4462300260307791, 0.71423920450784451, 0.58841028395735939, 0.71219246123478352, 0.48459154305817354, 0.50600916087727099, 0.55094636498532146, 0.74010767631776564, 0.44015371821542232, 0.52302726462394422, 0.62069036399797828, 0.54715494616307503, 0.70365772117337566, 0.39237252912805509, 0.4643494003467753, 0.71390743867009343, 0.67699958075558386, 0.59694117443192074, 0.50818334404299814, 0.42771110385548916, 0.6443711104401707, 0.44833055213081474, 0.58556553665887567, 0.42456016986422812, 0.44084868339820826, 0.68080301437618684, 0.59867852193569016, 0.51628131404618705, 0.59877060800270343, 0.64155743567315182, 0.43987054509423962, 0.47670300410708083, 0.50511674463021017, 0.63884909262804246, 0.66948460684167233, 0.39099872185649071, 0.28823990162162166, 0.43247646798199785, 0.77225624169469187, 0.46786025550948707, 0.4948722610278638, 0.42439361562064515, 0.36055487061572855, 0.44479849031949914, 0.44408586583900606, 0.61202832015299269]\n",
      "0.540711858046\n"
     ]
    }
   ],
   "source": [
    "bait = []\n",
    "non_bait = []\n",
    "for x in range(0, 100):\n",
    "    bait.append(spacy_acg_sim_en(clickbait_train[x]['targetTitle'], clickbait_train[x]['targetParagraphs']))\n",
    "    non_bait.append(spacy_acg_sim_en(non_clickbait_train[x]['targetTitle'], non_clickbait_train[x]['targetParagraphs']))\n",
    "print(bait)\n",
    "print(np.mean(bait))\n",
    "print(non_bait)\n",
    "print(np.mean(non_bait))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940947083805\n",
      "0.326296786973\n",
      "0.295776641004\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    " \n",
    "doc1 = nlp(u'Hello this is document similarity calculation')\n",
    "doc2 = nlp(u'Hello this is python similarity calculation')\n",
    "doc3 = nlp(u'Hi there')\n",
    " \n",
    "print (doc1.similarity(doc2)) \n",
    "print (doc2.similarity(doc3)) \n",
    "print (doc1.similarity(doc3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940947083805\n",
      "0.326296786973\n",
      "0.295776641004\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    " \n",
    "doc1 = nlp('Hello this is document similarity calculation')\n",
    "doc2 = nlp('Hello this is python similarity calculation')\n",
    "doc3 = nlp('Hi there')\n",
    " \n",
    "print (doc1.similarity(doc2)) \n",
    "print (doc2.similarity(doc3)) \n",
    "print (doc1.similarity(doc3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', vectors='en_glove_cc_300_1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.05126953 -0.02233887 -0.17285156  0.16113281 -0.08447266  0.05737305\n",
      "  0.05859375 -0.08251953 -0.01538086 -0.06347656]\n",
      "Михаил is an out of dictionary word\n",
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431607246399), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454)]\n",
      "cereal\n",
      "0.766401\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "dog = model['dog']\n",
    "print(dog.shape)\n",
    "print(dog[:10])\n",
    "\n",
    "# Deal with an out of dictionary word: Михаил (Michail)\n",
    "if 'Михаил' in model:\n",
    "    print(model['Михаил'].shape)\n",
    "else:\n",
    "    print('{0} is an out of dictionary word'.format('Михаил'))\n",
    "\n",
    "# Some predefined functions that show content related information for given words\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NullHandler', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_matutils', 'corpora', 'downloader', 'interfaces', 'logger', 'logging', 'matutils', 'models', 'parsing', 'scripts', 'similarities', 'summarization', 'test', 'topic_coherence', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(dir(gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5\n"
     ]
    }
   ],
   "source": [
    "def gensim_sim(title, body)\n",
    "\n",
    "\n",
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "             \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "             \"Legend has it that the mind is a mad monkey.\",\n",
    "            \"I make my own fun.\"]\n",
    "print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'m\", 'taking', 'the', 'show', 'on', 'the', 'road', '.'], ['my', 'socks', 'are', 'a', 'force', 'multiplier', '.'], ['i', 'am', 'the', 'barber', 'who', 'cuts', 'everyone', \"'s\", 'hair', 'who', 'does', \"n't\", 'cut', 'their', 'own', '.'], ['legend', 'has', 'it', 'that', 'the', 'mind', 'is', 'a', 'mad', 'monkey', '.'], ['i', 'make', 'my', 'own', 'fun', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in raw_documents]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_sim(title, body):\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] for text in body]\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    print(dictionary)\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    sims = gensim.similarities.Similarity(None,tf_idf[corpus],num_features=len(dictionary))\n",
    "    query_doc = [w.lower() for w in word_tokenize(title)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "    return np.mean(sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(341 unique tokens: [\"'s\", ',', '.', 'about', 'anyone']...)\n",
      "Dictionary(420 unique tokens: [\"'s\", '(', ')', '.', 'always']...)\n",
      "Dictionary(311 unique tokens: ['.', '1', 'another', 'artist', 'is']...)\n",
      "Dictionary(289 unique tokens: ['.', 'a', 'are', 'for', 'getting']...)\n",
      "Dictionary(569 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(165 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(65 unique tokens: [',', '.', 'a', 'and', 'barrier']...)\n",
      "Dictionary(64 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(281 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(706 unique tokens: [',', '.', 'a', 'and', 'are']...)\n",
      "Dictionary(49 unique tokens: [\"''\", ',', '--', '.', '``']...)\n",
      "Dictionary(547 unique tokens: ['(', ')', ',', ':', 'beach']...)\n",
      "Dictionary(120 unique tokens: [',', '.', '2014', 'adherents', 'aiding']...)\n",
      "Dictionary(8 unique tokens: ['.', 'are', 'available', 'currently', 'no']...)\n",
      "Dictionary(437 unique tokens: [',', '.', 'a', 'administration', 'allies']...)\n",
      "Dictionary(183 unique tokens: [\"''\", '.', '``', 'a', 'akira']...)\n",
      "Dictionary(476 unique tokens: ['.', '3', 'around', 'black', 'did']...)\n",
      "Dictionary(134 unique tokens: [\"'\", '.', 'a', 'and', 'boston']...)\n",
      "Dictionary(352 unique tokens: [\"'s\", ',', '.', '22', 'and']...)\n",
      "Dictionary(297 unique tokens: [',', '.', 'a', 'acknowledged', 'administration']...)\n",
      "Dictionary(277 unique tokens: [',', '.', '14', '27-year-old', 'a']...)\n",
      "Dictionary(91 unique tokens: ['.', 'a', 'captured', 'circles', 'crop']...)\n",
      "Dictionary(30 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(19 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(279 unique tokens: [',', '.', 'a', 'according', 'aiding']...)\n",
      "Dictionary(404 unique tokens: [',', 'are', 'as', 'attacks', 'barbaric']...)\n",
      "Dictionary(68 unique tokens: [',', '.', ':', 'a', 'and']...)\n",
      "Dictionary(228 unique tokens: ['(', ')', ',', '.', '9525']...)\n",
      "Dictionary(478 unique tokens: [',', '.', '7-year-old', '90s', 'a']...)\n",
      "Dictionary(255 unique tokens: [',', '.', 'a', 'announce', 'any']...)\n",
      "Dictionary(337 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(94 unique tokens: ['.', 'a', 'batten', 'before', 'both']...)\n",
      "Dictionary(518 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(59 unique tokens: [',', '.', 'about', 'advantage', 'as']...)\n",
      "Dictionary(419 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(470 unique tokens: [',', '.', '1903', 'a', 'almost']...)\n",
      "Dictionary(252 unique tokens: [\"'re\", '?', 'do', 'far', 'how']...)\n",
      "Dictionary(216 unique tokens: [',', '.', '114-year-old', 'a', 'about']...)\n",
      "Dictionary(456 unique tokens: [\"''\", \"'s\", ',', '.', ':']...)\n",
      "Dictionary(132 unique tokens: [\"'s\", '.', 'a', 'agency', 'and']...)\n",
      "Dictionary(153 unique tokens: [',', '.', 'a', 'and', 'been']...)\n",
      "Dictionary(65 unique tokens: [',', '.', '2', 'a', 'after']...)\n",
      "Dictionary(62 unique tokens: ['.', 'aka', 'd', 'death', 'group']...)\n",
      "Dictionary(309 unique tokens: ['a', 'article', 'by', 'column', 'editor']...)\n",
      "Dictionary(395 unique tokens: [',', '.', 'and', 'announce', 'appear']...)\n",
      "Dictionary(270 unique tokens: [',', '.', 'an', 'authorities', 'blind']...)\n",
      "Dictionary(208 unique tokens: ['.', 'a', 'allow', 'be', 'by']...)\n",
      "Dictionary(495 unique tokens: ['.', 'after', 'contact', 'daily', 'dramatically']...)\n",
      "Dictionary(104 unique tokens: [',', '.', 'a', 'and', 'called']...)\n",
      "Dictionary(555 unique tokens: [\"'s\", ',', '.', 'a', 'again']...)\n",
      "Dictionary(173 unique tokens: [',', '.', '12', 'and', 'animals']...)\n",
      "Dictionary(202 unique tokens: ['$', ',', '.', '200', '5']...)\n",
      "Dictionary(52 unique tokens: ['$', ',', '.', '141', 'a']...)\n",
      "Dictionary(229 unique tokens: [',', 'a', 'as', 'at', 'back']...)\n",
      "Dictionary(153 unique tokens: ['counterparts', 'finds', 'health', 'married', 'men']...)\n",
      "Dictionary(50 unique tokens: [\"'s\", '.', 'a', 'airport', 'appears']...)\n",
      "Dictionary(185 unique tokens: [\"'\", \"'quadrillions\", \"'s\", '(', ')']...)\n",
      "Dictionary(181 unique tokens: [\"''\", \"'m\", \"'s\", ',', '.']...)\n",
      "Dictionary(98 unique tokens: [\"'s\", '?', 'handshake', 'it', 'kind']...)\n",
      "Dictionary(215 unique tokens: [\"'s\", ',', '--', '.', ':']...)\n",
      "Dictionary(512 unique tokens: ['.', 'a', 'blair', 'build', 'business']...)\n",
      "Dictionary(232 unique tokens: [',', '.', 'as', 'change', 'climate']...)\n",
      "Dictionary(611 unique tokens: [',', '.', 'adults', 'an', 'and']...)\n",
      "Dictionary(111 unique tokens: ['.', '2017', '3rd', 'app', 'april']...)\n",
      "Dictionary(774 unique tokens: ['!', '?', 'conditions', 'do', 'how']...)\n",
      "Dictionary(524 unique tokens: [',', '.', '1,000', 'a', 'adventurous']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'a', 'also', 'americans']...)\n",
      "Dictionary(224 unique tokens: [',', '.', 'and', 'back', 'be']...)\n",
      "Dictionary(108 unique tokens: [',', '.', 'a', 'at', 'ca']...)\n",
      "Dictionary(409 unique tokens: [',', '.', '14', '4', 'a']...)\n",
      "Dictionary(399 unique tokens: [',', '.', '9/11', 'a', 'about']...)\n",
      "Dictionary(83 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(424 unique tokens: [',', '--', '.', 'ahead', 'and']...)\n",
      "Dictionary(80 unique tokens: ['.', 'by', 'far', 'is', 'season']...)\n",
      "Dictionary(524 unique tokens: [',', '.', '0', '10', 'a']...)\n",
      "Dictionary(235 unique tokens: [':', 'breslouer', 'by', 'lee', \"'s\"]...)\n",
      "Dictionary(590 unique tokens: [',', '.', '3', '96-91', 'a']...)\n",
      "Dictionary(382 unique tokens: ['pictures', 'universal', \"''\", ',', '.']...)\n",
      "Dictionary(450 unique tokens: [',', '.', 'a', 'after', 'an']...)\n",
      "Dictionary(712 unique tokens: [\"'s\", ',', '.', 'across', 'at']...)\n",
      "Dictionary(123 unique tokens: ['.', '2', '2015', 'a', 'about']...)\n",
      "Dictionary(235 unique tokens: [',', '67p', 'comet', 'craft', 'distant']...)\n",
      "Dictionary(241 unique tokens: [\"'\", \"'i\", \"'s\", '...', '.but']...)\n",
      "Dictionary(49 unique tokens: [\"''\", ',', '--', '.', '``']...)\n",
      "Dictionary(541 unique tokens: [',', '.', 'a', 'aesthetes', 'afflicted']...)\n",
      "Dictionary(105 unique tokens: ['a', 'are', 'been', 'belgian', 'by']...)\n",
      "Dictionary(314 unique tokens: ['(', ')', ',', '.', 'actor']...)\n",
      "Dictionary(237 unique tokens: [\"'s\", ',', '.', '36,000', '?']...)\n",
      "Dictionary(153 unique tokens: [\"'re\", '.', '?', 'a', 'and']...)\n",
      "Dictionary(206 unique tokens: [\"'\", \"''\", '.', '3', '``']...)\n",
      "Dictionary(287 unique tokens: [\"'\", ',', '.', 'ancient', 'are']...)\n",
      "Dictionary(415 unique tokens: ['(', ')', 'and', 'art', 'concept']...)\n",
      "Dictionary(419 unique tokens: [',', '.', '2016', 'a', 'about']...)\n",
      "Dictionary(509 unique tokens: ['?', 'a', 'becomes', 'copy', 'counterfeit']...)\n",
      "Dictionary(133 unique tokens: [',', '.', '2014', '60,000', '9']...)\n",
      "Dictionary(405 unique tokens: [',', '.', 'almost', 'and', 'comfy']...)\n",
      "Dictionary(115 unique tokens: ['.', 'a', 'and', 'camera', 'good']...)\n",
      "Dictionary(161 unique tokens: [',', '.', 'a', 'and', 'bottles']...)\n",
      "Dictionary(256 unique tokens: [\"'\", ',', '.', '100', 'and']...)\n",
      "Dictionary(324 unique tokens: [',', '.', 'a', 'about', 'according']...)\n",
      "Dictionary(325 unique tokens: ['!', \"'s\", '.', '22', 'appears']...)\n",
      "Dictionary(200 unique tokens: [',', '.', 'a', 'anthony', 'became']...)\n",
      "Dictionary(71 unique tokens: [',', '.', 'a', 'add', 'analysts']...)\n",
      "Dictionary(54 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(494 unique tokens: ['!', ',', '.', 'and', 'artwork']...)\n",
      "Dictionary(254 unique tokens: [',', '.', '61', 'a', 'arrests']...)\n",
      "Dictionary(2 unique tokens: ['featured', 'matches'])\n",
      "Dictionary(284 unique tokens: [',', 'a', 'air', 'ambition', 'and']...)\n",
      "Dictionary(88 unique tokens: ['.', 'about', 'bad', 'behavior', 'e.l.']...)\n",
      "Dictionary(176 unique tokens: ['-', '.', 'a', 'all', 'ca']...)\n",
      "Dictionary(579 unique tokens: [',', '.', 'a', 'administration', 'agency']...)\n",
      "Dictionary(132 unique tokens: ['.', 'call', 'coming', 'consoles', 'duty']...)\n",
      "Dictionary(183 unique tokens: [',', '.', '15', 'achieved', 'age']...)\n",
      "Dictionary(389 unique tokens: [',', '.', 'a', 'better', 'device']...)\n",
      "Dictionary(29 unique tokens: ['.', 'yaaaaas', '...', '?', 'come']...)\n",
      "Dictionary(75 unique tokens: [\"''\", ',', '.', '2015', '3']...)\n",
      "Dictionary(136 unique tokens: [\"''\", ',', '.', '24', '``']...)\n",
      "Dictionary(175 unique tokens: ['.', 'coming', 'empty-handed', 'keeps', 'tesla']...)\n",
      "Dictionary(335 unique tokens: ['-', '.', 'a', 'about', 'after']...)\n",
      "Dictionary(177 unique tokens: ['#', \"''\", ',', '.', ':']...)\n",
      "Dictionary(47 unique tokens: ['a', 'as', 'at', 'e', 'england']...)\n",
      "Dictionary(127 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(214 unique tokens: [',', '.', 'and', 'at', 'car']...)\n",
      "Dictionary(291 unique tokens: [\"'\", '(', ')', ',', '-']...)\n",
      "Dictionary(175 unique tokens: [',', '.', 'against', 'before', 'bernie']...)\n",
      "Dictionary(399 unique tokens: [',', '.', '2016', 'a', 'american']...)\n",
      "Dictionary(132 unique tokens: [',', '.', 'a', 'according', 'climbing']...)\n",
      "Dictionary(157 unique tokens: [',', 'and', 'be', 'but', 'come']...)\n",
      "Dictionary(282 unique tokens: [',', 'a', 'african', 'american', 'as']...)\n",
      "Dictionary(236 unique tokens: [\"'s\", ',', '.', '2015', 'a']...)\n",
      "Dictionary(121 unique tokens: ['.', 'a', 'after', 'an', 'and']...)\n",
      "Dictionary(121 unique tokens: [',', '.', '17', 'at', 'cheer']...)\n",
      "Dictionary(251 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(147 unique tokens: [',', '.', 'a', 'and', 'angry']...)\n",
      "Dictionary(338 unique tokens: ['amid', 'approves', 'by', 'computerised', 'drivers']...)\n",
      "Dictionary(437 unique tokens: ['!', ',', '.', 'an', 'as']...)\n",
      "Dictionary(239 unique tokens: [\"'s\", ',', '.', 'areas', 'been']...)\n",
      "Dictionary(250 unique tokens: [\"'s\", ',', '.', '40', 'a']...)\n",
      "Dictionary(154 unique tokens: ['$', \"'s\", '.', '1', 'a']...)\n",
      "Dictionary(72 unique tokens: [',', '.', ':', 'a', 'an']...)\n",
      "Dictionary(256 unique tokens: ['against', 'featurepics.com', 'in', 'is', 'move']...)\n",
      "Dictionary(968 unique tokens: [':', 'cocotos', 'for', 'illustration', 'news']...)\n",
      "Dictionary(206 unique tokens: [',', '.', 'a', 'against', 'all']...)\n",
      "Dictionary(391 unique tokens: ['.', 'a', 'after', 'attacked', 'cornering']...)\n",
      "Dictionary(17 unique tokens: ['!', '.', 'a', 'don', 'ending']...)\n",
      "Dictionary(101 unique tokens: [',', ':', 'a', 'and', 'baked']...)\n",
      "Dictionary(82 unique tokens: ['.', 'for', 'great', 'in', 'mountains']...)\n",
      "Dictionary(153 unique tokens: [\"'s\", ',', '.', '/r/fatpeoplehate', '/r/hamplanethatred']...)\n",
      "Dictionary(559 unique tokens: ['!', ',', 'a', 'dazzling', 'i']...)\n",
      "Dictionary(548 unique tokens: [',', '.', ':', 'a', 'an']...)\n",
      "Dictionary(396 unique tokens: [',', '.', 'a', 'aggression', 'allies']...)\n",
      "Dictionary(256 unique tokens: ['against', 'case', 'chief', 'china', 'communist']...)\n",
      "Dictionary(199 unique tokens: [\"'\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(102 unique tokens: [\"'s\", '.', ':', 'a', 'about']...)\n",
      "Dictionary(539 unique tokens: [',', '.', '1927', '8', 'and']...)\n",
      "Dictionary(58 unique tokens: [',', '.', 'as', 'budget', 'continued']...)\n",
      "Dictionary(353 unique tokens: ['(', ')', '.', 'a', 'agency']...)\n",
      "Dictionary(126 unique tokens: ['.', '23.5', '6.5', 'averaging', 'cavaliers']...)\n",
      "Dictionary(164 unique tokens: ['.', 'a', 'after', 'and', 'are']...)\n",
      "Dictionary(66 unique tokens: ['?', 'afraid', 'are', 'brown', 'of']...)\n",
      "Dictionary(188 unique tokens: [',', '.', 'father', 'like', 'son']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'a', 'across', 'and']...)\n",
      "Dictionary(173 unique tokens: ['.', '65-year-old', 'a', 'an', 'been']...)\n",
      "Dictionary(257 unique tokens: ['.', 'a', 'after', 'at', 'beagle']...)\n",
      "Dictionary(318 unique tokens: [\"'s\", ',', 'a', 'and', 'change']...)\n",
      "Dictionary(363 unique tokens: [',', 'a', 'an', 'away', 'but']...)\n",
      "Dictionary(276 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(118 unique tokens: [',', '.', 'allowed', 'anything', 'at']...)\n",
      "Dictionary(213 unique tokens: [',', '.', 'a', 'aide', 'as']...)\n",
      "Dictionary(419 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(431 unique tokens: [',', '.', '2015', '5,000', '50,000']...)\n",
      "Dictionary(162 unique tokens: [\"''\", ',', '--', '.', '``']...)\n",
      "Dictionary(333 unique tokens: [\"''\", '--', '.', '``', 'a']...)\n",
      "Dictionary(236 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(228 unique tokens: [\"'s\", ',', '.', '420', 'a']...)\n",
      "Dictionary(490 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(230 unique tokens: [',', '.', 'a', 'and', 'astrologers']...)\n",
      "Dictionary(257 unique tokens: ['.', '3-mile', 'a', 'ago', 'area']...)\n",
      "Dictionary(64 unique tokens: [',', '.', 'a', 'afternoon', 'an']...)\n",
      "Dictionary(54 unique tokens: [\"'s\", '.', '2016', 'addressing', 'campaign']...)\n",
      "Dictionary(495 unique tokens: [',', '.', '2014', 'a', 'all']...)\n",
      "Dictionary(726 unique tokens: [',', '.', 'a', 'argues', 'be']...)\n",
      "Dictionary(182 unique tokens: [',', '.', 'a', 'against', 'albert']...)\n",
      "Dictionary(547 unique tokens: [',', '.', 'a', 'and', 'dealt']...)\n",
      "Dictionary(106 unique tokens: [',', '.', 'a', 'brick', 'comes']...)\n",
      "Dictionary(524 unique tokens: [',', '.', 'a', 'ago', 'announced']...)\n",
      "Dictionary(132 unique tokens: ['allison', 'and', 'boice', 'by', 'jay']...)\n",
      "Dictionary(342 unique tokens: ['by', 'conway', 'richard', '5', 'bbc']...)\n",
      "Dictionary(108 unique tokens: ['a', 'accused', 'alan', 'appeared', 'as']...)\n",
      "Dictionary(65 unique tokens: [',', '.', '3,000-mile', 'a', 'after']...)\n",
      "Dictionary(198 unique tokens: [\"'s\", '.', 'a', 'after', 'an']...)\n",
      "Dictionary(170 unique tokens: [\"'\", '.', 'a', 'activists', 'affidavits']...)\n",
      "Dictionary(429 unique tokens: [',', '.', '22', ':', 'a']...)\n",
      "Dictionary(321 unique tokens: [',', '.', '6-year-old', ':', ';']...)\n",
      "Dictionary(75 unique tokens: [\"'re\", \"'s\", ',', '--', '.']...)\n",
      "Dictionary(605 unique tokens: [\"'s\", ',', '.', 'a', 'as']...)\n",
      "Dictionary(550 unique tokens: [',', '.', 'a', 'accused', 'and']...)\n",
      "Dictionary(291 unique tokens: [',', '.', '2010', 'be', 'been']...)\n",
      "Dictionary(320 unique tokens: [',', '.', 'a', 'around', 'arsenal']...)\n",
      "Dictionary(181 unique tokens: ['2', 'die', 'in', 'philly', 'police']...)\n",
      "Dictionary(25 unique tokens: [',', '.', 'a', 'asked', 'dogs']...)\n",
      "Dictionary(343 unique tokens: ['.', '?', 'a', 'asselin', 'changed']...)\n",
      "Dictionary(335 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(449 unique tokens: [',', '.', 'a', 'after', 'allowed']...)\n",
      "Dictionary(345 unique tokens: ['after', 'back', 'call', 'comes', 'divorce']...)\n",
      "Dictionary(257 unique tokens: ['.', '3-mile', 'a', 'ago', 'area']...)\n",
      "Dictionary(340 unique tokens: [',', '.', '1,000', 'a', 'across']...)\n",
      "Dictionary(176 unique tokens: [\"'s\", ',', '.', '21st', 'a']...)\n",
      "Dictionary(205 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(361 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(113 unique tokens: [',', '.', 'a', 'africa', 'after']...)\n",
      "Dictionary(523 unique tokens: [',', '.', 'acceptance', 'amphibious', 'and']...)\n",
      "Dictionary(259 unique tokens: [',', 'and', 'award', 'boyer', 'carey']...)\n",
      "Dictionary(169 unique tokens: [',', '.', '11-year-olds', 'a', 'achievement']...)\n",
      "Dictionary(227 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(450 unique tokens: [',', '.', 'a', 'acknowledged', 'also']...)\n",
      "Dictionary(401 unique tokens: ['activists', 'also', 'and', 'banned', 'country']...)\n",
      "Dictionary(286 unique tokens: [',', '.', 'a', 'and', 'capabilities']...)\n",
      "Dictionary(399 unique tokens: [',', '.', 'a', 'about', 'and']...)\n",
      "Dictionary(130 unique tokens: [',', '.', '2018', '2022', '2026']...)\n",
      "Dictionary(172 unique tokens: [\"'s\", ',', '.', 'bars', 'behind']...)\n",
      "Dictionary(195 unique tokens: ['.', 'a', 'an', 'apple', 'bacon']...)\n",
      "Dictionary(298 unique tokens: [\"'\", \"''\", \"'s\", '(', ')']...)\n",
      "Dictionary(60 unique tokens: [',', '.', 'a', 'and', 'commission']...)\n",
      "Dictionary(92 unique tokens: [\"'\", \"''\", ',', '.', '2015']...)\n",
      "Dictionary(497 unique tokens: [',', '.', 'a', 'britain', 'close']...)\n",
      "Dictionary(467 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(495 unique tokens: ['.', 'after', 'contact', 'daily', 'dramatically']...)\n",
      "Dictionary(312 unique tokens: ['.', 'battle', 'bbc3', 'biggest', 'british']...)\n",
      "Dictionary(51 unique tokens: ['.', 'a', 'cancers', 'colorectal', 'detect']...)\n",
      "Dictionary(386 unique tokens: ['.', 'alicia', 'assistant', 'been', 'cargile']...)\n",
      "Dictionary(94 unique tokens: [',', '.', 'a', 'after', 'against']...)\n",
      "Dictionary(198 unique tokens: ['.', 'american', 'dead', 'dream', 'is']...)\n",
      "Dictionary(108 unique tokens: ['day', 'fans', 'festival', 'four', 'night']...)\n",
      "Dictionary(117 unique tokens: ['#', '//t.co/0tg3xqitwl', '//t.co/gkn7idh28b', ':', 'a']...)\n",
      "Dictionary(385 unique tokens: [',', '.', '10', 'according', 'actual']...)\n",
      "Dictionary(358 unique tokens: [',', '.', 'a', 'across', 'allegedly']...)\n",
      "Dictionary(415 unique tokens: ['.', '?', 'a', 'abortion', 'accept']...)\n",
      "Dictionary(63 unique tokens: ['.', 'a', 'audience', 'big', 'colorado']...)\n",
      "Dictionary(425 unique tokens: [',', '.', 'acted', 'angeles', 'but']...)\n",
      "Dictionary(154 unique tokens: ['(', ')', '2.05', '2.40', '3.15']...)\n",
      "Dictionary(345 unique tokens: ['after', 'back', 'call', 'comes', 'divorce']...)\n",
      "Dictionary(113 unique tokens: [\"'\", ',', '.', 'a', 'all']...)\n",
      "Dictionary(265 unique tokens: ['?', 'a', 'actually', 'and', 'are']...)\n",
      "Dictionary(316 unique tokens: ['!', ',', '.', '?', 'a']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(184 unique tokens: ['by', 'foley', 'kaye', ',', '.']...)\n",
      "Dictionary(69 unique tokens: [',', '.', '?', 'a', 'and']...)\n",
      "Dictionary(422 unique tokens: ['!', '(', ')', ',', '.']...)\n",
      "Dictionary(749 unique tokens: [',', '.', '14', '73', 'and']...)\n",
      "Dictionary(215 unique tokens: [\"'s\", ',', '--', '.', ':']...)\n",
      "Dictionary(55 unique tokens: [',', '.', 'accident', 'an', 'between']...)\n",
      "Dictionary(149 unique tokens: ['.', '20.29sec', '200m', 'a', 'adidas']...)\n",
      "Dictionary(269 unique tokens: ['.', '13', 'a', 'belgium', 'doctors']...)\n",
      "Dictionary(114 unique tokens: [\"''\", ',', '.', ';', '``']...)\n",
      "Dictionary(171 unique tokens: [\"'s\", '(', ')', ',', '-']...)\n",
      "Dictionary(302 unique tokens: [\"'s\", ',', '.', 'a', 'airstrikes']...)\n",
      "Dictionary(241 unique tokens: ['#', ',', '.', 'and', 'archeologists']...)\n",
      "Dictionary(226 unique tokens: [\"'s\", '.', 'a', 'and', 'been']...)\n",
      "Dictionary(174 unique tokens: ['.', 'a', 'ago', 'as', 'declared']...)\n",
      "Dictionary(623 unique tokens: [',', '.', '?', 'after', 'and']...)\n",
      "Dictionary(447 unique tokens: ['and', 'as', 'aside', 'brushes', 'charge']...)\n",
      "Dictionary(351 unique tokens: ['.', 'a', 'after', 'as', 'been']...)\n",
      "Dictionary(319 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(81 unique tokens: ['.', 'actor', 'after', 'allegedly', 'and']...)\n",
      "Dictionary(407 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(54 unique tokens: ['.', 'a', 'bank', 'but', 'country']...)\n",
      "Dictionary(349 unique tokens: ['.', 'a', 'be', 'can', 'client']...)\n",
      "Dictionary(62 unique tokens: [',', '.', 'a', 'appeals', 'clearing']...)\n",
      "Dictionary(112 unique tokens: [\"'\", \"'s\", ',', '.', ';']...)\n",
      "Dictionary(193 unique tokens: ['.', 'a', 'always', 'crust', 'effort']...)\n",
      "Dictionary(229 unique tokens: [',', '.', '11th', 'a', 'an']...)\n",
      "Dictionary(52 unique tokens: [',', '.', 'before', 'bringing', 'cable-tv']...)\n",
      "Dictionary(416 unique tokens: [',', '.', 'a', 'blew', 'bomber']...)\n",
      "Dictionary(130 unique tokens: [',', '.', 'a', 'activity', 'at']...)\n",
      "Dictionary(432 unique tokens: ['.', 'a', 'abyad', 'and', 'barbed']...)\n",
      "Dictionary(1330 unique tokens: ['.', 'a', 'by', 'changsha', 'china']...)\n",
      "Dictionary(153 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(519 unique tokens: [',', '.', 'announce', 'at', 'baton']...)\n",
      "Dictionary(192 unique tokens: ['coming', 'countdown', 'countdownlbl', 'in', 'next']...)\n",
      "Dictionary(269 unique tokens: [',', '.', '16-year-old', ':', 'a']...)\n",
      "Dictionary(594 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(131 unique tokens: [\"'s\", ',', '.', 'a', 'amazon']...)\n",
      "Dictionary(312 unique tokens: [',', '.', '80-year-old', '87', 'additional']...)\n",
      "Dictionary(166 unique tokens: [',', '.', 'but', 'contraband', 'did']...)\n",
      "Dictionary(367 unique tokens: ['.', ':', 'above', 'advised', 'contains']...)\n",
      "Dictionary(350 unique tokens: [\"'\", \"'obscene\", ',', 'act', 'and']...)\n",
      "Dictionary(455 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(233 unique tokens: ['(', ')', '?', 'about', 'america']...)\n",
      "Dictionary(427 unique tokens: [',', '.', 'a', 'about', 'affect']...)\n",
      "Dictionary(229 unique tokens: [\"'s\", ',', '.', 'accustomed', 'all']...)\n",
      "Dictionary(216 unique tokens: [',', '.', '2016', 'against', 'anghel']...)\n",
      "Dictionary(117 unique tokens: ['.', 'been', 'cavaliers', 'cleveland', 'finals']...)\n",
      "Dictionary(111 unique tokens: [',', '.', ':', 'a', 'against']...)\n",
      "Dictionary(230 unique tokens: [',', '.', '2', 'a', 'admits']...)\n",
      "Dictionary(385 unique tokens: [',', '.', '10', 'according', 'actual']...)\n",
      "Dictionary(262 unique tokens: ['.', 'an', 'appears', 'broomfield', 'facebook']...)\n",
      "Dictionary(61 unique tokens: ['.', ':', 'an', 'are', 'awesome']...)\n",
      "Dictionary(381 unique tokens: [',', '.', 'a', 'and', 'as']...)\n",
      "Dictionary(240 unique tokens: ['.', 'a', 'dream', 'future', 'is']...)\n",
      "Dictionary(399 unique tokens: ['(', ')', ',', '.', 'across']...)\n",
      "Dictionary(347 unique tokens: [\"'s\", '.', 'a', 'anâ', 'barack']...)\n",
      "Dictionary(224 unique tokens: [',', '.', 'and', 'back', 'be']...)\n",
      "Dictionary(225 unique tokens: [',', '.', '12-year-old', 'a', 'acting']...)\n",
      "Dictionary(250 unique tokens: [',', '.', '?', 'a', 'a.']...)\n",
      "Dictionary(114 unique tokens: [\"''\", ',', '.', ';', '``']...)\n",
      "Dictionary(45 unique tokens: [\"''\", ',', '.', '``', 'additions']...)\n",
      "Dictionary(172 unique tokens: [\"'\", \"'structural\", 'about', 'concerned', 'creating']...)\n",
      "Dictionary(62 unique tokens: [',', '.', '1215', '20', 'a']...)\n",
      "Dictionary(198 unique tokens: [\"'s\", '.', 'a', 'and', 'college']...)\n",
      "Dictionary(199 unique tokens: ['12', 'a', 'alcohol', 'basis', 'cent']...)\n",
      "Dictionary(436 unique tokens: [\"'s\", ',', '.', 'a', 'act']...)\n",
      "Dictionary(270 unique tokens: ['by', 'dhani', 'mau', '.', ':']...)\n",
      "Dictionary(163 unique tokens: [',', '.', 'a', 'agency', 'and']...)\n",
      "Dictionary(80 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(254 unique tokens: [',', '.', '93', 'a', 'actor']...)\n",
      "Dictionary(8 unique tokens: ['.', 'are', 'available', 'currently', 'no']...)\n",
      "Dictionary(265 unique tokens: ['!', '(', ')', '.', '2016']...)\n",
      "Dictionary(534 unique tokens: ['.', 'a', 'after', 'allies', 'and']...)\n",
      "Dictionary(705 unique tokens: ['2-1', 'a', 'and', 'canada', 'carney']...)\n",
      "Dictionary(17 unique tokens: ['.', '3', 'after', 'beat', 'cavaliers']...)\n",
      "Dictionary(191 unique tokens: ['advertisement', ',', '09', '2015', 'business']...)\n",
      "Dictionary(321 unique tokens: [',', '.', 'a', 'all', 'at']...)\n",
      "Dictionary(15 unique tokens: [',', '.', 'a', 'at', 'career']...)\n",
      "Dictionary(133 unique tokens: [',', '.', '2014', '60,000', '9']...)\n",
      "Dictionary(395 unique tokens: [',', '.', '400', 'a', 'administration']...)\n",
      "Dictionary(111 unique tokens: ['.', 'a', 'easy', 'friend', 'goodbye']...)\n",
      "Dictionary(728 unique tokens: [',', 'a', 'and', 'area', 'between']...)\n",
      "Dictionary(269 unique tokens: ['.', 'a', 'accused', 'actually', 'address']...)\n",
      "Dictionary(103 unique tokens: [\"'\", ',', '--', '.', '2']...)\n",
      "Dictionary(670 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(360 unique tokens: [\"'s\", ',', '.', 'a', 'after']...)\n",
      "Dictionary(58 unique tokens: ['%', ',', '.', '80', 'a']...)\n",
      "Dictionary(716 unique tokens: ['.', 'kinabalu', 'life', 'mount', 'my']...)\n",
      "Dictionary(54 unique tokens: ['.', 'a', 'beach', 'captured', 'florida']...)\n",
      "Dictionary(374 unique tokens: [\"''\", \"'s\", '-like', '.', '48']...)\n",
      "Dictionary(128 unique tokens: [\"'s\", '.', 'a', 'been', 'boy']...)\n",
      "Dictionary(251 unique tokens: [',', '.', 'a', 'and', 'awesome']...)\n",
      "Dictionary(699 unique tokens: [',', '.', 'and', 'back', 'france']...)\n",
      "Dictionary(52 unique tokens: ['.', 'accounts', 'armies', 'banks', 'breaking']...)\n",
      "Dictionary(626 unique tokens: ['#', 'beautiful', 'izabellaxamana', 'pic.twitter.com/ychyl9voa2', 'rip']...)\n",
      "Dictionary(60 unique tokens: [',', '.', 'after', 'an', 'at']...)\n",
      "Dictionary(99 unique tokens: [',', '.', '24', 'ago', 'an']...)\n",
      "Dictionary(402 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(90 unique tokens: ['.', 'at', 'cavaliers', 'cleveland', 'go']...)\n",
      "Dictionary(284 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(477 unique tokens: [',', '.', '2013', 'a', 'and']...)\n",
      "Dictionary(318 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(244 unique tokens: [\"''\", ',', '.', '``', 'at']...)\n",
      "Dictionary(218 unique tokens: [',', '.', '2017', 'a', 'after']...)\n",
      "Dictionary(465 unique tokens: ['&', \"'\", \"'s\", '(', ')']...)\n",
      "Dictionary(187 unique tokens: ['.', 'a', 'after', 'and', 'are']...)\n",
      "Dictionary(258 unique tokens: ['.', '2015', 'a', 'against', 'and']...)\n",
      "Dictionary(365 unique tokens: ['.', 'and', 'are', 'budget', 'designs']...)\n",
      "Dictionary(288 unique tokens: ['.', 'a', 'about', 'and', 'articles']...)\n",
      "Dictionary(431 unique tokens: [',', '27', '3', 'after', 'and']...)\n",
      "Dictionary(212 unique tokens: [',', '.', ':', 'a', 'according']...)\n",
      "Dictionary(715 unique tokens: ['1958', 'a', 'and', 'appearance', 'at']...)\n",
      "Dictionary(307 unique tokens: [\"'s\", ',', '-', '.', '/']...)\n",
      "Dictionary(151 unique tokens: ['.', 'cv', 'on', 'put', 'this']...)\n",
      "Dictionary(235 unique tokens: ['by', 'shemilt', 'stephan', 'at', 'bbc']...)\n",
      "Dictionary(568 unique tokens: [',', '.', '93', 'a', 'actor']...)\n",
      "Dictionary(80 unique tokens: [',', '.', '80s', 'a', 'aaron']...)\n",
      "Dictionary(383 unique tokens: ['(', ')', '.', 'ap', 'his']...)\n",
      "Dictionary(205 unique tokens: ['.', 'a', 'after', 'classmate', 'day']...)\n",
      "Dictionary(289 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(154 unique tokens: [',', '.', ':', 'actually', 'away']...)\n",
      "Dictionary(275 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(418 unique tokens: ['67p/churyumov-gerasimenko', 'after', 'comet', 'contact', 'first']...)\n",
      "Dictionary(367 unique tokens: [\"''\", ',', '.', '30', ':']...)\n",
      "Dictionary(64 unique tokens: [',', 'good', 'morning', '.', '...']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(146 unique tokens: ['#', ',', '--', '.', 'a']...)\n",
      "Dictionary(529 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(376 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(124 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(97 unique tokens: [',', '.', 'a', 'and', 'athens']...)\n",
      "Dictionary(310 unique tokens: [\"'s\", ',', '.', '86', 'age']...)\n",
      "Dictionary(233 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(195 unique tokens: ['.', 'candidate', 'clinton', 'comes', 'democratic']...)\n",
      "Dictionary(157 unique tokens: ['circus', 'elephant', 'escaped', 'from', 'had']...)\n",
      "Dictionary(119 unique tokens: ['(', ')', ',', '.', 'and']...)\n",
      "Dictionary(105 unique tokens: ['--', '.', 'a', 'and', 'grab']...)\n",
      "Dictionary(412 unique tokens: [',', '.', '80', 'a', 'angeles']...)\n",
      "Dictionary(120 unique tokens: [\"'\", ',', '.', 'a', 'abuse']...)\n",
      "Dictionary(97 unique tokens: [',', '.', 'a', 'accepted', 'alexandra']...)\n",
      "Dictionary(350 unique tokens: ['(', ')', '-', '.', ':']...)\n",
      "Dictionary(576 unique tokens: ['(', ')', '.', 'a', 'aided']...)\n",
      "Dictionary(490 unique tokens: ['.', '12', 'a', 'after', 'also']...)\n",
      "Dictionary(210 unique tokens: [\"'s\", '.', 'and', 'artist', 'discovered']...)\n",
      "Dictionary(185 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(728 unique tokens: [',', '.', 'and', 'bar', 'beer']...)\n",
      "Dictionary(70 unique tokens: [',', '.', 'a', 'an', 'answer']...)\n",
      "Dictionary(64 unique tokens: [',', '.', 'a', 'afternoon', 'an']...)\n",
      "Dictionary(121 unique tokens: ['.', '83-year-old', 'a', 'an', 'at']...)\n",
      "Dictionary(105 unique tokens: ['--', '.', 'a', 'according', 'anyway']...)\n",
      "Dictionary(37 unique tokens: ['cleveland', ',', '.', 'coming', 'finals']...)\n",
      "Dictionary(308 unique tokens: [\"''\", ',', '.', '16', '2012']...)\n",
      "Dictionary(193 unique tokens: ['photo', 'view', \"''\", ',', '.']...)\n",
      "Dictionary(173 unique tokens: ['2016', 'and', 'april', 'concerns', 'failures']...)\n",
      "Dictionary(282 unique tokens: [\"'s\", ',', '.', '?', 'a']...)\n",
      "Dictionary(661 unique tokens: ['!', \"'s\", '.', '22', 'appears']...)\n",
      "Dictionary(198 unique tokens: [',', 'accused', 'hiding', 'infant', 'killing']...)\n",
      "Dictionary(302 unique tokens: [',', '.', 'a', 'and', 'are']...)\n",
      "Dictionary(468 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(395 unique tokens: ['.', 'almost', 'any', 'brits', 'but']...)\n",
      "Dictionary(553 unique tokens: [',', '.', '69th', 'a', 'about']...)\n",
      "Dictionary(235 unique tokens: [',', '67p', 'comet', 'craft', 'distant']...)\n",
      "Dictionary(19 unique tokens: ['#', 'drawyourswords', 'at', 'breaktheinternet', 'last']...)\n",
      "Dictionary(116 unique tokens: ['director', 'for', 'had', 'lumbering', 'no']...)\n",
      "Dictionary(77 unique tokens: [',', '.', '?', 'as', 'can']...)\n",
      "Dictionary(302 unique tokens: ['(', ')', ',', '.', '18']...)\n",
      "Dictionary(121 unique tokens: [\"'s\", '?', 'a', 'andrey', 'at']...)\n",
      "Dictionary(595 unique tokens: [\"''\", ',', '.', '``', 'aggression']...)\n",
      "Dictionary(350 unique tokens: ['.', '60', 'a', 'babies', 'been']...)\n",
      "Dictionary(345 unique tokens: ['after', 'back', 'call', 'comes', 'divorce']...)\n",
      "Dictionary(233 unique tokens: [',', '.', '22-year-old', 'a', 'and']...)\n",
      "Dictionary(236 unique tokens: ['2014', '40', 'funniest', 'of', 'the']...)\n",
      "Dictionary(272 unique tokens: ['(', ')', ':', 'as', 'black']...)\n",
      "Dictionary(122 unique tokens: [\"'\", '.', 'best', 'collins', 'former']...)\n",
      "Dictionary(205 unique tokens: [\"'\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(138 unique tokens: ['!', \"'s\", ',', '.', '11,927']...)\n",
      "Dictionary(269 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(262 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(126 unique tokens: [',', '?', 'a', 'actually', 'again']...)\n",
      "Dictionary(270 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(510 unique tokens: ['.', 'a', 'after', 'articles', 'at']...)\n",
      "Dictionary(285 unique tokens: ['.', '50', '9', 'across', 'all']...)\n",
      "Dictionary(105 unique tokens: [\"''\", ',', '.', '62', '``']...)\n",
      "Dictionary(154 unique tokens: ['$', \"'s\", '.', '1', 'a']...)\n",
      "Dictionary(75 unique tokens: [\"''\", ',', '``', 'a', 'age']...)\n",
      "Dictionary(214 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(360 unique tokens: ['.', 'a', 'answered', 'buzzfeed', 'diy']...)\n",
      "Dictionary(202 unique tokens: ['.', '2-0', 'a', 'back', 'campaign']...)\n",
      "Dictionary(57 unique tokens: [',', '.', 'a', 'allegedly', 'arrested']...)\n",
      "Dictionary(247 unique tokens: ['.', 'a', 'again', 'at', 'being']...)\n",
      "Dictionary(56 unique tokens: ['.', 'a', 'assassinated', 'back', 'being']...)\n",
      "Dictionary(78 unique tokens: ['%', '&', ',', '.', '25']...)\n",
      "Dictionary(316 unique tokens: [\"'s\", ',', '--', '.', 'alien-like']...)\n",
      "Dictionary(470 unique tokens: [\"'s\", \"'ve\", '(', ')', '.']...)\n",
      "Dictionary(491 unique tokens: [',', '.', 'and', 'another', 'be']...)\n",
      "Dictionary(668 unique tokens: ['(', ')', ',', ':', 'ap']...)\n",
      "Dictionary(414 unique tokens: [',', '--', '.', 'a', 'adrian']...)\n",
      "Dictionary(485 unique tokens: [',', '.', '30', 'a', 'an']...)\n",
      "Dictionary(277 unique tokens: [',', '.', '14', '27-year-old', 'a']...)\n",
      "Dictionary(121 unique tokens: [',', '.', 'a', 'after', 'an']...)\n",
      "Dictionary(644 unique tokens: [',', 'address', 'bitterly', 'budge', 'but']...)\n",
      "Dictionary(80 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(134 unique tokens: ['.', 'a', 'an', 'elderly', 'gone']...)\n",
      "Dictionary(465 unique tokens: ['.', 'a', 'around', 'bringing', 'from']...)\n",
      "Dictionary(606 unique tokens: ['at', 'best', 'ceremony', 'composer', 'emotional']...)\n",
      "Dictionary(330 unique tokens: [',', '.', '9525', 'a', 'because']...)\n",
      "Dictionary(436 unique tokens: ['.', '21', 'coaches', 'day', 'find']...)\n",
      "Dictionary(537 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(287 unique tokens: ['a', 'account', 'escapees', 'first-hand', 'from']...)\n",
      "Dictionary(539 unique tokens: [',', '.', '1927', '8', 'and']...)\n",
      "Dictionary(246 unique tokens: [\"''\", \"'re\", \"'s\", ',', '.']...)\n",
      "Dictionary(59 unique tokens: [\"'s\", ',', '.', 'a', 'add']...)\n",
      "Dictionary(492 unique tokens: ['.', 'about', 'is', 'post', 'probably']...)\n",
      "Dictionary(341 unique tokens: [',', '.', 'a', 'abandoned', 'after']...)\n",
      "Dictionary(1098 unique tokens: ['(', ')', ',', '.', 'and']...)\n",
      "Dictionary(379 unique tokens: [',', '2015', '8', '9th', 'american']...)\n",
      "Dictionary(577 unique tokens: [',', '.', 'activists', 'as', 'at']...)\n",
      "Dictionary(186 unique tokens: ['!', 'and', 'are', 'back', 'mulder']...)\n",
      "Dictionary(112 unique tokens: ['.', 'a', 'boy', 'dapper', 'gentlemanly']...)\n",
      "Dictionary(75 unique tokens: [\"'s\", '.', '2', 'after', 'arena']...)\n",
      "Dictionary(145 unique tokens: [\"''\", \"'s\", ',', '.', '145']...)\n",
      "Dictionary(726 unique tokens: [',', '.', 'a', 'argues', 'be']...)\n",
      "Dictionary(218 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(202 unique tokens: ['.', 'american', 'faster', 'few', 'for']...)\n",
      "Dictionary(133 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(467 unique tokens: ['?', 'a', 'and', 'bit', 'can']...)\n",
      "Dictionary(11 unique tokens: ['!', ',', 'bangkok', 'go', 'to']...)\n",
      "Dictionary(233 unique tokens: ['(', ')', ',', '-', '.']...)\n",
      "Dictionary(280 unique tokens: [\"'s\", ',', '.', 'an', 'campaign']...)\n",
      "Dictionary(336 unique tokens: [\"'s\", ',', '.', '1', 'although']...)\n",
      "Dictionary(90 unique tokens: [',', '.', 'a', 'able', 'and']...)\n",
      "Dictionary(365 unique tokens: [',', '.', 'a', 'affairs', 'and']...)\n",
      "Dictionary(235 unique tokens: [\"''\", ',', '.', 'a', 'at']...)\n",
      "Dictionary(252 unique tokens: [\"'re\", '?', 'do', 'far', 'how']...)\n",
      "Dictionary(519 unique tokens: [',', '.', ':', 'about', 'an']...)\n",
      "Dictionary(220 unique tokens: [',', '.', '69', 'a', 'age']...)\n",
      "Dictionary(179 unique tokens: ['.', 'a', 'aimed', 'at', 'been']...)\n",
      "Dictionary(108 unique tokens: [\"'s\", ',', '.', 'advice', 'barack']...)\n",
      "Dictionary(321 unique tokens: [',', 'a', 'area', 'as', 'causes']...)\n",
      "Dictionary(238 unique tokens: [\"'\", \"'break\", '.', 'attempted', 'back']...)\n",
      "Dictionary(685 unique tokens: [',', '.', '15-year-old', 'a', 'about']...)\n",
      "Dictionary(72 unique tokens: [\"'ll\", \"'s\", ',', '.', 'at']...)\n",
      "Dictionary(162 unique tokens: ['all', 'and', 'barred', 'club', 'correspondents']...)\n",
      "Dictionary(956 unique tokens: [',', '.', 'a', 'according', 'against']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(246 unique tokens: [',', '.', 'a', 'agenda', 'and']...)\n",
      "Dictionary(149 unique tokens: ['.', 'a', 'avalanche', 'emerged', 'escaping']...)\n",
      "Dictionary(209 unique tokens: ['.', 'a', 'after', 'broke', 'dave']...)\n",
      "Dictionary(148 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(166 unique tokens: [',', '.', '2007', 'an', 'and']...)\n",
      "Dictionary(77 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(708 unique tokens: [\"''\", ',', '.', '27', '``']...)\n",
      "Dictionary(181 unique tokens: [\"'re\", ',', '--', '.', '3-d']...)\n",
      "Dictionary(245 unique tokens: ['.', 'as', 'chief', 'dorsey', 'executive']...)\n",
      "Dictionary(57 unique tokens: ['.', '11th-hour', 'a', 'agenda', 'ahead']...)\n",
      "Dictionary(317 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(1072 unique tokens: ['.', 'a', 'azalea', 'done', 'fork']...)\n",
      "Dictionary(124 unique tokens: ['.', 'a', 'captured', 'close-up', 'footage']...)\n",
      "Dictionary(339 unique tokens: ['.', ';', 'an', 'ancient', 'glamour']...)\n",
      "Dictionary(215 unique tokens: [\"'s\", ',', '.', 'a', 'all']...)\n",
      "Dictionary(247 unique tokens: ['.', ':', 'about', 'and', 'at']...)\n",
      "Dictionary(62 unique tokens: ['$', ',', '.', '3,074', 'a']...)\n",
      "Dictionary(353 unique tokens: [',', '.', 'a', 'and', 'arthur']...)\n",
      "Dictionary(752 unique tokens: [\"''\", \"'s\", ',', '.', '38,000-word']...)\n",
      "Dictionary(138 unique tokens: [\"'s\", ',', '.', 'a', 'against']...)\n",
      "Dictionary(385 unique tokens: [',', '.', '40', 'a', 'about']...)\n",
      "Dictionary(41 unique tokens: [\"'s\", ',', '.', '9', 'a']...)\n",
      "Dictionary(97 unique tokens: [\"'s\", ',', '.', '?', 'abroad']...)\n",
      "Dictionary(483 unique tokens: ['a', 'artist', 'bexhill-on-sea', 'britain', 'curve']...)\n",
      "Dictionary(700 unique tokens: [',', '.', ':', 'a', 'already']...)\n",
      "Dictionary(203 unique tokens: ['.', 'a', 'after', 'alive', 'baby']...)\n",
      "Dictionary(135 unique tokens: [',', '.', 'at', 'been', 'china']...)\n",
      "Dictionary(309 unique tokens: [',', '.', 'a', 'at', 'become']...)\n",
      "Dictionary(246 unique tokens: ['.', 'a', 'agent', 'better', 'flights']...)\n",
      "Dictionary(543 unique tokens: [',', '.', 'all', 'all-round', 'although']...)\n",
      "Dictionary(526 unique tokens: ['(', ')', 'apple', 'courtesy', 'of']...)\n",
      "Dictionary(6 unique tokens: ['abc', 'coverage', 'from', 'live', 'news']...)\n",
      "Dictionary(288 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(60 unique tokens: ['%', ',', '.', '25', 'ag']...)\n",
      "Dictionary(34 unique tokens: ['.', 'as', 'do', 'you', 'instagram']...)\n",
      "Dictionary(270 unique tokens: [\"'s\", ',', '.', 'america', 'annihilate']...)\n",
      "Dictionary(34 unique tokens: [',', '.', 'analysis', 'and', 'andrew']...)\n",
      "Dictionary(142 unique tokens: ['$', ',', '.', '2', '315,829']...)\n",
      "Dictionary(356 unique tokens: [\"''\", '.', '``', 'a', 'around']...)\n",
      "Dictionary(101 unique tokens: [\"'s\", ',', '.', 'a', 'adorable']...)\n",
      "Dictionary(1016 unique tokens: [',', '.', 'about', 'airbnb', 'alone']...)\n",
      "Dictionary(115 unique tokens: ['.', 'a', 'and', 'camera', 'good']...)\n",
      "Dictionary(395 unique tokens: [',', '.', 'a', 'all', 'an']...)\n",
      "Dictionary(189 unique tokens: [\"''\", ',', '--', '.', '``']...)\n",
      "Dictionary(481 unique tokens: [\"'s\", ',', '.', '24-year-old', 'a']...)\n",
      "Dictionary(384 unique tokens: [',', '.', 'an', 'and', 'another']...)\n",
      "Dictionary(532 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(188 unique tokens: [',', '.', '1', 'a', 'all']...)\n",
      "Dictionary(594 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(431 unique tokens: [',', '.', 'about', 'again', 'and']...)\n",
      "Dictionary(234 unique tokens: [\"'s\", ',', '17', 'after', 'boy']...)\n",
      "Dictionary(230 unique tokens: [\"'d\", ',', '.', '16-year-old', '2-1']...)\n",
      "Dictionary(96 unique tokens: [\"'s\", ',', '.', \"2's\", 'although']...)\n",
      "Dictionary(297 unique tokens: ['.', 'and', 'are', 'brother', 'carrabino']...)\n",
      "Dictionary(389 unique tokens: [',', '.', 'a', 'and', 'approach']...)\n",
      "Dictionary(454 unique tokens: ['.', 'boyfriend', 'calvin', 'first', 'harris']...)\n",
      "Dictionary(173 unique tokens: [',', '--', '.', 'a', 'and']...)\n",
      "Dictionary(370 unique tokens: ['(', ')', ',', '.', '2016']...)\n",
      "Dictionary(30 unique tokens: [\"'\", \"'s\", '(', ')', '.']...)\n",
      "Dictionary(236 unique tokens: ['.', 'a', 'and', 'barack', 'care']...)\n",
      "Dictionary(316 unique tokens: [',', 'and', 'archaeologists', 'at', 'been']...)\n",
      "Dictionary(34 unique tokens: ['!', \"'s\", '.', '2016', 'and']...)\n",
      "Dictionary(202 unique tokens: ['a', 'atlaoui', 'began', 'convicts', 'face']...)\n",
      "Dictionary(226 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(732 unique tokens: [':', 'american', 'horror', 'kitchen…', 'on']...)\n",
      "Dictionary(202 unique tokens: [',', '.', 'a', 'aggressive', 'an']...)\n",
      "Dictionary(187 unique tokens: ['$', \"'s\", ',', '.', '10-year']...)\n",
      "Dictionary(406 unique tokens: [',', '--', '.', 'after', 'are']...)\n",
      "Dictionary(293 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(287 unique tokens: [',', 'a', 'after', 'america', 'americans']...)\n",
      "Dictionary(110 unique tokens: [\"'s\", ',', '.', 'a', 'administered']...)\n",
      "Dictionary(154 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(237 unique tokens: [',', '.', 'a', 'before', 'broomfield']...)\n",
      "Dictionary(214 unique tokens: ['.', 'after', 'and', 'broken', 'concert']...)\n",
      "Dictionary(19 unique tokens: [',', '.', 'good', 'got', 'men']...)\n",
      "Dictionary(178 unique tokens: [\"'s\", '.', 'a', 'baz', 'break']...)\n",
      "Dictionary(157 unique tokens: ['.', 'a', 'at', 'battled', 'black']...)\n",
      "Dictionary(309 unique tokens: [',', '.', 'a', 'and', 'as']...)\n",
      "Dictionary(443 unique tokens: [',', '.', '1990s', ':', 'a']...)\n",
      "Dictionary(260 unique tokens: ['(', ')', ',', '-', '.']...)\n",
      "Dictionary(306 unique tokens: [\"'s\", ',', '.', 'a', 'ago']...)\n",
      "Dictionary(236 unique tokens: ['an', 'and', 'anupe', 'at', 'bullied']...)\n",
      "Dictionary(80 unique tokens: ['.', 'a', 'bay', 'been', 'building']...)\n",
      "Dictionary(252 unique tokens: [',', '.', 'a', 'at', 'authorities']...)\n",
      "Dictionary(968 unique tokens: [':', 'cocotos', 'for', 'illustration', 'news']...)\n",
      "Dictionary(623 unique tokens: [',', '.', '16', '193', 'and']...)\n",
      "Dictionary(502 unique tokens: ['$', '(', ')', ',', '.']...)\n",
      "Dictionary(23 unique tokens: [',', '.', 'by', 'cities', 'congested']...)\n",
      "Dictionary(518 unique tokens: [',', '.', '1978', 'a', 'after']...)\n",
      "Dictionary(322 unique tokens: ['(', ')', '.', 'and', 'bucks']...)\n",
      "Dictionary(207 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(324 unique tokens: [',', '.', '2009', '2011', 'a']...)\n",
      "Dictionary(385 unique tokens: [',', '-directed', '.', ':', 'a']...)\n",
      "Dictionary(131 unique tokens: [',', '.', 'a', 'abu', 'across']...)\n",
      "Dictionary(1109 unique tokens: ['.', '2017', 'a', 'draft', 'first']...)\n",
      "Dictionary(359 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(225 unique tokens: [',', 'business', 'davis', 'insider', 'scott']...)\n",
      "Dictionary(264 unique tokens: ['.', 'call', 'curse', 'hare', 'it']...)\n",
      "Dictionary(113 unique tokens: [\"'s\", '.', '//t.co/zzpabfywm9', ':', 'a']...)\n",
      "Dictionary(469 unique tokens: ['.', 'a', 'appears', 'be', 'border']...)\n",
      "Dictionary(237 unique tokens: ['.', 'adviser', 'by', 'disclosing', 'flynn']...)\n",
      "Dictionary(229 unique tokens: ['.', '1', 'adhuna', 'after', 'akhtar']...)\n",
      "Dictionary(428 unique tokens: ['.', '2000', 'arkansas', 'carried', 'concerns']...)\n",
      "Dictionary(504 unique tokens: ['.', 'am', 'anymore', 'i', 'not']...)\n",
      "Dictionary(149 unique tokens: [\"'s\", ',', '.', 'allman', 'at']...)\n",
      "Dictionary(243 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(77 unique tokens: [',', '.', 'a', 'affluent', 'america']...)\n",
      "Dictionary(315 unique tokens: ['more', '.', 'about', 'all', 'demi']...)\n",
      "Dictionary(668 unique tokens: ['firefox', 'news', 'on', 'try', 'yahoo']...)\n",
      "Dictionary(299 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(153 unique tokens: [',', '.', '434', 'according', 'against']...)\n",
      "Dictionary(525 unique tokens: [',', '.', 'about', 'action', 'against']...)\n",
      "Dictionary(59 unique tokens: ['-', '.', '36-year-old', 'a', 'ago']...)\n",
      "Dictionary(118 unique tokens: ['(', ')', ',', '.', '15.8']...)\n",
      "Dictionary(180 unique tokens: ['420', 'activists', 'annual', 'as', 'big']...)\n",
      "Dictionary(2065 unique tokens: ['.', 'an', 'are', 'easy', 'football']...)\n",
      "Dictionary(259 unique tokens: ['$', ',', '.', '200,000', '2010']...)\n",
      "Dictionary(14 unique tokens: [\"'s\", '.', '?', 'canada', 'daniel']...)\n",
      "Dictionary(298 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(568 unique tokens: [',', '.', ':', 'an', 'ancient']...)\n",
      "Dictionary(516 unique tokens: [',', '.', 'a', 'add', 'an']...)\n",
      "Dictionary(310 unique tokens: [\"'\", \"'as\", ',', 'a', 'clear']...)\n",
      "Dictionary(268 unique tokens: ['.', 'a', 'airlines', 'closer', 'eye']...)\n",
      "Dictionary(588 unique tokens: [',', '.', 'a', 'afternoon', 'ago']...)\n",
      "Dictionary(178 unique tokens: [',', '.', '8', 'a', 'an']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(768 unique tokens: ['firefox', 'news', 'on', 'try', 'yahoo']...)\n",
      "Dictionary(199 unique tokens: [',', '.', 'according', 'bill', 'coach']...)\n",
      "Dictionary(318 unique tokens: [',', '.', 'a', 'after', 'an']...)\n",
      "Dictionary(260 unique tokens: [\"'\", \"'kicked\", 'air', 'and', 'battles']...)\n",
      "Dictionary(257 unique tokens: [',', '--', '.', '2012', 'a']...)\n",
      "Dictionary(317 unique tokens: ['.', 'a', 'an', 'apparent', 'back']...)\n",
      "Dictionary(215 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(225 unique tokens: [\"'re\", \"'s\", ',', '.', 'a']...)\n",
      "Dictionary(22 unique tokens: [',', '/30-under-30-asia/2017/consumer-technology', '2017', '30', ':']...)\n",
      "Dictionary(366 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(177 unique tokens: [',', '.', 'a', 'agree', 'all']...)\n",
      "Dictionary(354 unique tokens: [',', '.', 'a', 'and', 'at']...)\n",
      "Dictionary(300 unique tokens: [',', '.', 'along', 'at', 'foreign']...)\n",
      "Dictionary(100 unique tokens: [',', 'attract', 'bright', 'britain', 'ever']...)\n",
      "Dictionary(2606 unique tokens: ['!', \"'s\", '.', '?', 'and']...)\n",
      "Dictionary(269 unique tokens: [\"''\", ',', '.', '``', 'adolf']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'a', 'altercation', 'an']...)\n",
      "Dictionary(142 unique tokens: [',', '.', 'a', 'are', 'bobby']...)\n",
      "Dictionary(216 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(391 unique tokens: ['--', '.', 'act', 'administration', 'against']...)\n",
      "Dictionary(340 unique tokens: [',', '.', '2', 'a', 'against']...)\n",
      "Dictionary(504 unique tokens: [',', 'a', 'across', 'and', 'archaeologists']...)\n",
      "Dictionary(188 unique tokens: [\"'\", \"'crazy\", ',', '.', 'a']...)\n",
      "Dictionary(153 unique tokens: ['!', \"''\", '(', ')', ':']...)\n",
      "Dictionary(187 unique tokens: [',', '.', 'according', 'advisers', 'and']...)\n",
      "Dictionary(169 unique tokens: [\"'s\", ',', '.', '2017', 'a']...)\n",
      "Dictionary(359 unique tokens: [',', 'activities', 'altering', 'are', 'extreme']...)\n",
      "Dictionary(305 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(388 unique tokens: [',', '.', '27', 'a', 'affront']...)\n",
      "Dictionary(314 unique tokens: [',', '.', 'a', 'and', 'bannon']...)\n",
      "Dictionary(240 unique tokens: [\"''\", \"'s\", ',', '--', '.']...)\n",
      "Dictionary(246 unique tokens: ['.', 'and', 'bad', 'be', 'conditions']...)\n",
      "Dictionary(214 unique tokens: [',', '.', '5-7', 'act', 'and']...)\n",
      "Dictionary(532 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(198 unique tokens: [\"'s\", ',', '.', 'a', 'against']...)\n",
      "Dictionary(151 unique tokens: [',', '.', 'a', 'america', 'an']...)\n",
      "Dictionary(45 unique tokens: [',', '.', 'a', 'according', 'candidate']...)\n",
      "Dictionary(467 unique tokens: [',', '.', 'a', 'and', 'are']...)\n",
      "Dictionary(321 unique tokens: [',', '.', 'a', 'airport', 'and']...)\n",
      "Dictionary(103 unique tokens: ['(', ')', ',', '.', '20']...)\n",
      "Dictionary(68 unique tokens: [\"''\", '.', '``', 'autism', 'awareness']...)\n",
      "Dictionary(457 unique tokens: [',', 'attitudes', 'change', 'cousin', 'experts']...)\n",
      "Dictionary(230 unique tokens: ['.', 'accountants', 'allowed', 'angeles', 'backstage']...)\n",
      "Dictionary(53 unique tokens: ['$', ',', '.', '630,000', 'a']...)\n",
      "Dictionary(458 unique tokens: [',', '.', ':', 'a', 'administration']...)\n",
      "Dictionary(518 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(192 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(451 unique tokens: ['more', ',', '.', '24', ':']...)\n",
      "Dictionary(364 unique tokens: ['(', ')', ',', '.', 'affectionately']...)\n",
      "Dictionary(296 unique tokens: [',', '.', '10', 'all', 'are']...)\n",
      "Dictionary(202 unique tokens: ['.', 'a', 'after', 'and', 'bus']...)\n",
      "Dictionary(98 unique tokens: [',', 'business', 'cork', 'gaines', 'insider']...)\n",
      "Dictionary(188 unique tokens: ['.', 'a', 'act', 'alive', 'americans']...)\n",
      "Dictionary(227 unique tokens: [',', '.', '2008', '31-year-old', 'a']...)\n",
      "Dictionary(211 unique tokens: [\"'s\", ',', '.', '192m', '210m']...)\n",
      "Dictionary(168 unique tokens: [\"'re\", ',', '.', 'a', 'about']...)\n",
      "Dictionary(257 unique tokens: [',', '.', 'a', 'as', 'blasted']...)\n",
      "Dictionary(75 unique tokens: [',', '--', '.', '40,000', 'a']...)\n",
      "Dictionary(109 unique tokens: [',', '.', '2010', 'an', 'believes']...)\n",
      "Dictionary(172 unique tokens: [\"''\", \"'s\", ',', '.', '1']...)\n",
      "Dictionary(113 unique tokens: ['//t.co/9ynjuy4dv6', '//t.co/twcxbuvtju', ':', 'are', 'baseball']...)\n",
      "Dictionary(531 unique tokens: [',', '.', '1', 'a', 'after']...)\n",
      "Dictionary(201 unique tokens: [',', '.', 'according', 'after', 'analyst']...)\n",
      "Dictionary(58 unique tokens: [',', '.', 'avon', 'crockpot', 'go']...)\n",
      "Dictionary(163 unique tokens: [',', '.', '15-year-old', '40', 'a']...)\n",
      "Dictionary(260 unique tokens: [',', '.', '2018', 'a', 'according']...)\n",
      "Dictionary(238 unique tokens: [',', '.', '11', '2001', 'alleging']...)\n",
      "Dictionary(261 unique tokens: [',', '.', 'and', 'ben', 'certain']...)\n",
      "Dictionary(451 unique tokens: [\"''\", '.', '``', 'a', 'and']...)\n",
      "Dictionary(128 unique tokens: [',', '.', 'according', 'brandon', 'coach']...)\n",
      "Dictionary(61 unique tokens: [\"'s\", ',', '.', '29', '50']...)\n",
      "Dictionary(601 unique tokens: [',', '.', 'and', 'are', 'genitals']...)\n",
      "Dictionary(239 unique tokens: ['!', ',', '.', ':', 'ann']...)\n",
      "Dictionary(468 unique tokens: [',', '.', '1800', 'a', 'academic']...)\n",
      "Dictionary(137 unique tokens: ['.', 'am', 'be', 'captions', 'drake']...)\n",
      "Dictionary(410 unique tokens: [\"'s\", '.', 'a', 'among', 'and']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'a', 'apple', 'beefing']...)\n",
      "Dictionary(145 unique tokens: ['.', 'adverts', 'alongside', 'appearing', 'are']...)\n",
      "Dictionary(483 unique tokens: [',', '.', ':', 'a', 'africa']...)\n",
      "Dictionary(147 unique tokens: [\"'s\", ',', '.', 'a', 'children']...)\n",
      "Dictionary(79 unique tokens: [',', '.', 'a', 'against', 'ball']...)\n",
      "Dictionary(19 unique tokens: [',', '/feature/financially-fit', ':', '=', 'channel_2']...)\n",
      "Dictionary(279 unique tokens: [',', 'it', 'light', 'lights…', 'music']...)\n",
      "Dictionary(161 unique tokens: [',', '.', 'a', 'become', 'black']...)\n",
      "Dictionary(272 unique tokens: [\"'s\", '(', ')', '.', 'a']...)\n",
      "Dictionary(225 unique tokens: [',', '.', 'according', 'are', 'backgrounds']...)\n",
      "Dictionary(5 unique tokens: ['?', 'still', 'watching', 'device', 'rotate'])\n",
      "Dictionary(306 unique tokens: [',', '.', 'a', 'according', 'because']...)\n",
      "Dictionary(270 unique tokens: [',', '.', '2,500', 'ago', 'also']...)\n",
      "Dictionary(217 unique tokens: [\"'s\", ',', '.', 'a', 'aircraft']...)\n",
      "Dictionary(376 unique tokens: [\"'s\", '.', 'a', 'admissions', 'could']...)\n",
      "Dictionary(138 unique tokens: ['.', 'about', 'doesn', 'her', 'kristen']...)\n",
      "Dictionary(129 unique tokens: [\"'s\", ',', '.', 'advisory', 'been']...)\n",
      "Dictionary(189 unique tokens: ['242', 'becoming', 'believed', 'down', 'florida']...)\n",
      "Dictionary(94 unique tokens: [\"''\", '.', '``', 'a', 'against']...)\n",
      "Dictionary(279 unique tokens: [',', '.', 'a', 'according', 'administration']...)\n",
      "Dictionary(68 unique tokens: [',', '.', 'a', 'act', 'affordable']...)\n",
      "Dictionary(141 unique tokens: [',', '.', 'a', 'about', 'already']...)\n",
      "Dictionary(513 unique tokens: [\"'s\", '.', 'advocates', 'and', 'announcement']...)\n",
      "Dictionary(230 unique tokens: [',', '.', 'ahead', 'centrist', 'election']...)\n",
      "Dictionary(138 unique tokens: [',', '.', '50th', 'a', 'age']...)\n",
      "Dictionary(502 unique tokens: ['against', 'as', 'asylum', 'being', 'border']...)\n",
      "Dictionary(536 unique tokens: ['.', ';', 'a', 'cat', 'garfield']...)\n",
      "Dictionary(311 unique tokens: ['.', '2010', 'a', 'about', 'abramowicz']...)\n",
      "Dictionary(212 unique tokens: [',', '.', 'a', 'after', 'along']...)\n",
      "Dictionary(296 unique tokens: [',', 'akin', 'business', 'insider', 'oyedele']...)\n",
      "Dictionary(122 unique tokens: [',', '.', 'album', 'and', 'earns']...)\n",
      "Dictionary(120 unique tokens: ['$', ',', '-', '.', '67']...)\n",
      "Dictionary(216 unique tokens: ['.', '80-year-old', 'a', 'after', 'allotment']...)\n",
      "Dictionary(384 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(174 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(105 unique tokens: ['&', \"'ve\", ',', '.', '27-year-old']...)\n",
      "Dictionary(269 unique tokens: ['more', \"'s\", \"'ve\", ',', '.']...)\n",
      "Dictionary(246 unique tokens: ['(', ')', '--', '.', 'a']...)\n",
      "Dictionary(195 unique tokens: ['.', 'and', 'awarding', 'border', 'contracts']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(368 unique tokens: [',', '.', 'an', 'and', 'angeles']...)\n",
      "Dictionary(437 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(416 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(435 unique tokens: ['.', 'a', 'and', 'are', 'australia']...)\n",
      "Dictionary(195 unique tokens: [\"'s\", '.', 'bank', 'been', 'british']...)\n",
      "Dictionary(471 unique tokens: ['.', 'actually', 'amsterdam', 'believe', 'called']...)\n",
      "Dictionary(644 unique tokens: ['more', ',', '.', '?', 'all']...)\n",
      "Dictionary(241 unique tokens: ['.', 'a', 'allowing', 'and', 'authorities']...)\n",
      "Dictionary(358 unique tokens: ['(', ')', '.', 'again', 'and']...)\n",
      "Dictionary(799 unique tokens: [',', '.', '[', ']', 'an']...)\n",
      "Dictionary(274 unique tokens: ['!', '#', '.', '//t.co/eglskbcwbr', ':']...)\n",
      "Dictionary(366 unique tokens: [',', '.', 'address', 'an', 'asserting']...)\n",
      "Dictionary(80 unique tokens: ['.', 'at', 'carjacked', 'gunpoint', 'he']...)\n",
      "Dictionary(349 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(156 unique tokens: [\"'s\", '.', '1,000', '11', 'a']...)\n",
      "Dictionary(528 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(262 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(287 unique tokens: [\"'s\", '.', 'ai-driven', 'be', 'could']...)\n",
      "Dictionary(297 unique tokens: ['embed', 'facebook', 'google', 'plus', 'twitter']...)\n",
      "Dictionary(284 unique tokens: [',', '.', '2005', '2015', '?']...)\n",
      "Dictionary(430 unique tokens: [\"'s\", ',', '.', 'a', 'age']...)\n",
      "Dictionary(61 unique tokens: ['$', ',', '.', '82', 'a']...)\n",
      "Dictionary(289 unique tokens: [\"'s\", ',', '.', 'a', 'about']...)\n",
      "Dictionary(48 unique tokens: ['.', 'after', 'and', 'being', 'border']...)\n",
      "Dictionary(460 unique tokens: ['$', ',', '.', '2', 'administration']...)\n",
      "Dictionary(279 unique tokens: ['afp', ',', '13', '2017', '451']...)\n",
      "Dictionary(461 unique tokens: [',', '.', 'adele', 'beyonce', 'bowed']...)\n",
      "Dictionary(144 unique tokens: [',', '.', '?', 'a', 'adopt']...)\n",
      "Dictionary(461 unique tokens: [',', '.', ';', 'a', 'and']...)\n",
      "Dictionary(145 unique tokens: ['.', 'a', 'comprehensive', 'down', 'education']...)\n",
      "Dictionary(383 unique tokens: [\"'s\", ',', '.', '?', 'about']...)\n",
      "Dictionary(327 unique tokens: [',', '.', ':', 'a', 'been']...)\n",
      "Dictionary(747 unique tokens: ['blogs', 'by', 'jylian', 'russell', '2017social']...)\n",
      "Dictionary(162 unique tokens: [\"'s\", ',', '.', 'activities', 'and']...)\n",
      "Dictionary(131 unique tokens: [',', '.', 'a', 'a-list', 'after']...)\n",
      "Dictionary(231 unique tokens: [',', 'abby', 'business', 'insider', 'jackson']...)\n",
      "Dictionary(640 unique tokens: [\"'s\", ',', '.', 'across', 'and']...)\n",
      "Dictionary(589 unique tokens: [',', '.', 'a', 'administration', 'appeals']...)\n",
      "Dictionary(409 unique tokens: ['2', 'related', '3', \"''\", \"'s\"]...)\n",
      "Dictionary(296 unique tokens: [',', '.', 'a', 'although', 'being']...)\n",
      "Dictionary(142 unique tokens: ['.', '2k', 'and', 'company', 'eleague']...)\n",
      "Dictionary(224 unique tokens: ['.', 'abandoned', 'abused', 'across', 'an']...)\n",
      "Dictionary(23 unique tokens: ['08', '2017', 'feb', ',', '19']...)\n",
      "Dictionary(237 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(295 unique tokens: ['.', 'a', 'and', 'avatar', 'breitbart']...)\n",
      "Dictionary(224 unique tokens: [',', '.', 'a', 'abdur-rashid', 'affirming']...)\n",
      "Dictionary(433 unique tokens: ['more', ',', '.', 'a', 'and']...)\n",
      "Dictionary(270 unique tokens: [\"'\", \"''\", \"'blizzard\", \"'essence\", \"'fox\"]...)\n",
      "Dictionary(157 unique tokens: ['.', 'a', 'and', 'arid', 'desert']...)\n",
      "Dictionary(187 unique tokens: ['!', \"'re\", '(', ')', ',']...)\n",
      "Dictionary(41 unique tokens: ['.', 'after', 'cattle', 'come', 'conflict']...)\n",
      "Dictionary(305 unique tokens: ['a', 'act', 'dodd-frank', 'expected', 'is']...)\n",
      "Dictionary(199 unique tokens: ['.', 'a', 'after', 'an', 'balls']...)\n",
      "Dictionary(257 unique tokens: [',', '.', 'appear', 'err', 'for']...)\n",
      "Dictionary(106 unique tokens: [',', 'business', 'insider', 'jethro', 'nededog']...)\n",
      "Dictionary(167 unique tokens: [',', 'alex', 'business', 'insider', 'lockie']...)\n",
      "Dictionary(63 unique tokens: ['chris', 'snyder', ',', '1', '2017']...)\n",
      "Dictionary(235 unique tokens: [':', 'attorney', 'debate', 'general', 'jeff']...)\n",
      "Dictionary(237 unique tokens: ['(', ')', '.', 'an', 'caller']...)\n",
      "Dictionary(293 unique tokens: ['alex', 'heath', ',', '20:16', '30.01.2017']...)\n",
      "Dictionary(203 unique tokens: [',', '.', 'a', 'amazing', 'but']...)\n",
      "Dictionary(275 unique tokens: ['.', '11', 'an', 'are', 'as']...)\n",
      "Dictionary(115 unique tokens: [',', '--', '.', 'a', 'asked']...)\n",
      "Dictionary(446 unique tokens: ['more', ',', '.', 'a', 'all']...)\n",
      "Dictionary(476 unique tokens: ['$', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(174 unique tokens: ['matt', 'turner', ',', '13:42', '23.01.2017']...)\n",
      "Dictionary(377 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(461 unique tokens: [',', '.', 'a', 'bike', 'hasn']...)\n",
      "Dictionary(197 unique tokens: [',', '.', 'as', 'be', 'but']...)\n",
      "Dictionary(614 unique tokens: [\"'s\", ',', '.', 'alzheimer', 'and']...)\n",
      "Dictionary(182 unique tokens: [',', '.', 'a', 'against', 'alleged']...)\n",
      "Dictionary(80 unique tokens: ['garfield', 'leanna', ',', '17:30', '26.01.2017']...)\n",
      "Dictionary(373 unique tokens: [',', '.', 'action', 'activists', 'afternoon']...)\n",
      "Dictionary(341 unique tokens: ['&', \"''\", ',', '``', 'album']...)\n",
      "Dictionary(642 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(119 unique tokens: [',', '.', '?', 'a', 'always']...)\n",
      "Dictionary(223 unique tokens: [',', '.', 'a', 'america', 'as']...)\n",
      "Dictionary(212 unique tokens: ['and', 'elena', 'holodny', 'reuters', ',']...)\n",
      "Dictionary(157 unique tokens: ['#', ',', '.', '2017', 'a']...)\n",
      "Dictionary(472 unique tokens: [\"''\", \"'s\", '.', '``', 'after']...)\n",
      "Dictionary(237 unique tokens: ['25,000', 'disrupted', 'of', 'passengers', 'plans']...)\n",
      "Dictionary(278 unique tokens: ['embed', 'facebook', 'google', 'plus', 'twitter']...)\n",
      "Dictionary(50 unique tokens: ['.', 'a', 'all', 'and', 'boy']...)\n",
      "Dictionary(541 unique tokens: ['&', '(', ')', ',', '--']...)\n",
      "Dictionary(232 unique tokens: ['.', 'and', 'dalai', 'donald', 'for']...)\n",
      "Dictionary(211 unique tokens: [\"''\", \"'s\", '--', '.', '105.1']...)\n",
      "Dictionary(376 unique tokens: [\"'s\", ',', '.', 'analysis', 'cnn']...)\n",
      "Dictionary(189 unique tokens: [\"'\", \"'do\", \"'s\", ',', '.']...)\n",
      "Dictionary(510 unique tokens: [',', '.', '45th', 'a', 'after']...)\n",
      "Dictionary(7 unique tokens: [':', 'donald', 'gigapixel', 'inauguration', 'of']...)\n",
      "Dictionary(217 unique tokens: [',', '.', '200,000', '2020', 'announced']...)\n",
      "Dictionary(538 unique tokens: ['.', 'a', 'and', 'angelou', 'ass.']...)\n",
      "Dictionary(256 unique tokens: [':', 'against', 'betsy', 'democrats', 'devos']...)\n",
      "Dictionary(243 unique tokens: [',', '.', 'a', 'after', 'as']...)\n",
      "Dictionary(174 unique tokens: [\"'s\", ',', '.', 'a', 'ablaze']...)\n",
      "Dictionary(241 unique tokens: [\"'s\", '.', '1', 'award', 'beautiful']...)\n",
      "Dictionary(248 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(772 unique tokens: [',', '.', 'a', 'agency', 'angst']...)\n",
      "Dictionary(168 unique tokens: ['kif', 'leswing', ',', '108', '17.01.2017']...)\n",
      "Dictionary(296 unique tokens: ['$', '.', '11', 'a', 'about']...)\n",
      "Dictionary(250 unique tokens: [\"'s\", '.', 'accessing', 'blocked', 'from']...)\n",
      "Dictionary(147 unique tokens: [',', '.', 'a', 'challenge', 'conservative']...)\n",
      "Dictionary(364 unique tokens: [',', 'christian', 'reuters', 'shepherd', '12,112']...)\n",
      "Dictionary(11 unique tokens: ['.', '1964', 'american', 'campaigner', 'civil']...)\n",
      "Dictionary(101 unique tokens: [\"'s\", '.', 'a', 'america', 'and']...)\n",
      "Dictionary(503 unique tokens: [',', '?', 'any', 'battle', 'davos']...)\n",
      "Dictionary(250 unique tokens: [\"''\", \"'s\", ',', '.', '19']...)\n",
      "Dictionary(92 unique tokens: [\"'s\", '.', '//t.co/jvamgcqeq3', ':', 'a']...)\n",
      "Dictionary(59 unique tokens: ['.', 'britain', 'coming', 'details', 'european']...)\n",
      "Dictionary(253 unique tokens: ['.', 'not', 'sorry', '1', 'an']...)\n",
      "Dictionary(211 unique tokens: ['#', \"'\", \"''\", \"'re\", \"'s\"]...)\n",
      "Dictionary(38 unique tokens: [\"'s\", ',', '.', 'a', 'aubameyang']...)\n",
      "Dictionary(329 unique tokens: [',', '.', 'a', 'according', 'aid']...)\n",
      "Dictionary(1188 unique tokens: [',', '.', 'a', 'already', 'and']...)\n",
      "Dictionary(342 unique tokens: [\"'s\", '.', 'and', 'appears', 'banned']...)\n",
      "Dictionary(390 unique tokens: [\"''\", \"'s\", '.', '50-over', '``']...)\n",
      "Dictionary(288 unique tokens: [',', '.', ':', 'capitol', 'confirmation']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(401 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(318 unique tokens: [',', '--', '.', 'a', 'alarmed']...)\n",
      "Dictionary(76 unique tokens: ['.', 'a', 'been', 'condom', 'girlfriend']...)\n",
      "Dictionary(231 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(575 unique tokens: [',', '.', '3-year', '6:30', 'almost']...)\n",
      "Dictionary(188 unique tokens: ['.', 'bless', 'god', 'memes', ',']...)\n",
      "Dictionary(176 unique tokens: [\"'s\", ',', '.', 'all', 'and']...)\n",
      "Dictionary(243 unique tokens: [',', '.', 'a', 'al-habbash', 'all']...)\n",
      "Dictionary(251 unique tokens: [',', '.', '2016', 'a', 'against']...)\n",
      "Dictionary(371 unique tokens: ['.', 'airport', 'and', 'at', 'bag']...)\n",
      "Dictionary(139 unique tokens: [',', '.', ';', 'about', 'and']...)\n",
      "Dictionary(75 unique tokens: [',', '.', '269-square-foot', 'a', 'across']...)\n",
      "Dictionary(439 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(149 unique tokens: [\"'\", \"''\", \"'grow\", \"'have\", \"'re\"]...)\n",
      "Dictionary(336 unique tokens: [',', '.', '5', 'a', 'and']...)\n",
      "Dictionary(382 unique tokens: ['more', ',', '.', '67-year-old', 'another']...)\n",
      "Dictionary(263 unique tokens: [',', '.', 'can', 'do', 'girls']...)\n",
      "Dictionary(65 unique tokens: [',', '.', 'and', 'away', 'cable']...)\n",
      "Dictionary(185 unique tokens: [\"'s\", '.', 'a', 'abbas', 'an']...)\n",
      "Dictionary(472 unique tokens: [',', '.', '65', '83', 'a']...)\n",
      "Dictionary(382 unique tokens: ['by', 'joanne', 'orlando', '.', 'children']...)\n",
      "Dictionary(115 unique tokens: ['(', ')', 'bloomberg', 'casts', 'lots']...)\n",
      "Dictionary(259 unique tokens: [',', '.', 'china', 'communications', 'considering']...)\n",
      "Dictionary(451 unique tokens: ['images', 'omar', 'torres/afp/getty', ',', '.']...)\n",
      "Dictionary(181 unique tokens: ['.', 'american', 'are', 'it', 'just']...)\n",
      "Dictionary(87 unique tokens: ['#', \"'\", '.', '//t.co/x11sah4ciq', '84']...)\n",
      "Dictionary(44 unique tokens: ['gene', 'kim', ',', '179,942', '2016']...)\n",
      "Dictionary(151 unique tokens: ['(', ')', '.', 'a', 'above']...)\n",
      "Dictionary(11 unique tokens: [',', '.', 'for', 'is', 'page']...)\n",
      "Dictionary(261 unique tokens: [\"'re\", '(', ')', ',', '-']...)\n",
      "Dictionary(451 unique tokens: ['$', '(', ')', '*well', ',']...)\n",
      "Dictionary(308 unique tokens: [',', '.', 'a', 'and', 'appeal']...)\n",
      "Dictionary(370 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(163 unique tokens: [',', '.', 'and', 'are', 'cozy']...)\n",
      "Dictionary(238 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(154 unique tokens: [',', '.', 'a', 'among', 'bird']...)\n",
      "Dictionary(156 unique tokens: [\"'\", \"''\", \"'white\", ',', '.']...)\n",
      "Dictionary(76 unique tokens: ['!', ',', '.', '1980s', '53']...)\n",
      "Dictionary(213 unique tokens: ['.', 'a', 'after', 'app', 'authorities']...)\n",
      "Dictionary(313 unique tokens: ['bidding', 'by', 'call', 'defence', 'exclude']...)\n",
      "Dictionary(145 unique tokens: [',', '--', '.', 'an', 'at']...)\n",
      "Dictionary(362 unique tokens: [',', '.', 'a', 'according', 'been']...)\n",
      "Dictionary(417 unique tokens: ['more', ',', '.', '2016', '27']...)\n",
      "Dictionary(611 unique tokens: ['!', ',', '.', 'about', 'all']...)\n",
      "Dictionary(102 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(140 unique tokens: ['.', 'be', 'ceo', 'costs', 'do']...)\n",
      "Dictionary(569 unique tokens: ['?', 'able', 'are', 'away', 'detail']...)\n",
      "Dictionary(164 unique tokens: [\"''\", ',', '.', '3', '``']...)\n",
      "Dictionary(394 unique tokens: [',', '.', '2016', 'americans', 'are']...)\n",
      "Dictionary(102 unique tokens: ['emmie', 'martin', ',', '1,339', '18.12.2016']...)\n",
      "Dictionary(132 unique tokens: ['.', '2', '21', 'a', 'and']...)\n",
      "Dictionary(874 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(569 unique tokens: [',', '.', ';', 'a', 'admit']...)\n",
      "Dictionary(421 unique tokens: ['$', '.', '500,000', ':', 'a']...)\n",
      "Dictionary(230 unique tokens: [',', '.', 'alex', 'end', 'from']...)\n",
      "Dictionary(185 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(163 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(252 unique tokens: [',', '.', '3.5mm', '7', 'a']...)\n",
      "Dictionary(371 unique tokens: ['!', ',', '.', '11', '2001']...)\n",
      "Dictionary(147 unique tokens: ['.', 'a', 'after', 'an', 'arizona']...)\n",
      "Dictionary(132 unique tokens: [\"'s\", ',', '.', '31st', 'after']...)\n",
      "Dictionary(66 unique tokens: ['ludacer', 'rob', ',', '2016', '23,582']...)\n",
      "Dictionary(416 unique tokens: ['author', 'contact', ',', '.', 'a']...)\n",
      "Dictionary(276 unique tokens: ['.', 'a', 'been', 'british', 'by']...)\n",
      "Dictionary(168 unique tokens: [\"'s\", ',', '.', '65-year-old', 'a']...)\n",
      "Dictionary(283 unique tokens: [',', '.', 'america', 'arctic', 'climates']...)\n",
      "Dictionary(116 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(429 unique tokens: [',', '15', '2016', '4:33', 'dec']...)\n",
      "Dictionary(98 unique tokens: [\"'s\", '.', '2016', 'as', 'breathtaking']...)\n",
      "Dictionary(176 unique tokens: ['jethro', 'nededog', ',', '15.12.2016', '16:56']...)\n",
      "Dictionary(84 unique tokens: ['garber', 'jonathan', ',', '14:09', '15.12.2016']...)\n",
      "Dictionary(364 unique tokens: ['.', 'a', 'aleppo', 'and', 'attention']...)\n",
      "Dictionary(369 unique tokens: [\"'s\", ',', '.', '15', '2016']...)\n",
      "Dictionary(154 unique tokens: ['!', '.', '?', 'a', 'blue']...)\n",
      "Dictionary(41 unique tokens: [',', '.', ':', 'appearance', 'at']...)\n",
      "Dictionary(373 unique tokens: [',', '.', ':', 'a', 'american']...)\n",
      "Dictionary(199 unique tokens: [\"''\", '.', '//cnn.it/2gerubl', '//cnn.it/2hbbkuv', ':']...)\n",
      "Dictionary(168 unique tokens: [\"'\", \"'it\", '.', 'a', 'bit']...)\n",
      "Dictionary(420 unique tokens: ['embed', 'facebook', 'google', 'plus', 'twitter']...)\n",
      "Dictionary(355 unique tokens: [',', '.', 'a', 'from', 'generations']...)\n",
      "Dictionary(234 unique tokens: [\"'s\", ',', '.', '2005', 'agustawestland']...)\n",
      "Dictionary(169 unique tokens: ['!', \"'\", \"''\", \"'apprentice\", \"'fake\"]...)\n",
      "Dictionary(622 unique tokens: [',', 'a', 'conviction', 'despite', 'discrimination']...)\n",
      "Dictionary(394 unique tokens: [\"'s\", ',', '.', 'a', 'activities']...)\n",
      "Dictionary(261 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(603 unique tokens: ['more', '(', ')', ':', 'getty']...)\n",
      "Dictionary(360 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(312 unique tokens: ['1950s', '1970s', 'a', 'across', 'and']...)\n",
      "Dictionary(196 unique tokens: [\"'s\", ',', '.', 'a', 'abortions']...)\n",
      "Dictionary(199 unique tokens: [',', '.', 'after', 'and', 'batteries']...)\n",
      "Dictionary(368 unique tokens: ['--', '.', 'a', 'an', 'approach']...)\n",
      "Dictionary(183 unique tokens: ['.', 'a', 'and', 'as', 'been']...)\n",
      "Dictionary(176 unique tokens: ['.', 'already', 'change', 'channel', 'just']...)\n",
      "Dictionary(45 unique tokens: ['...', 'geladen', 'wird', 'verarbeitet', ',']...)\n",
      "Dictionary(1838 unique tokens: [\"'the\", ',', '-', '.', '4,000']...)\n",
      "Dictionary(325 unique tokens: ['(', ')', 'ap', 'photo/francisco', 'seco']...)\n",
      "Dictionary(378 unique tokens: [',', '.', 'a', 'abortion', 'abortions']...)\n",
      "Dictionary(261 unique tokens: ['.', 'an', 'commercials', 'competition', 'coverage']...)\n",
      "Dictionary(270 unique tokens: ['.', 'a', 'been', 'cartel-linked', 'coahuila']...)\n",
      "Dictionary(164 unique tokens: ['doomed', 'festival', 'from', 'has', 'he']...)\n",
      "Dictionary(152 unique tokens: ['//t.co/f132w6nzkc', ':', 'ball', 'https', 'into']...)\n",
      "Dictionary(518 unique tokens: [',', '.', 'a', 'act', 'affordable']...)\n",
      "Dictionary(802 unique tokens: [',', '.', '100', 'and', 'at']...)\n",
      "Dictionary(169 unique tokens: [\"''\", \"'s\", ',', '.', '2013']...)\n",
      "Dictionary(398 unique tokens: [\"'s\", ',', '.', '2015', '47']...)\n",
      "Dictionary(219 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(65 unique tokens: [',', '.', '6,000', 'about', 'and']...)\n",
      "Dictionary(231 unique tokens: [\"'\", \"'tremendous\", 'a', 'after', 'audible']...)\n",
      "Dictionary(297 unique tokens: [',', '.', 'a', 'an', 'between']...)\n",
      "Dictionary(187 unique tokens: [',', '.', 'a', 'at', 'back']...)\n",
      "Dictionary(44 unique tokens: ['.', '102nd', 'a', 'across', 'and']...)\n",
      "Dictionary(134 unique tokens: [\"'s\", '--', '.', '113-109', '4']...)\n",
      "Dictionary(750 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(155 unique tokens: ['.', '140', 'a', 'afghan', 'army']...)\n",
      "Dictionary(328 unique tokens: [',', '.', '2013', 'a', 'aaron']...)\n",
      "Dictionary(169 unique tokens: ['.', 'an', 'been', 'copy', 'declaration']...)\n",
      "Dictionary(150 unique tokens: [\"'s\", '.', 'a', 'about', 'bad']...)\n",
      "Dictionary(666 unique tokens: ['&', \"''\", ',', '.', '63']...)\n",
      "Dictionary(424 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(137 unique tokens: [',', 'bob', 'bryan', 'business', 'insider']...)\n",
      "Dictionary(225 unique tokens: [',', '.', '100', '2008', 'alaska']...)\n",
      "Dictionary(159 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(479 unique tokens: ['arrested', 'but', 'february', 'in', 'kill']...)\n",
      "Dictionary(150 unique tokens: [\"'s\", '.', '...', 'a', 'as']...)\n",
      "Dictionary(72 unique tokens: [\"'s\", ',', '.', '10-track', '28']...)\n",
      "Dictionary(627 unique tokens: ['.', 'a', 'and', 'as', 'cameron']...)\n",
      "Dictionary(197 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(396 unique tokens: [',', '.', '23,000', 'a', 'as']...)\n",
      "Dictionary(205 unique tokens: [\"'\", \"'unique\", ',', 'a', 'blair']...)\n",
      "Dictionary(331 unique tokens: ['.', 'are', 'be', 'could', 'dog-house']...)\n",
      "Dictionary(791 unique tokens: ['as', 'coverage', 'day', 'developments', 'happen']...)\n",
      "Dictionary(554 unique tokens: [',', '.', 'a', 'after', 'afterglow']...)\n",
      "Dictionary(372 unique tokens: [',', '.', 'a', 'according', 'case']...)\n",
      "Dictionary(189 unique tokens: ['.', 'a', 'are', 'as', 'being']...)\n",
      "Dictionary(425 unique tokens: [',', '.', 'a', 'and', 'been']...)\n",
      "Dictionary(171 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(114 unique tokens: ['.', '100', 'a', 'about', 'and']...)\n",
      "Dictionary(83 unique tokens: ['.', '90s', ':', 'about', 'away']...)\n",
      "Dictionary(180 unique tokens: [',', '.', '13-year-old', 'a', 'accidentally']...)\n",
      "Dictionary(284 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(102 unique tokens: [\"'\", \"'s\", \"'white\", ',', '.']...)\n",
      "Dictionary(125 unique tokens: ['(', ')', '@', 'a', 'anthony']...)\n",
      "Dictionary(435 unique tokens: ['(', ')', '.', 'about', 'allegedly']...)\n",
      "Dictionary(121 unique tokens: [\"'s\", '.', 'a', 'an', 'arrested']...)\n",
      "Dictionary(286 unique tokens: ['!', ',', '.', '140-character', '@']...)\n",
      "Dictionary(22 unique tokens: [',', '.', 'a', 'airlines', 'bell']...)\n",
      "Dictionary(377 unique tokens: ['more', ',', '.', '100th', 'a']...)\n",
      "Dictionary(184 unique tokens: [',', '.', 'among', 'and', 'animals']...)\n",
      "Dictionary(48 unique tokens: ['%', \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(527 unique tokens: ['$', \"'\", \"'s\", ',', '.']...)\n",
      "Dictionary(255 unique tokens: ['.', 'are', 'claimed', 'collapse', 'economy']...)\n",
      "Dictionary(505 unique tokens: ['$', '&', \"'re\", \"'s\", ',']...)\n",
      "Dictionary(188 unique tokens: [\"'s\", '.', 'al-assad', 'andrew', 'attack']...)\n",
      "Dictionary(165 unique tokens: [\"'\", \"'solve\", 'china', 'could', 'president']...)\n",
      "Dictionary(150 unique tokens: [',', 'alex', 'business', 'insider', 'lockie']...)\n",
      "Dictionary(275 unique tokens: ['.', 'adani', 'australia', 'be', 'chairman']...)\n",
      "Dictionary(771 unique tokens: [\"'s\", 'and', 'as', 'buried', 'centuries']...)\n",
      "Dictionary(411 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(352 unique tokens: [',', '.', 'a', 'and', 'by']...)\n",
      "Dictionary(147 unique tokens: ['co-star', 'fell', 'he', 'his', 'on']...)\n",
      "Dictionary(254 unique tokens: [',', '.', 'air', 'another', 'as']...)\n",
      "Dictionary(57 unique tokens: [',', ':', '?', 'a', 'as']...)\n",
      "Dictionary(274 unique tokens: ['15', '150', '2014', '2015', 'a']...)\n",
      "Dictionary(118 unique tokens: ['.', 'a', 'and', 'benefits', 'burying']...)\n",
      "Dictionary(301 unique tokens: [',', '.', '2020', 'a', 'all']...)\n",
      "Dictionary(216 unique tokens: [',', '.', 'a', 'about', 'allegedly']...)\n",
      "Dictionary(38 unique tokens: [\"''\", '.', '``', 'a', 'against']...)\n",
      "Dictionary(79 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(133 unique tokens: [',', '.', '58', 'a', 'at']...)\n",
      "Dictionary(165 unique tokens: [',', '.', '2018', 'announced', 'be']...)\n",
      "Dictionary(111 unique tokens: ['.', 'a', 'after', 'an', 'developing']...)\n",
      "Dictionary(147 unique tokens: ['1987', 'comedy', 'mannequin', 'romantic', 'spoofed']...)\n",
      "Dictionary(530 unique tokens: [',', '?', 'an', 'around', 'as']...)\n",
      "Dictionary(88 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(700 unique tokens: [\"'s\", 'abc', 'broadcast', 'emergency', 'for']...)\n",
      "Dictionary(329 unique tokens: ['.', 'a', 'bloomberg', 'columnist', 'covering']...)\n",
      "Dictionary(318 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(276 unique tokens: [',', 'about', 'ahmed', 'belief', 'complains']...)\n",
      "Dictionary(72 unique tokens: ['(', ')', ',', '.', '1']...)\n",
      "Dictionary(401 unique tokens: [\"'s\", ',', '.', 'adaptations', 'allegiant']...)\n",
      "Dictionary(245 unique tokens: [',', '.', 'a', 'as', 'china']...)\n",
      "Dictionary(89 unique tokens: ['.', 'a', 'affair', 'are', 'but']...)\n",
      "Dictionary(175 unique tokens: ['.', 'a', 'cities', 'cut', 'executive']...)\n",
      "Dictionary(246 unique tokens: [\"'\", \"'coloured\", \"'s\", ',', '.']...)\n",
      "Dictionary(312 unique tokens: [',', '.', '2019', 'a', 'agenda']...)\n",
      "Dictionary(359 unique tokens: [',', '.', '100', 'a', 'ago']...)\n",
      "Dictionary(702 unique tokens: ['.', 'an', 'attic', 'building', 'but']...)\n",
      "Dictionary(325 unique tokens: ['.', '2,000', 'a', 'ago', 'been']...)\n",
      "Dictionary(219 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(417 unique tokens: [',', '.', 'a', 'acts', 'african']...)\n",
      "Dictionary(257 unique tokens: [',', '.', 'a', 'about', 'and']...)\n",
      "Dictionary(29 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(174 unique tokens: ['.', 'a', 'against', 'alexei', 'at']...)\n",
      "Dictionary(302 unique tokens: [',', '.', '15', '2012', ';']...)\n",
      "Dictionary(672 unique tokens: ['.', 'congresswoman', 'is', 'maxine', 'mince']...)\n",
      "Dictionary(146 unique tokens: ['%', '&', \"'\", \"''\", \"'s\"]...)\n",
      "Dictionary(464 unique tokens: ['.', 'actually', 'british', 'but', 'coming']...)\n",
      "Dictionary(166 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(267 unique tokens: ['.', 'a', 'about', 'an', 'and']...)\n",
      "Dictionary(112 unique tokens: ['.', 'a', 'an', 'apple', 'could']...)\n",
      "Dictionary(465 unique tokens: [',', '--', '.', '30', 'a']...)\n",
      "Dictionary(149 unique tokens: ['.', 'ban', 'electronics', 'flights', 'following']...)\n",
      "Dictionary(150 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(339 unique tokens: ['.', '//www.facebook.com/nbcnews', '//www.snapchat.com/add/nbcnews', '75', ':']...)\n",
      "Dictionary(392 unique tokens: [',', '.', 'against', 'and', 'are']...)\n",
      "Dictionary(227 unique tokens: ['&', \"'\", \"''\", \"'genuinely\", \"'i\"]...)\n",
      "Dictionary(438 unique tokens: [\"'\", \"'s\", ',', '--', '.']...)\n",
      "Dictionary(169 unique tokens: [',', '.', 'a', 'act', 'any']...)\n",
      "Dictionary(464 unique tokens: [',', 'a', 'accept', 'allegedly', 'as']...)\n",
      "Dictionary(171 unique tokens: ['(', ')', '.', 'a', 'act']...)\n",
      "Dictionary(269 unique tokens: ['.', 'all', 'america', 'at', 'by']...)\n",
      "Dictionary(236 unique tokens: [',', 'anti-aircraft', 'but', 'came', 'fighter']...)\n",
      "Dictionary(292 unique tokens: ['$', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(345 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(61 unique tokens: [',', '.', 'a', 'and', 'as']...)\n",
      "Dictionary(602 unique tokens: ['(', ')', ',', '.', 'americans']...)\n",
      "Dictionary(231 unique tokens: ['&', \"''\", \"'s\", '(', ')']...)\n",
      "Dictionary(300 unique tokens: [',', '--', '.', 'along', 'and']...)\n",
      "Dictionary(166 unique tokens: ['.', 'and', 'anxious', 'families', 'generation']...)\n",
      "Dictionary(94 unique tokens: ['.', 'a', 'accidentally', 'experienced', 'feeling']...)\n",
      "Dictionary(40 unique tokens: ['(', ')', ',', '.', '15th']...)\n",
      "Dictionary(734 unique tokens: ['.', 'ambitions', 'and', 'contain', 'country']...)\n",
      "Dictionary(24 unique tokens: ['.', '?', 'and', 'are', 'day']...)\n",
      "Dictionary(237 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(308 unique tokens: ['(', ')', ',', '.', 'across']...)\n",
      "Dictionary(131 unique tokens: ['.', 'a', 'and', 'correspondent', 'david']...)\n",
      "Dictionary(81 unique tokens: [',', '.', 'a', 'and', 'hair']...)\n",
      "Dictionary(731 unique tokens: ['.', 'ambitions', 'and', 'contain', 'country']...)\n",
      "Dictionary(320 unique tokens: [',', '--', '.', 'abenomics', 'all']...)\n",
      "Dictionary(222 unique tokens: ['(', ')', ',', 'and', 'ceo']...)\n",
      "Dictionary(208 unique tokens: [',', '.', '29', 'a', 'according']...)\n",
      "Dictionary(93 unique tokens: ['.', 'a', 'after', 'country', 'death']...)\n",
      "Dictionary(361 unique tokens: ['more', '#', '(', ')', ',']...)\n",
      "Dictionary(61 unique tokens: ['ludacer', 'rob', ',', '.', '11:12']...)\n",
      "Dictionary(762 unique tokens: [',', 'and', 'animal', 'animals', 'around']...)\n",
      "Dictionary(155 unique tokens: ['.', '50-year-old', 'a', 'adult', 'an']...)\n",
      "Dictionary(333 unique tokens: [',', '.', 'a', 'and', 'britons']...)\n",
      "Dictionary(508 unique tokens: [',', '.', '1', '1986', '20']...)\n",
      "Dictionary(73 unique tokens: ['$', ',', '.', '2', '760']...)\n",
      "Dictionary(99 unique tokens: [',', '--', '.', '2015', '27']...)\n",
      "Dictionary(268 unique tokens: [',', '.', 'been', 'days', 'it']...)\n",
      "Dictionary(251 unique tokens: [',', 'antonio', 'business', 'insider', 'villas-boas']...)\n",
      "Dictionary(52 unique tokens: [',', '.', '22nd', 'a', 'and']...)\n",
      "Dictionary(202 unique tokens: [',', '.', 'a', 'about', 'amsterdam']...)\n",
      "Dictionary(180 unique tokens: [\"'\", \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(686 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(358 unique tokens: [',', '.', 'a', 'afghanistan', 'among']...)\n",
      "Dictionary(188 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(331 unique tokens: [\"'s\", '(', ')', '3310', 'hmd']...)\n",
      "Dictionary(458 unique tokens: [',', '.', 'a', 'all', 'bizarre']...)\n",
      "Dictionary(452 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(236 unique tokens: [\"'s\", '.', 'a', 'allegations', 'and']...)\n",
      "Dictionary(316 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(543 unique tokens: [',', '.', 'a', 'about', 'accounts']...)\n",
      "Dictionary(79 unique tokens: ['.', 'are', 'as', 'begins', 'celebrating']...)\n",
      "Dictionary(951 unique tokens: [',', '.', '11', 'a', 'all-star']...)\n",
      "Dictionary(416 unique tokens: [',', '.', 'a', 'arkansas', 'behind']...)\n",
      "Dictionary(325 unique tokens: [\"'s\", ',', '.', 'are', 'as']...)\n",
      "Dictionary(202 unique tokens: ['.', 'a', 'accusations', 'after', 'against']...)\n",
      "Dictionary(241 unique tokens: ['shutterstock', ',', '.', 'ads', 'among']...)\n",
      "Dictionary(59 unique tokens: [\"'\", \"'diana\", ',', '.', '1997']...)\n",
      "Dictionary(196 unique tokens: ['&', ',', '.', ';', 'after']...)\n",
      "Dictionary(155 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(121 unique tokens: ['(', ')', ',', '.', 'and']...)\n",
      "Dictionary(242 unique tokens: ['able', 'be', 'britain', 'commission', 'cut-price']...)\n",
      "Dictionary(594 unique tokens: ['&', ',', '.', '100', '250']...)\n",
      "Dictionary(285 unique tokens: [',', '.', 'a', 'according', 'at']...)\n",
      "Dictionary(270 unique tokens: [',', 'and', 'celebratory', 'collections', 'decade']...)\n",
      "Dictionary(355 unique tokens: [\"'s\", ',', '.', ':', 'american']...)\n",
      "Dictionary(198 unique tokens: ['%', \"'s\", ',', '2', '2016']...)\n",
      "Dictionary(124 unique tokens: [',', '.', '...', '2016', 'a']...)\n",
      "Dictionary(459 unique tokens: [',', '.', '17', '19', '?']...)\n",
      "Dictionary(120 unique tokens: [',', '.', '1548', 'a', 'also']...)\n",
      "Dictionary(235 unique tokens: [\"'\", \"'master\", \"'s\", ',', '.']...)\n",
      "Dictionary(197 unique tokens: [',', '.', 'about', 'british', 'by']...)\n",
      "Dictionary(100 unique tokens: [',', '.', '60', 'an', 'are']...)\n",
      "Dictionary(292 unique tokens: ['--', '.', 'a', 'all', 'at']...)\n",
      "Dictionary(324 unique tokens: ['.', 'and', 'are', 'be', 'can']...)\n",
      "Dictionary(118 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(246 unique tokens: ['$', '&', \"'s\", ',', '.']...)\n",
      "Dictionary(296 unique tokens: [\"''\", '(', ')', '.', '``']...)\n",
      "Dictionary(115 unique tokens: ['-', '.', 'a', 'about', 'angeles']...)\n",
      "Dictionary(178 unique tokens: [',', '.', 'a', 'adviser', 'appeared']...)\n",
      "Dictionary(265 unique tokens: ['(', ')', ',', '-', '.']...)\n",
      "Dictionary(129 unique tokens: ['.', 'at', 'awards', 'comes', 'excellence']...)\n",
      "Dictionary(181 unique tokens: ['$', \"'\", \"''\", \"'i\", \"'ll\"]...)\n",
      "Dictionary(445 unique tokens: [',', '.', 'and', 'are', 'be']...)\n",
      "Dictionary(693 unique tokens: ['a', 'comfortable', 'disappointing', 'flying', 'gave']...)\n",
      "Dictionary(217 unique tokens: [',', '.', 'a', 'and', 'arguing']...)\n",
      "Dictionary(328 unique tokens: [',', '.', 'and', 'chat', 'danny']...)\n",
      "Dictionary(260 unique tokens: [',', '.', 'a', 'about', 'as']...)\n",
      "Dictionary(302 unique tokens: [',', '.', '500-pound', '75,000', 'a']...)\n",
      "Dictionary(206 unique tokens: ['.', ':', 'a', 'after', 'and']...)\n",
      "Dictionary(251 unique tokens: [',', '.', 'a', 'american', 'and']...)\n",
      "Dictionary(340 unique tokens: ['more', \"'s\", ',', '.', 'according']...)\n",
      "Dictionary(380 unique tokens: ['courtesy', 'ken', 'of', 'williams', ',']...)\n",
      "Dictionary(250 unique tokens: ['(', ')', '.', 'a', 'best']...)\n",
      "Dictionary(198 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(20 unique tokens: [',', '/investment-guide/2017', '2017', ':', '=']...)\n",
      "Dictionary(183 unique tokens: [',', '.', 'a', 'about', 'acclaimed']...)\n",
      "Dictionary(299 unique tokens: ['.', 'america', 'as', 'but', 'continues']...)\n",
      "Dictionary(233 unique tokens: [',', '.', 'address', 'bercow', 'commons']...)\n",
      "Dictionary(230 unique tokens: ['calls', 'chair', 'empty', 'female', 'for']...)\n",
      "Dictionary(457 unique tokens: [',', '--', '.', ':', 'a']...)\n",
      "Dictionary(66 unique tokens: ['.', '360', 'a', 'around', 'bring']...)\n",
      "Dictionary(286 unique tokens: [',', 'a', 'after', 'attack', 'bardo']...)\n",
      "Dictionary(689 unique tokens: ['more', ',', '.', ':', 'a']...)\n",
      "Dictionary(703 unique tokens: ['a', 'and', 'as', 'dazed', 'eased']...)\n",
      "Dictionary(344 unique tokens: ['and', 'by', 'perry', 'tom', 'torbati']...)\n",
      "Dictionary(731 unique tokens: [',', '.', 'a', 'and', 'are']...)\n",
      "Dictionary(263 unique tokens: [\"''\", \"'s\", '.', '``', 'advising']...)\n",
      "Dictionary(62 unique tokens: [',', '.', 'a', 'and', 'apb']...)\n",
      "Dictionary(235 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(145 unique tokens: ['.', 'even', 'got', 'grammy', 'just']...)\n",
      "Dictionary(245 unique tokens: [',', '--', '.', 'already', 'always']...)\n",
      "Dictionary(250 unique tokens: [',', '.', '15', '90', 'a']...)\n",
      "Dictionary(257 unique tokens: [':', 'against', 'betsy', 'democrats', 'devos']...)\n",
      "Dictionary(629 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(275 unique tokens: ['$', \"'s\", ',', '.', '500']...)\n",
      "Dictionary(371 unique tokens: ['!', ',', 'a', 'and', 'because']...)\n",
      "Dictionary(207 unique tokens: [',', '.', 'as', 'attorney', 'but']...)\n",
      "Dictionary(544 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(166 unique tokens: ['garber', 'jonathan', ',', '11', '17:10']...)\n",
      "Dictionary(237 unique tokens: [\"'\", \"'apprentice\", '.', 'also', 'america']...)\n",
      "Dictionary(538 unique tokens: ['booth', 'by', 'stephanie', '.', 'appeared']...)\n",
      "Dictionary(320 unique tokens: [\"'s\", ',', '.', 'against', 'america']...)\n",
      "Dictionary(336 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(42 unique tokens: [',', '.', 'adding', 'adviser', 'and']...)\n",
      "Dictionary(350 unique tokens: ['more', ',', '.', '25', '80']...)\n",
      "Dictionary(193 unique tokens: ['.', 'and', 'at', 'bank', 'central']...)\n",
      "Dictionary(108 unique tokens: [\"'s\", ',', '.', 'a', 'all']...)\n",
      "Dictionary(435 unique tokens: [',', '.', 'along', 'and', 'are']...)\n",
      "Dictionary(263 unique tokens: [',', '.', 'ability', 'and', 'are']...)\n",
      "Dictionary(342 unique tokens: ['(', ')', '.', '/', '/mediapunch/ipx']...)\n",
      "Dictionary(35 unique tokens: ['!', ',', '.', 'alexa', 'her']...)\n",
      "Dictionary(105 unique tokens: [':', 'against', 'betsy', 'democrats', 'devos']...)\n",
      "Dictionary(248 unique tokens: [\"'s\", ',', '.', 'agreement', 'an']...)\n",
      "Dictionary(382 unique tokens: ['.', '20,000', 'dow', 'finally', 'here']...)\n",
      "Dictionary(191 unique tokens: [',', '-', '.', 'and', 'arctic']...)\n",
      "Dictionary(334 unique tokens: [\"'s\", ',', '.', '1858', '2.3']...)\n",
      "Dictionary(367 unique tokens: [',', '.', 'a', 'academy', 'august']...)\n",
      "Dictionary(139 unique tokens: ['!', \"'s\", '.', '16-year-old', '42']...)\n",
      "Dictionary(390 unique tokens: ['(', ')', ',', '.', 'afp']...)\n",
      "Dictionary(724 unique tokens: ['#', ',', '.', '14', 'a']...)\n",
      "Dictionary(148 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(158 unique tokens: [\"'s\", '.', '100,000', 'aid', 'and']...)\n",
      "Dictionary(140 unique tokens: [\"''\", ',', '.', '``', 'alternative']...)\n",
      "Dictionary(59 unique tokens: ['and', 'antonio', 'corey', 'protin', 'villas-boas']...)\n",
      "Dictionary(470 unique tokens: ['&', \"'s\", ';', 'a', 'case']...)\n",
      "Dictionary(108 unique tokens: [',', '.', '450', 'about', 'according']...)\n",
      "Dictionary(375 unique tokens: [':', 'against', 'betsy', 'democrats', 'devos']...)\n",
      "Dictionary(85 unique tokens: [',', '.', 'a', 'and', 'ashley']...)\n",
      "Dictionary(287 unique tokens: [',', '.', '20', '2016', 'after']...)\n",
      "Dictionary(306 unique tokens: [\"'s\", ',', '.', '8', 'about']...)\n",
      "Dictionary(59 unique tokens: [',', '.', 'and', 'cabinet', 'confirmation']...)\n",
      "Dictionary(151 unique tokens: ['.', 'ahead', 'and', 'are', 'equality']...)\n",
      "Dictionary(162 unique tokens: [\"''\", \"'m\", \"'s\", ',', '.']...)\n",
      "Dictionary(329 unique tokens: [',', '.', 'a', 'according', 'administration']...)\n",
      "Dictionary(137 unique tokens: ['2', 'related', '1', \"'\", ',']...)\n",
      "Dictionary(146 unique tokens: [',', '.', 'after', 'an', 'attempted']...)\n",
      "Dictionary(316 unique tokens: ['.', '12', 'a', 'after', 'an']...)\n",
      "Dictionary(290 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(160 unique tokens: ['bi', 'intelligence', ',', '17.11.2016', '23:14']...)\n",
      "Dictionary(195 unique tokens: ['.', 'a', 'after', 'are', 'children']...)\n",
      "Dictionary(243 unique tokens: [',', '.', '10', 'a', 'an']...)\n",
      "Dictionary(332 unique tokens: [',', '.', '14-year-old', '30', 'a']...)\n",
      "Dictionary(240 unique tokens: [',', '.', 'a-list', 'actress', 'and']...)\n",
      "Dictionary(85 unique tokens: [',', '.', 'be', 'cover', 'donald']...)\n",
      "Dictionary(156 unique tokens: [',', '20', '2017', '8:58', 'am']...)\n",
      "Dictionary(256 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(634 unique tokens: ['more', ',', '.', 'about', 'choice']...)\n",
      "Dictionary(393 unique tokens: [',', '.', 'a', 'absence', 'after']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(278 unique tokens: ['garber', 'jonathan', ',', '11.01.2017', '12:50']...)\n",
      "Dictionary(334 unique tokens: [\"''\", '.', '``', 'can', 'did']...)\n",
      "Dictionary(340 unique tokens: ['.', 'alabama', 'american', 'believe', 'immigration']...)\n",
      "Dictionary(220 unique tokens: ['airline', 'and', 'board', 'drink', 'food']...)\n",
      "Dictionary(301 unique tokens: ['more', ',', '.', '1,000', 'be']...)\n",
      "Dictionary(165 unique tokens: ['.', '?', 'a', 'be', 'breastfeed']...)\n",
      "Dictionary(271 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(867 unique tokens: [',', '.', 'and', 'any', 'bbc']...)\n",
      "Dictionary(424 unique tokens: [',', '.', 'actress', 'and', 'art']...)\n",
      "Dictionary(704 unique tokens: ['.', ':', 'a', 'advice', 'adviser']...)\n",
      "Dictionary(460 unique tokens: [\"''\", ',', '.', ':', '``']...)\n",
      "Dictionary(155 unique tokens: [',', '.', 'all', 'as', 'be']...)\n",
      "Dictionary(153 unique tokens: ['.', '300lbs', 'a', 'accomplice', 'an']...)\n",
      "Dictionary(134 unique tokens: [',', '.', 'a', 'accidently', 'an']...)\n",
      "Dictionary(212 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(140 unique tokens: ['4', 'and', 'back', 'british', 'but']...)\n",
      "Dictionary(310 unique tokens: ['10', 'cars', 'diesel', 'emissions', 'eu']...)\n",
      "Dictionary(361 unique tokens: ['by', 'daniel', 'mcdermon', ',', '.']...)\n",
      "Dictionary(250 unique tokens: [',', '.', 'a', 'and', 'assessment']...)\n",
      "Dictionary(356 unique tokens: ['.', 'a', 'behind-the-scenes', 'bridges', 'build']...)\n",
      "Dictionary(311 unique tokens: [',', '.', '10,000', '100', '730']...)\n",
      "Dictionary(320 unique tokens: [',', '.', '32', 'a', 'activists']...)\n",
      "Dictionary(294 unique tokens: [',', '.', '1945', 'announced', 'australian']...)\n",
      "Dictionary(593 unique tokens: [',', '.', '1968', 'a', 'according']...)\n",
      "Dictionary(32 unique tokens: ['2017', '9', ':', 'in', 'nominations']...)\n",
      "Dictionary(66 unique tokens: [',', '.', '50', 'a', 'across']...)\n",
      "Dictionary(406 unique tokens: [',', '.', 'a', 'and', 'are']...)\n",
      "Dictionary(163 unique tokens: [',', '.', 'a', 'and', 'comes']...)\n",
      "Dictionary(777 unique tokens: [',', '1', 'day', 'islands', 'january']...)\n",
      "Dictionary(350 unique tokens: ['more', '.', 'a', 'about', 'classic']...)\n",
      "Dictionary(161 unique tokens: [',', '.', 'a', 'and', 'angeles']...)\n",
      "Dictionary(111 unique tokens: ['.', '1st', '2017', 'already', 'and']...)\n",
      "Dictionary(503 unique tokens: ['more', '.', '31', '?', 'a']...)\n",
      "Dictionary(242 unique tokens: ['.', '2016', 'behind', 'believes', 'committee']...)\n",
      "Dictionary(199 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(243 unique tokens: [',', '--', '.', 'abdominal', 'an']...)\n",
      "Dictionary(203 unique tokens: ['.', ':', '@', 'a', 'dc-area']...)\n",
      "Dictionary(222 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(252 unique tokens: [\"'\", \"'sleepless\", 'before', 'chamberlin', 'decision']...)\n",
      "Dictionary(510 unique tokens: [\"''\", '(', ')', ',', '--']...)\n",
      "Dictionary(302 unique tokens: [',', '.', '8,000', 'a', 'announced']...)\n",
      "Dictionary(209 unique tokens: ['(', ')', '.', 'a', 'after']...)\n",
      "Dictionary(555 unique tokens: [',', '.', '2016', 'a', 'and']...)\n",
      "Dictionary(687 unique tokens: ['by', 'daley', 'john', '.', 'a']...)\n",
      "Dictionary(707 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(440 unique tokens: [',', '.', 'and', 'at', 'but']...)\n",
      "Dictionary(94 unique tokens: [',', '.', 'a', 'an', 'closely']...)\n",
      "Dictionary(647 unique tokens: [',', '.', '96', 'a', 'adams']...)\n",
      "Dictionary(1072 unique tokens: [',', '.', ':', '?', 'a']...)\n",
      "Dictionary(201 unique tokens: [',', '.', '...', 'brexit', 'now']...)\n",
      "Dictionary(130 unique tokens: ['.', 'cat', 'comedic', 'is', 'purr-fect']...)\n",
      "Dictionary(348 unique tokens: ['.', 'a', 'after', 'arrested', 'at']...)\n",
      "Dictionary(889 unique tokens: ['&', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(232 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(326 unique tokens: [',', '--', '.', 'all', 'at']...)\n",
      "Dictionary(184 unique tokens: [',', '.', 'a', 'about', 'american']...)\n",
      "Dictionary(414 unique tokens: ['.', 'a', 'activists', 'and', 'around']...)\n",
      "Dictionary(228 unique tokens: [',', '.', 'according', 'after', 'apparent']...)\n",
      "Dictionary(134 unique tokens: [',', '.', 'appears', 'co.', 'debut']...)\n",
      "Dictionary(115 unique tokens: [\"'s\", '.', 'cheer', 'children', 'has']...)\n",
      "Dictionary(65 unique tokens: ['.', 'and', 'by', 'christmas', 'cities']...)\n",
      "Dictionary(295 unique tokens: [',', '.', 'and', 'chicken', 'fried']...)\n",
      "Dictionary(890 unique tokens: [':', 'against', 'betsy', 'democrats', 'devos']...)\n",
      "Dictionary(446 unique tokens: [',', '--', '.', 'a', 'and']...)\n",
      "Dictionary(371 unique tokens: ['more', '!', '.', 'family', 'feud']...)\n",
      "Dictionary(589 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(173 unique tokens: ['!', '&', \"'\", \"''\", \"'great\"]...)\n",
      "Dictionary(589 unique tokens: [\"'s\", ',', '.', '2017', 'a']...)\n",
      "Dictionary(168 unique tokens: [\"'s\", ',', '.', '140', 'a']...)\n",
      "Dictionary(415 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(280 unique tokens: ['#', ',', '-', '.', 'alabed']...)\n",
      "Dictionary(377 unique tokens: [',', '.', 'a', 'already', 'christmas']...)\n",
      "Dictionary(466 unique tokens: [',', '.', 'ancient', 'and', 'art']...)\n",
      "Dictionary(140 unique tokens: ['2017', 'at', 'in', 'kicks', 'off']...)\n",
      "Dictionary(90 unique tokens: [',', '.', '10-percent', 'a', 'app']...)\n",
      "Dictionary(343 unique tokens: [\"''\", ',', '.', '61-year-old', '``']...)\n",
      "Dictionary(107 unique tokens: ['.', '50-year-old', '9news', ':', 'a']...)\n",
      "Dictionary(223 unique tokens: [',', '.', 'expensive', 'for', 'instant']...)\n",
      "Dictionary(452 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(372 unique tokens: ['.', '2016', 'bank', 'be', 'blamed']...)\n",
      "Dictionary(2080 unique tokens: [',', '.', '2015', 'about', 'adrian']...)\n",
      "Dictionary(156 unique tokens: ['davis', 'scott', ',', '14.12.2016', '17:41']...)\n",
      "Dictionary(299 unique tokens: ['.', 'aleppo', 'an', 'and', 'been']...)\n",
      "Dictionary(173 unique tokens: [',', '?', 'about', 'agree', 'all']...)\n",
      "Dictionary(308 unique tokens: [',', '.', 'a', 'and', 'batteries']...)\n",
      "Dictionary(153 unique tokens: [',', '.', 'most', 'of', 'them']...)\n",
      "Dictionary(223 unique tokens: ['-', '.', 'an', 'are', 'armed']...)\n",
      "Dictionary(368 unique tokens: [',', '.', 'about', 'bottles', 'french']...)\n",
      "Dictionary(105 unique tokens: ['(', ')', ',', '.', '?']...)\n",
      "Dictionary(631 unique tokens: [\"'s\", ',', '-', '.', 'a']...)\n",
      "Dictionary(1292 unique tokens: ['after', 'again', 'and', 'another', 'are']...)\n",
      "Dictionary(13 unique tokens: [',', '.', '15', '69', 'bombings']...)\n",
      "Dictionary(56 unique tokens: [',', '.', 'a', 'along', 'and']...)\n",
      "Dictionary(506 unique tokens: [\"'s\", 'and', 'dangerous', 'death', 'in']...)\n",
      "Dictionary(765 unique tokens: [',', '.', '2009', 'a', 'ab-soul']...)\n",
      "Dictionary(297 unique tokens: ['mcalone', 'nathan', ',', '23:30', '40']...)\n",
      "Dictionary(262 unique tokens: ['.', 'a', 'ago', 'all', 'announced']...)\n",
      "Dictionary(113 unique tokens: ['1', 'related', '2', '3', \"''\"]...)\n",
      "Dictionary(646 unique tokens: ['(', ')', ',', '.', '2016']...)\n",
      "Dictionary(210 unique tokens: ['3', 'related', \"'s\", ',', '.']...)\n",
      "Dictionary(429 unique tokens: ['.', 'a', 'asks', 'heads', 'hidden']...)\n",
      "Dictionary(144 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(114 unique tokens: [',', '18:00', '28.08.2016', '328', 'alexa']...)\n",
      "Dictionary(180 unique tokens: [\"''\", '.', '2016', '``', 'a']...)\n",
      "Dictionary(163 unique tokens: [',', '.', '59th', 'a', 'annual']...)\n",
      "Dictionary(122 unique tokens: [\"'s\", ',', '--', '.', '1']...)\n",
      "Dictionary(469 unique tokens: ['.', 'relax', 'breathe', 'a', 'an']...)\n",
      "Dictionary(55 unique tokens: ['$', ',', '.', '1', '25']...)\n",
      "Dictionary(388 unique tokens: ['.', 'and', 'at', 'banks', 'bonds']...)\n",
      "Dictionary(69 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(368 unique tokens: [':', 'actively', 'and', 'be', 'charity']...)\n",
      "Dictionary(488 unique tokens: ['@', 'independent.co.uk', 'letters', 'please', 'send']...)\n",
      "Dictionary(51 unique tokens: ['\\uea77', '#', '.', 'favorite', 'hashtag']...)\n",
      "Dictionary(58 unique tokens: ['.', 'a', 'at', 'beijing', 'beijing—a']...)\n",
      "Dictionary(270 unique tokens: [',', '10:29', '2', '2016', 'dec']...)\n",
      "Dictionary(403 unique tokens: [',', '.', '40-minute', 'a', 'album']...)\n",
      "Dictionary(61 unique tokens: [',', '.', ':', 'and', 'art-gallery']...)\n",
      "Dictionary(301 unique tokens: ['$', \"'\", \"''\", \"'dirty\", \"'hey\"]...)\n",
      "Dictionary(139 unique tokens: ['.', 'and', 'done', 'he', 'one']...)\n",
      "Dictionary(472 unique tokens: [':', 'biden', 'floor', 'honored', 'joe']...)\n",
      "Dictionary(165 unique tokens: [',', '.', 'a', 'and', 'baguettes']...)\n",
      "Dictionary(166 unique tokens: [',', '.', '2015', '24', 'a']...)\n",
      "Dictionary(456 unique tokens: [',', '.', 'a', 'an', 'as']...)\n",
      "Dictionary(209 unique tokens: [',', '--', '.', 'a', 'beast']...)\n",
      "Dictionary(299 unique tokens: [',', '.', ':', 'a', 'about']...)\n",
      "Dictionary(499 unique tokens: [',', '.', 'all', 'are', 'bird']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(281 unique tokens: ['.', 'all', 'and', 'fun', 'games']...)\n",
      "Dictionary(369 unique tokens: [\"'s\", '.', 'annual', 'five', 'from']...)\n",
      "Dictionary(392 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(431 unique tokens: ['.', 'and', 'bobby', 'by', 'eyes']...)\n",
      "Dictionary(437 unique tokens: [\"'s\", ',', 'among', 'and', 'been']...)\n",
      "Dictionary(200 unique tokens: [',', '.', '47', 'a', 'been']...)\n",
      "Dictionary(167 unique tokens: ['.', '7', 'die', 'galaxy', 'never']...)\n",
      "Dictionary(79 unique tokens: ['.', 'administration', 'again', 'america', 'announced']...)\n",
      "Dictionary(280 unique tokens: [',', '.', 'and', 'be', 'but']...)\n",
      "Dictionary(108 unique tokens: ['$', '//t.co/9yfp6sdynl', '5,000', ':', 'a']...)\n",
      "Dictionary(429 unique tokens: [\"'\", '--', '.', 'a', 'americans']...)\n",
      "Dictionary(102 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(97 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(277 unique tokens: [',', '.', 'a', 'americans', 'and']...)\n",
      "Dictionary(356 unique tokens: [\"'s\", '.', 'about', 'earth', 'how']...)\n",
      "Dictionary(51 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(144 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(1026 unique tokens: [\"''\", '.', '?', '``', 'abc']...)\n",
      "Dictionary(335 unique tokens: [':', 'a', 'blast', 'conor', 'from']...)\n",
      "Dictionary(314 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(467 unique tokens: ['more', ',', '.', 'a', 'according']...)\n",
      "Dictionary(423 unique tokens: [',', '.', 'a', 'after', 'apple']...)\n",
      "Dictionary(409 unique tokens: [',', '.', '19', 'a', 'amid']...)\n",
      "Dictionary(204 unique tokens: ['(', ')', ',', '.', '10th']...)\n",
      "Dictionary(543 unique tokens: ['as', 'attack', 'campaign', 'cancel', 'candidates']...)\n",
      "Dictionary(259 unique tokens: ['.', 'along', 'began', 'champs-élysées', 'described']...)\n",
      "Dictionary(481 unique tokens: [',', '.', '24', 'a', 'arrested']...)\n",
      "Dictionary(82 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(70 unique tokens: [',', '.', 'a', 'and', 'before']...)\n",
      "Dictionary(195 unique tokens: ['.', '15', 'a', 'airborne', 'aircraft']...)\n",
      "Dictionary(866 unique tokens: [\"''\", ',', '.', '12', '``']...)\n",
      "Dictionary(640 unique tokens: ['.', ':', ';', '@', 'and']...)\n",
      "Dictionary(53 unique tokens: [',', '.', '2015', 'administration', 'agreement']...)\n",
      "Dictionary(494 unique tokens: [\"''\", ',', '.', ':', '``']...)\n",
      "Dictionary(209 unique tokens: [\"'\", \"'s\", ',', '.', '10']...)\n",
      "Dictionary(315 unique tokens: ['.', 'a', 'and', 'basically', 'believes']...)\n",
      "Dictionary(243 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(33 unique tokens: [\"'\", ',', '.', 'a', 'along']...)\n",
      "Dictionary(354 unique tokens: [\"''\", ',', '.', '``', 'an']...)\n",
      "Dictionary(138 unique tokens: ['.', '?', 'appeared', 'contact', 'exits']...)\n",
      "Dictionary(303 unique tokens: [\"'d\", \"'s\", '.', ':', 'an']...)\n",
      "Dictionary(356 unique tokens: [\"'s\", ',', '.', 'a', 'crisis']...)\n",
      "Dictionary(294 unique tokens: [\"'s\", ',', '.', 'a', 'boost']...)\n",
      "Dictionary(83 unique tokens: ['.', 'chappelle', 'charlie', 'comedian', 'dave']...)\n",
      "Dictionary(145 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(229 unique tokens: [',', '.', 'a', 'according', 'creation']...)\n",
      "Dictionary(634 unique tokens: [',', '?', 'and', 'approach', 'are']...)\n",
      "Dictionary(319 unique tokens: [',', '.', 'against', 'budapest', 'by']...)\n",
      "Dictionary(234 unique tokens: ['.', 'against', 'air', 'airstrikes', 'an']...)\n",
      "Dictionary(27 unique tokens: [',', '.', 'a', 'are', 'battle']...)\n",
      "Dictionary(456 unique tokens: ['more', \"''\", '.', '``', 'a']...)\n",
      "Dictionary(257 unique tokens: [\"'ve\", ',', '.', 'a', 'about']...)\n",
      "Dictionary(154 unique tokens: ['$', \"''\", \"'s\", '(', ')']...)\n",
      "Dictionary(400 unique tokens: ['&', '.', 'a', 'african', 'and']...)\n",
      "Dictionary(565 unique tokens: [',', 'air', 'and', 'by', 'cheating']...)\n",
      "Dictionary(572 unique tokens: [',', '.', 'a', 'about', 'administration']...)\n",
      "Dictionary(159 unique tokens: ['.', 'family', 'force', 'remains', 'star']...)\n",
      "Dictionary(78 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(366 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(214 unique tokens: [',', '.', 'age', 'and', 'are']...)\n",
      "Dictionary(36 unique tokens: [\"'d\", ',', '.', 'a', 'accomplished']...)\n",
      "Dictionary(368 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(147 unique tokens: ['03', '2017', 'apr', \"''\", ',']...)\n",
      "Dictionary(146 unique tokens: [',', '.', '200', 'a', 'after']...)\n",
      "Dictionary(152 unique tokens: [',', '.', 'airport', 'and', 'bombs']...)\n",
      "Dictionary(211 unique tokens: ['.', '21-year-old', 'a', 'about', 'by']...)\n",
      "Dictionary(146 unique tokens: [',', 'akin', 'business', 'insider', 'oyedele']...)\n",
      "Dictionary(209 unique tokens: ['1973', 'about', 'blanco', 'carrero', 'cassandra']...)\n",
      "Dictionary(174 unique tokens: ['20', 'from', 'has', 'in', 'last']...)\n",
      "Dictionary(145 unique tokens: [\"'s\", '.', 'all', 'and', 'as']...)\n",
      "Dictionary(207 unique tokens: ['.', '16-year-old', 'a', 'and', 'friday']...)\n",
      "Dictionary(69 unique tokens: [',', '.', '2016', 'alleged', 'by']...)\n",
      "Dictionary(150 unique tokens: [',', '.', 'a', 'according', 'believe']...)\n",
      "Dictionary(192 unique tokens: [\"'\", \"'sahayak\", '.', 'a', 'about']...)\n",
      "Dictionary(485 unique tokens: ['more', ',', '.', 'a', 'about']...)\n",
      "Dictionary(135 unique tokens: [',', '.', 'a', 'after', 'an']...)\n",
      "Dictionary(332 unique tokens: [',', '.', 'already', 'humming', 'just']...)\n",
      "Dictionary(116 unique tokens: ['.', 'a', 'aaron', 'also', 'as']...)\n",
      "Dictionary(203 unique tokens: [\"''\", \"'s\", ',', '.', '10']...)\n",
      "Dictionary(295 unique tokens: [',', '.', 'another', 'disney', 'for']...)\n",
      "Dictionary(251 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(182 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(58 unique tokens: [',', '.', '100', 'a', 'an']...)\n",
      "Dictionary(429 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(336 unique tokens: [',', '.', '60', '8:30', '9:30']...)\n",
      "Dictionary(138 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(184 unique tokens: [',', '.', 'and', 'as', 'before']...)\n",
      "Dictionary(155 unique tokens: [\"'\", \"''\", \"'gun\", \"'incidental\", \"'independent\"]...)\n",
      "Dictionary(324 unique tokens: ['.', 'a', 'afford', 'as', 'city']...)\n",
      "Dictionary(86 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(410 unique tokens: [',', '.', 'a', 'as', 'at']...)\n",
      "Dictionary(166 unique tokens: ['$', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(121 unique tokens: ['.', '20', 'day', 'happiness', 'international']...)\n",
      "Dictionary(566 unique tokens: ['more', ',', '.', '?', 'a']...)\n",
      "Dictionary(208 unique tokens: [',', '.', 'acknowledged', 'and', 'backed']...)\n",
      "Dictionary(50 unique tokens: [',', '.', 'coming', 'contrarian', 'economy']...)\n",
      "Dictionary(200 unique tokens: [',', '.', 'don', 'hate', 'just']...)\n",
      "Dictionary(238 unique tokens: [\"'\", '--', '.', 'a', 'cardinals']...)\n",
      "Dictionary(22 unique tokens: [\"'s\", '-', '.', 'appearance', 'at']...)\n",
      "Dictionary(172 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(460 unique tokens: [\"'s\", 'a', 'and', 'briefing', 'day']...)\n",
      "Dictionary(333 unique tokens: [\"'s\", '.', '2017', 'an', 'award']...)\n",
      "Dictionary(314 unique tokens: ['.', '//www.facebook.com/nbcnews', '//www.snapchat.com/add/nbcnews', '75', ':']...)\n",
      "Dictionary(62 unique tokens: ['15', '2017', 'mar', \"'\", \"'ninots\"]...)\n",
      "Dictionary(157 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(362 unique tokens: ['&', ',', '.', 'for', 'gaga']...)\n",
      "Dictionary(648 unique tokens: [',', '.', 'a', 'afghanistan', 'at']...)\n",
      "Dictionary(321 unique tokens: ['more', 'a', 'and', 'champ', 'em']...)\n",
      "Dictionary(132 unique tokens: ['.', 'afghanistan', 'aid', 'an', 'australian']...)\n",
      "Dictionary(187 unique tokens: [\"'\", \"'beauty\", \"'gay\", ',', '.']...)\n",
      "Dictionary(254 unique tokens: [',', '--', '.', '62', 'a']...)\n",
      "Dictionary(77 unique tokens: [',', '.', '?', 'a', 'are']...)\n",
      "Dictionary(247 unique tokens: [\"''\", ',', '.', '``', 'after']...)\n",
      "Dictionary(177 unique tokens: ['#', '?', '@', 'als', 'andylenz']...)\n",
      "Dictionary(437 unique tokens: [',', '.', '2016', 'abc', 'administration']...)\n",
      "Dictionary(489 unique tokens: ['.', 'american', 'and', 'at', 'because']...)\n",
      "Dictionary(103 unique tokens: ['.', 'and', 'back', 'got', 'straight']...)\n",
      "Dictionary(428 unique tokens: ['.', 'an', 'english', 'flower', 'garden']...)\n",
      "Dictionary(394 unique tokens: ['shutterstock', \"'s\", ',', '.', 'a']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(76 unique tokens: [',', '.', '...', ':', 'a']...)\n",
      "Dictionary(274 unique tokens: ['.', 'across', 'and', 'as', 'attorney']...)\n",
      "Dictionary(434 unique tokens: ['(', ')', ',', '.', '10,000']...)\n",
      "Dictionary(161 unique tokens: [',', '.', 'a', 'all', 'and']...)\n",
      "Dictionary(273 unique tokens: ['.', 'a', 'after', 'an', 'and']...)\n",
      "Dictionary(364 unique tokens: ['.', '15', 'a', 'and', 'assets']...)\n",
      "Dictionary(245 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(116 unique tokens: ['$', \"'s\", ',', '.', '72']...)\n",
      "Dictionary(55 unique tokens: ['.', '13', ';', 'already', 'and']...)\n",
      "Dictionary(118 unique tokens: [',', 'alex', 'business', 'insider', 'lockie']...)\n",
      "Dictionary(315 unique tokens: [\"'d\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(411 unique tokens: [',', '.', ':', 'a', 'as']...)\n",
      "Dictionary(149 unique tokens: [\"''\", \"'s\", ',', '.', '48']...)\n",
      "Dictionary(707 unique tokens: [':', 'a', 'and', 'club', 'donor']...)\n",
      "Dictionary(183 unique tokens: [',', '.', 'a', 'all', 'bad']...)\n",
      "Dictionary(222 unique tokens: [\"'s\", '.', '2017', 'about', 'an']...)\n",
      "Dictionary(233 unique tokens: ['(', ')', '.', 'adviser', 'cedric']...)\n",
      "Dictionary(146 unique tokens: ['.', 'after', 'asked', 'blocked', 'committee']...)\n",
      "Dictionary(125 unique tokens: ['.', 'are', 'babies', 'born', 'just']...)\n",
      "Dictionary(346 unique tokens: [',', '-', '.', 'a', 'amazon']...)\n",
      "Dictionary(250 unique tokens: ['20,000', 'ago', 'first', 'for', 'history']...)\n",
      "Dictionary(358 unique tokens: ['author', 'contact', ',', '.', '20-year-old']...)\n",
      "Dictionary(1635 unique tokens: ['.', 'approaches', 'as', 'aspiring', 'athletes']...)\n",
      "Dictionary(482 unique tokens: ['$', ',', '.', '1', 'a']...)\n",
      "Dictionary(382 unique tokens: [',', '.', 'a', 'around', 'as']...)\n",
      "Dictionary(71 unique tokens: ['.', 'art', 'as', 'classroom', 'ed']...)\n",
      "Dictionary(720 unique tokens: [',', '.', 'a', 'away', 'been']...)\n",
      "Dictionary(64 unique tokens: [',', '.', ';', 'a', 'about']...)\n",
      "Dictionary(150 unique tokens: ['.', 'a', 'are', 'can', 'empowerment']...)\n",
      "Dictionary(166 unique tokens: [',', '19', 'and', 'causes', 'dead']...)\n",
      "Dictionary(343 unique tokens: ['%', ',', '40', 'as', 'by']...)\n",
      "Dictionary(253 unique tokens: ['.', 'and', 'as', 'asia-pacific', 'australian']...)\n",
      "Dictionary(178 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(450 unique tokens: [',', '.', '105.1', 'about', 'an']...)\n",
      "Dictionary(481 unique tokens: [',', '.', 'and', 'are', 'beautiful']...)\n",
      "Dictionary(248 unique tokens: [',', '44-year-old', 'about', 'after', 'and']...)\n",
      "Dictionary(415 unique tokens: [',', '.', 'a', 'all', 'and']...)\n",
      "Dictionary(357 unique tokens: [',', '.', '15', 'a', 'an']...)\n",
      "Dictionary(462 unique tokens: ['.', 'air', 'all', 'apprehension', 'as']...)\n",
      "Dictionary(585 unique tokens: [\"'re\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(48 unique tokens: ['.', 'a', 'about', 'age', 'and']...)\n",
      "Dictionary(101 unique tokens: ['.', 'a', 'active', 'an', 'bargained']...)\n",
      "Dictionary(395 unique tokens: ['(', ')', ',', '.', ';']...)\n",
      "Dictionary(212 unique tokens: [',', '.', '49ers', 'a', 'an']...)\n",
      "Dictionary(288 unique tokens: [',', '.', 'a', 'and', 'are']...)\n",
      "Dictionary(262 unique tokens: [',', '.', 'a', 'alien', 'and']...)\n",
      "Dictionary(461 unique tokens: ['.', 'a', 'affirmed', 'also', 'as']...)\n",
      "Dictionary(123 unique tokens: ['.', 'a', 'be', 'can', 'finding']...)\n",
      "Dictionary(174 unique tokens: [\"'s\", '.', 'a', 'and', 'balvin']...)\n",
      "Dictionary(555 unique tokens: ['.', 'a', 'at', 'by', 'city']...)\n",
      "Dictionary(210 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(349 unique tokens: [',', '.', 'a', 'an', 'appeals']...)\n",
      "Dictionary(107 unique tokens: ['.', 'a', 'african', 'almost', 'annual']...)\n",
      "Dictionary(143 unique tokens: [',', '.', ':', 'a', 'atlanta']...)\n",
      "Dictionary(481 unique tokens: ['by', 'dr', 'for', 'life', 'matters']...)\n",
      "Dictionary(63 unique tokens: [',', '.', '1976', 'a', 'back']...)\n",
      "Dictionary(392 unique tokens: [',', '.', '2013', 'algarve', 'along']...)\n",
      "Dictionary(529 unique tokens: ['more', '.', 'a', 'be', 'been']...)\n",
      "Dictionary(527 unique tokens: ['.', 'a', 'age', 'bronze', 'by']...)\n",
      "Dictionary(62 unique tokens: ['?', 'against', 'campaign', 'candidate', 'donald']...)\n",
      "Dictionary(480 unique tokens: ['$', ',', '.', '66,000', 'a']...)\n",
      "Dictionary(689 unique tokens: ['more', \"''\", '(', ')', ',']...)\n",
      "Dictionary(376 unique tokens: [\"'s\", ',', '.', 'a', 'alexandria']...)\n",
      "Dictionary(646 unique tokens: ['$', \"'s\", '.', '20', '35-minute']...)\n",
      "Dictionary(261 unique tokens: [',', '.', 'a', 'accept', 'are']...)\n",
      "Dictionary(336 unique tokens: ['a', 'association-commissioned', 'be', 'by', 'end']...)\n",
      "Dictionary(244 unique tokens: ['.', 'childhood', 'from', 'innocent', 'is']...)\n",
      "Dictionary(204 unique tokens: [',', '.', 'allegedly', 'altercation', 'an']...)\n",
      "Dictionary(198 unique tokens: [\"'\", \"''\", \"'colluded\", \"'m\", \"'s\"]...)\n",
      "Dictionary(107 unique tokens: ['(', ')', '@', 'a', 'by']...)\n",
      "Dictionary(571 unique tokens: [',', '.', '45-year-old', 'a', 'abel']...)\n",
      "Dictionary(506 unique tokens: [',', '.', 'act', 'administration', 'affordable']...)\n",
      "Dictionary(178 unique tokens: [',', 'alex', 'business', 'heath', 'insider']...)\n",
      "Dictionary(271 unique tokens: [',', '.', 'be', 'britons', 'but']...)\n",
      "Dictionary(394 unique tokens: [',', '.', 'a', 'and', 'anything']...)\n",
      "Dictionary(255 unique tokens: ['.', 'a', 'action', 'and', 'as']...)\n",
      "Dictionary(74 unique tokens: ['.', 'advisor', 'at', 'colbert', 'donald']...)\n",
      "Dictionary(256 unique tokens: ['.', '59th', 'a', 'annual', 'as']...)\n",
      "Dictionary(562 unique tokens: ['!', '(', ')', ',', '.paak']...)\n",
      "Dictionary(524 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(239 unique tokens: [\"''\", '.', '``', 'and', 'bring']...)\n",
      "Dictionary(576 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(302 unique tokens: [\"'s\", ',', '.', 'a', 'across']...)\n",
      "Dictionary(262 unique tokens: [',', '.', ';', 'a', 'adept']...)\n",
      "Dictionary(68 unique tokens: ['&', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(276 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(288 unique tokens: ['.', 'a', 'al-qaeda', 'aleppo', 'and']...)\n",
      "Dictionary(94 unique tokens: [\"'s\", '.', 'actor', 'and', 'at']...)\n",
      "Dictionary(409 unique tokens: [\"'s\", '.', 'academy', 'awards', 'be']...)\n",
      "Dictionary(207 unique tokens: [',', '.', '2012', '22nd', 'an']...)\n",
      "Dictionary(173 unique tokens: [',', '.', ':', 'a', 'are']...)\n",
      "Dictionary(104 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(235 unique tokens: [\"'s\", ',', '.', 'as', 'australian']...)\n",
      "Dictionary(453 unique tokens: [',', '.', '2008', '2016', 'after']...)\n",
      "Dictionary(419 unique tokens: ['.', 'a', 'come', 'has', 'long']...)\n",
      "Dictionary(106 unique tokens: ['.', 'as', 'old', 'tale', 'time']...)\n",
      "Dictionary(591 unique tokens: [',', '.', 'a', 'administration', 'but']...)\n",
      "Dictionary(275 unique tokens: ['.', 'a', 'at', 'be', 'blue']...)\n",
      "Dictionary(188 unique tokens: ['$', \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(613 unique tokens: [',', '.', 'almost', 'an', 'and']...)\n",
      "Dictionary(201 unique tokens: [\"'re\", \"'s\", ',', '.', 'amazon']...)\n",
      "Dictionary(476 unique tokens: [\"'s\", '(', ')', '.', ':']...)\n",
      "Dictionary(266 unique tokens: ['.', 'a', 'are', 'autism', 'children']...)\n",
      "Dictionary(417 unique tokens: ['$', ',', '.', '140,000', '2017']...)\n",
      "Dictionary(115 unique tokens: ['!', \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(367 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(650 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(480 unique tokens: [',', '.', 'a', 'and', 'astronauts']...)\n",
      "Dictionary(362 unique tokens: [',', '.', 'a', 'according', 'adding']...)\n",
      "Dictionary(431 unique tokens: [',', '.', ':', 'about', 'actually']...)\n",
      "Dictionary(97 unique tokens: [\"''\", \"'s\", '(', ')', '.']...)\n",
      "Dictionary(86 unique tokens: ['?', 'are', 'books', 'days', 'devices']...)\n",
      "Dictionary(272 unique tokens: ['a', 'chase', 'comes', 'despite', 'from']...)\n",
      "Dictionary(238 unique tokens: ['.', 'also', 'another', 'brought', 'carried']...)\n",
      "Dictionary(233 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(331 unique tokens: [',', '.', 'a', 'address', 'campaign']...)\n",
      "Dictionary(57 unique tokens: [',', '.', '89th', 'academy', 'after']...)\n",
      "Dictionary(226 unique tokens: ['.', 'adams', 'amy', 'snubbed', 'was']...)\n",
      "Dictionary(144 unique tokens: [\"'s\", ',', '.', '10', 'after']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1483 unique tokens: [',', '.', ':', 'alike', 'all-star']...)\n",
      "Dictionary(50 unique tokens: ['.', 'and', 'are', 'bowl', 'championships']...)\n",
      "Dictionary(496 unique tokens: [',', '.', 'are', 'around', 'first-time']...)\n",
      "Dictionary(222 unique tokens: ['and', 'brexit', 'can', 'case', 'decides']...)\n",
      "Dictionary(224 unique tokens: [',', '.', '7', 'a', 'all']...)\n",
      "Dictionary(521 unique tokens: ['more', ',', '.', 'a', 'airlines']...)\n",
      "Dictionary(451 unique tokens: [',', '.', 'a', 'address', 'america']...)\n",
      "Dictionary(83 unique tokens: ['danielle', 'muoio', ',', '15:37', '21.01.2017']...)\n",
      "Dictionary(608 unique tokens: ['creditdoug', 'mills/the', 'new', 'times', 'york']...)\n",
      "Dictionary(544 unique tokens: [',', '.', '40th', 'a', 'and']...)\n",
      "Dictionary(246 unique tokens: [\"'\", \"'we\", 'all', 'and', 'hands']...)\n",
      "Dictionary(181 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(116 unique tokens: [',', '.', 'and', 'are', 'bloomberg']...)\n",
      "Dictionary(256 unique tokens: [':', 'against', 'betsy', 'democrats', 'devos']...)\n",
      "Dictionary(347 unique tokens: ['more', '!', '(', ')', ',']...)\n",
      "Dictionary(387 unique tokens: ['(', ')', ',', '.', '1990']...)\n",
      "Dictionary(333 unique tokens: [',', '--', '.', 'a', 'amid']...)\n",
      "Dictionary(64 unique tokens: ['.', 'a', 'as', 'at', 'away']...)\n",
      "Dictionary(116 unique tokens: ['.', 'a', 'allegedly', 'arrested', 'as']...)\n",
      "Dictionary(440 unique tokens: [',', '.', 'a', 'also', 'an']...)\n",
      "Dictionary(445 unique tokens: ['a', 'guy', 'how', 'i', 'me']...)\n",
      "Dictionary(220 unique tokens: ['.', 'a', 'an', 'austria', 'beautiful']...)\n",
      "Dictionary(174 unique tokens: ['!', \"'\", \"''\", \"'bs\", \"'coming\"]...)\n",
      "Dictionary(198 unique tokens: [',', '.', 'a', 'american', 'and']...)\n",
      "Dictionary(189 unique tokens: [',', '.', 'a', 'according', 'accusations']...)\n",
      "Dictionary(613 unique tokens: [\"'s\", '.', 'a', 'as', 'britain']...)\n",
      "Dictionary(494 unique tokens: ['more', ',', 'by', 'graeme', 'hollywood']...)\n",
      "Dictionary(241 unique tokens: ['.', 'a', 'adoptive', 'annoying', 'because']...)\n",
      "Dictionary(50 unique tokens: ['and', 'gene', 'jessica', 'kim', 'orwig']...)\n",
      "Dictionary(117 unique tokens: [',', '.', ':', 'a', 'after']...)\n",
      "Dictionary(207 unique tokens: [',', '.', 'a', 'be', 'but']...)\n",
      "Dictionary(165 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(109 unique tokens: ['chaparro', 'frank', ',', '17:19', '44']...)\n",
      "Dictionary(327 unique tokens: ['more', '(', ')', ',', '.']...)\n",
      "Dictionary(52 unique tokens: [',', '.', 'almost', 'apple', 'birth']...)\n",
      "Dictionary(139 unique tokens: ['.', 'meta', 'so', '1', 'circle']...)\n",
      "Dictionary(189 unique tokens: [',', '.', '18-month', 'a', 'an']...)\n",
      "Dictionary(147 unique tokens: [',', '.', 'actually', 'also', 'another']...)\n",
      "Dictionary(239 unique tokens: [\"'\", \"'ambition-less\", \"'sanskaari\", ',', '.']...)\n",
      "Dictionary(90 unique tokens: ['!', \"'s\", '?', 'any', 'blurry']...)\n",
      "Dictionary(430 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(353 unique tokens: ['(', ')', ',', '.', 'agency']...)\n",
      "Dictionary(147 unique tokens: [',', '.', 'as', 'but', 'faster']...)\n",
      "Dictionary(468 unique tokens: [',', '.', '9', 'administration', 'and']...)\n",
      "Dictionary(456 unique tokens: [',', '.', '2012', ':', ';']...)\n",
      "Dictionary(174 unique tokens: [',', '2012', 'according', 'costs', 'data']...)\n",
      "Dictionary(192 unique tokens: ['elena', 'holodny', ',', '14:12', '4.01.2017']...)\n",
      "Dictionary(444 unique tokens: [\"'s\", ',', '.', 'across', 'alone']...)\n",
      "Dictionary(127 unique tokens: [',', '.', '35', 'a', 'and']...)\n",
      "Dictionary(281 unique tokens: ['.', 'and', 'aren', 'bigger', 'getting']...)\n",
      "Dictionary(154 unique tokens: ['(', ')', ',', '.', '10']...)\n",
      "Dictionary(70 unique tokens: ['chris', 'snyder', ',', '2016', '24,207']...)\n",
      "Dictionary(182 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(235 unique tokens: ['.', 'a', 'associated', 'been', 'broad']...)\n",
      "Dictionary(96 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(154 unique tokens: [\"'s\", ':', 'arguments', 'ban', 'on']...)\n",
      "Dictionary(282 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(193 unique tokens: ['.', '2016', 'by', 'caused', 'charlie']...)\n",
      "Dictionary(676 unique tokens: ['embed', 'facebook', 'google', 'plus', 'twitter']...)\n",
      "Dictionary(180 unique tokens: [\"'\", \"'pay\", ',', '.', 'a']...)\n",
      "Dictionary(495 unique tokens: [',', '.', 'a', 'ago', 'and']...)\n",
      "Dictionary(80 unique tokens: ['.', 'almost', 'and', 'as', 'away']...)\n",
      "Dictionary(207 unique tokens: ['(', ')', '.', 'a', 'arrest']...)\n",
      "Dictionary(506 unique tokens: [',', '.', '2019', ':', 'a']...)\n",
      "Dictionary(322 unique tokens: ['.', 'all', 'and', 'charity', 'claim']...)\n",
      "Dictionary(393 unique tokens: [',', '.', 'actor', 'advocate', 'also']...)\n",
      "Dictionary(268 unique tokens: [',', '.', 'a', 'actually', 'businesses']...)\n",
      "Dictionary(240 unique tokens: [',', '.', 'a', 'and', 'closures']...)\n",
      "Dictionary(220 unique tokens: ['.', 'a', 'ago', 'another', 'article']...)\n",
      "Dictionary(121 unique tokens: ['.', 'deserve', 'don', 'memes', 't']...)\n",
      "Dictionary(248 unique tokens: [',', '--', '.', 'always', 'carolina']...)\n",
      "Dictionary(163 unique tokens: [\"''\", \"'m\", ',', '.', '2017']...)\n",
      "Dictionary(147 unique tokens: ['.', '1', 'and', 'deepika', 'dubai']...)\n",
      "Dictionary(513 unique tokens: ['.', 'an', 'asos', 'collaboration', 'created']...)\n",
      "Dictionary(280 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(480 unique tokens: ['more', 'alister', 'by', 'doyle', '(']...)\n",
      "Dictionary(256 unique tokens: [\"'\", \"'highly\", ',', '.', 'a']...)\n",
      "Dictionary(224 unique tokens: ['(', ')', ',', '.', 'about']...)\n",
      "Dictionary(172 unique tokens: ['1', 'related', '3', \"'s\", ',']...)\n",
      "Dictionary(209 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(189 unique tokens: [\"'80s\", \"'90s\", \"'s\", ',', '.']...)\n",
      "Dictionary(263 unique tokens: ['.', '[', ']', 'faaaace', 'her']...)\n",
      "Dictionary(459 unique tokens: [',', '16', '2016', '2:40', 'dec']...)\n",
      "Dictionary(360 unique tokens: [',', '.', 'al-qaeda-linked', 'aleppo', 'an']...)\n",
      "Dictionary(570 unique tokens: [',', '.', 'a', 'alone', 'and']...)\n",
      "Dictionary(124 unique tokens: [',', '.', 'a', 'and', 'between']...)\n",
      "Dictionary(465 unique tokens: ['.', '1988', ':', 'a', 'atlantic']...)\n",
      "Dictionary(181 unique tokens: ['embed', 'facebook', 'google', 'plus', 'twitter']...)\n",
      "Dictionary(301 unique tokens: ['(', ')', ',', '.', '65']...)\n",
      "Dictionary(201 unique tokens: [',', '.', '2017', '20th', 'announced']...)\n",
      "Dictionary(137 unique tokens: [',', '.', 'a', 'according', 'after']...)\n",
      "Dictionary(34 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(284 unique tokens: ['--', '.', 'a', 'american', 'and']...)\n",
      "Dictionary(583 unique tokens: [',', '.', 'black', 'cole', 'excellence']...)\n",
      "Dictionary(297 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(492 unique tokens: ['$', ',', '.', '1,250', ':']...)\n",
      "Dictionary(342 unique tokens: [',', '.', 'a', 'across', 'and']...)\n",
      "Dictionary(156 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(71 unique tokens: ['.', 'asl', 'knowledge', 'put', 'test']...)\n",
      "Dictionary(255 unique tokens: [',', '.', '14', 'a', 'acl']...)\n",
      "Dictionary(144 unique tokens: [',', '.', 'a', 'and', 'been']...)\n",
      "Dictionary(509 unique tokens: [',', '.', 'a', 'allegedly', 'barack']...)\n",
      "Dictionary(201 unique tokens: [\"'s\", '.', 'a', 'but', 'classic']...)\n",
      "Dictionary(139 unique tokens: ['harrington', 'rebecca', ',', '110', '23:32']...)\n",
      "Dictionary(136 unique tokens: [',', '.', '35', 'a', 'an']...)\n",
      "Dictionary(238 unique tokens: ['.', 'are', 'cats', 'downing', 'five']...)\n",
      "Dictionary(389 unique tokens: [',', '-', '.', '255', 'a']...)\n",
      "Dictionary(148 unique tokens: ['a', 'and', 'be', 'bid', 'both']...)\n",
      "Dictionary(34 unique tokens: [',', '.', 'adviser', 'against', 'and']...)\n",
      "Dictionary(142 unique tokens: ['%', '(', ')', ',', '.']...)\n",
      "Dictionary(229 unique tokens: [',', '.', '2030', '5', 'after']...)\n",
      "Dictionary(277 unique tokens: [',', '.', 'candidate', 'cbs', 'donald']...)\n",
      "Dictionary(127 unique tokens: ['(', ')', ',', '.', '6']...)\n",
      "Dictionary(100 unique tokens: [\"'ll\", '.', 'a', 'ads', 'and']...)\n",
      "Dictionary(277 unique tokens: [\"'\", \"'s\", '.', 'about', 'annan']...)\n",
      "Dictionary(296 unique tokens: [\"'90s\", ',', '.', 'a', 'after']...)\n",
      "Dictionary(103 unique tokens: [\"'s\", ',', '.', 'a', 'allegedly']...)\n",
      "Dictionary(227 unique tokens: ['$', ',', '.', '125', 'a']...)\n",
      "Dictionary(356 unique tokens: [\"'ve\", ',', '.', '...', 'and']...)\n",
      "Dictionary(130 unique tokens: [\"'s\", ',', '.', 'also', 'batted']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(411 unique tokens: [\"'s\", ',', '.', 'a', 'admired']...)\n",
      "Dictionary(480 unique tokens: ['$', ',', '.', '100-million', '?']...)\n",
      "Dictionary(674 unique tokens: ['.', 'and', 'as', 'austrian', 'candidate']...)\n",
      "Dictionary(93 unique tokens: ['gillett', 'lee', 'rachel', 'samantha', 'und']...)\n",
      "Dictionary(443 unique tokens: [\"'s\", '(', ')', ',', '--']...)\n",
      "Dictionary(216 unique tokens: [',', '.', '60', '80', 'a']...)\n",
      "Dictionary(144 unique tokens: ['#', ',', '.', '2016', 'a']...)\n",
      "Dictionary(87 unique tokens: ['.', 'bulls', 'conchi', 'female', 'fighting']...)\n",
      "Dictionary(811 unique tokens: ['vancouver', 'canada', 'cascadia', 'seattle', 'pacific']...)\n",
      "Dictionary(458 unique tokens: ['--', '.', 'a', 'alleged', 'and']...)\n",
      "Dictionary(304 unique tokens: ['$', \"'\", \"'s\", '--', '.']...)\n",
      "Dictionary(400 unique tokens: ['1', 'related', ',', '.', 'a']...)\n",
      "Dictionary(226 unique tokens: [',', '.', 'a', 'after', 'alabama']...)\n",
      "Dictionary(281 unique tokens: [',', '.', 'a', 'an', 'annual']...)\n",
      "Dictionary(336 unique tokens: ['2', 'related', '1', '3', '.']...)\n",
      "Dictionary(300 unique tokens: [\"''\", ',', '.', '``', 'after']...)\n",
      "Dictionary(363 unique tokens: ['(', ')', ',', '.', '11']...)\n",
      "Dictionary(496 unique tokens: [',', 'bort', 'business', 'insider', 'julie']...)\n",
      "Dictionary(15 unique tokens: ['.', 'for', 'patience', 'thank', 'you']...)\n",
      "Dictionary(86 unique tokens: [',', 'business', 'hanbury', 'insider', 'mary']...)\n",
      "Dictionary(174 unique tokens: ['.', 'all', 'be', 'browns', 'cleveland']...)\n",
      "Dictionary(164 unique tokens: [',', '.', 'a', 'according', 'after']...)\n",
      "Dictionary(133 unique tokens: [',', '.', '25', 'a', 'ambushed']...)\n",
      "Dictionary(585 unique tokens: [',', '.', 'accord', 'as', 'carbon']...)\n",
      "Dictionary(532 unique tokens: ['.', 'a', 'answer', 'been', 'by']...)\n",
      "Dictionary(411 unique tokens: [\"'s\", ',', '.', 'a', 'album']...)\n",
      "Dictionary(273 unique tokens: [',', 'david', 'drucker', 'examiner', 'm.']...)\n",
      "Dictionary(278 unique tokens: ['.', 'announcing', 'as', 'by', 'candidate']...)\n",
      "Dictionary(172 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(459 unique tokens: [\"'s\", ',', '.', 'a', 'as']...)\n",
      "Dictionary(448 unique tokens: [',', '.', 'about', 'after', 'already']...)\n",
      "Dictionary(208 unique tokens: ['.', 'and', 'deputy', 'dismissed', 'general']...)\n",
      "Dictionary(292 unique tokens: [',', '.', 'a', 'and', 'as']...)\n",
      "Dictionary(39 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(200 unique tokens: [',', '.', '13', 'a', 'and']...)\n",
      "Dictionary(146 unique tokens: [',', '.', '25-year', 'a', 'and']...)\n",
      "Dictionary(107 unique tokens: [',', '.', 'and', 'as', 'at']...)\n",
      "Dictionary(339 unique tokens: [',', '.', '100,000', '26', 'and']...)\n",
      "Dictionary(167 unique tokens: ['.', 'a', 'again', 'are', 'australian']...)\n",
      "Dictionary(447 unique tokens: ['.', '38-year-old', 'about', 'actor', 'after']...)\n",
      "Dictionary(221 unique tokens: [\"'s\", ',', '.', '117', '1800s']...)\n",
      "Dictionary(289 unique tokens: [\"'s\", 'about', 'artist', 'awards', 'bet']...)\n",
      "Dictionary(498 unique tokens: ['.', 'a', 'allies', 'almost', 'always']...)\n",
      "Dictionary(271 unique tokens: [\"'\", '.', 'about', 'carrots', 'dark']...)\n",
      "Dictionary(28 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(324 unique tokens: ['.', 'a', 'and', 'been', 'buses']...)\n",
      "Dictionary(264 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(320 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(120 unique tokens: [',', '.', 'belief', 'beyond', 'christians']...)\n",
      "Dictionary(538 unique tokens: [',', '.', 'a', 'already', 'and']...)\n",
      "Dictionary(469 unique tokens: [\"'\", \"'s\", \"'the\", '.', 'after']...)\n",
      "Dictionary(140 unique tokens: [',', '.', 'according', 'after', 'battery']...)\n",
      "Dictionary(695 unique tokens: [',', ':', 'and', 'at', 'best']...)\n",
      "Dictionary(106 unique tokens: [',', '.', 'be', 'but', 'during']...)\n",
      "Dictionary(204 unique tokens: ['.', 'as', 'been', 'buffets', 'caledonia']...)\n",
      "Dictionary(137 unique tokens: ['(', ')', ',', '.', '15th']...)\n",
      "Dictionary(304 unique tokens: [',', '.', '100', '38', 'according']...)\n",
      "Dictionary(306 unique tokens: [\"'s\", ',', '.', 'a', 'acclaimed']...)\n",
      "Dictionary(299 unique tokens: [',', '.', 'a', 'activism', 'activists']...)\n",
      "Dictionary(423 unique tokens: [',', '.', 'a', 'adjacent', 'agnew']...)\n",
      "Dictionary(232 unique tokens: [',', '.', '2005', '2016', 'a']...)\n",
      "Dictionary(200 unique tokens: [',', '.', 'a', 'bailed', 'birthday']...)\n",
      "Dictionary(384 unique tokens: [',', '.', 'a', 'and', 'been']...)\n",
      "Dictionary(408 unique tokens: [',', '.', ':', 'a', 'and']...)\n",
      "Dictionary(536 unique tokens: ['more', '(', ')', ',', '.']...)\n",
      "Dictionary(47 unique tokens: [',', '.', 'a', 'after', 'antitrust']...)\n",
      "Dictionary(218 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(616 unique tokens: ['.', 'live', 'ones', 're', 'the']...)\n",
      "Dictionary(214 unique tokens: ['!', '#', \"'\", \"''\", \"'bozos\"]...)\n",
      "Dictionary(205 unique tokens: [',', 'business', 'feloni', 'insider', 'richard']...)\n",
      "Dictionary(461 unique tokens: [',', '.', '2014', ':', 'a']...)\n",
      "Dictionary(414 unique tokens: [',', '.', 'a', 'abstaining', 'all']...)\n",
      "Dictionary(762 unique tokens: ['.', '2000', ':', 'a', 'anyway']...)\n",
      "Dictionary(296 unique tokens: ['.', 'a', 'and', 'between', 'condoleezza']...)\n",
      "Dictionary(20 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(233 unique tokens: ['!', 'cheating', 'sucks', ',', '.']...)\n",
      "Dictionary(402 unique tokens: [',', '.', 'a', 'and', 'big']...)\n",
      "Dictionary(462 unique tokens: ['all', 'and', 'author', 'future', 'generation']...)\n",
      "Dictionary(523 unique tokens: ['?', 'ahead', 'for', 'her', 'lies']...)\n",
      "Dictionary(390 unique tokens: [\"'s\", ',', '.', '30', 'a']...)\n",
      "Dictionary(569 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(30 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(291 unique tokens: [\"''\", '(', ')', '.', ':']...)\n",
      "Dictionary(308 unique tokens: [',', '.', '2002', 'abusing', 'affected']...)\n",
      "Dictionary(157 unique tokens: [\"'s\", ',', '.', '50', 'a']...)\n",
      "Dictionary(118 unique tokens: [',', '.', 'again', 'anandi', 'april']...)\n",
      "Dictionary(490 unique tokens: ['.', 'for', 'machines', 'one', 'score']...)\n",
      "Dictionary(126 unique tokens: ['1', 'related', '2', '300', ':']...)\n",
      "Dictionary(153 unique tokens: ['eight', 'episodes', 'for', 'new', 'next']...)\n",
      "Dictionary(634 unique tokens: [',', 'a', 'and', 'anti-climate', 'anti-science']...)\n",
      "Dictionary(145 unique tokens: [',', '.', ':', 'a', 'and']...)\n",
      "Dictionary(238 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(160 unique tokens: ['affected', 'at', 'attack', 'by', 'henry']...)\n",
      "Dictionary(182 unique tokens: ['.', 'a', 'after', 'allegedly', 'and']...)\n",
      "Dictionary(90 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(208 unique tokens: ['.', '20', 'a', 'afternoon', 'apartment']...)\n",
      "Dictionary(584 unique tokens: ['(', ')', '.', '?', 'briefing']...)\n",
      "Dictionary(302 unique tokens: [',', '.', '15', '2012', ';']...)\n",
      "Dictionary(267 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(56 unique tokens: ['&', \"''\", ',', '.', '2']...)\n",
      "Dictionary(105 unique tokens: [',', 'alex', 'business', 'heath', 'insider']...)\n",
      "Dictionary(674 unique tokens: ['?', 'amsterdam', 'hid', 'jews', 'she']...)\n",
      "Dictionary(216 unique tokens: ['--', '.', 'angeles', 'baby', 'by']...)\n",
      "Dictionary(287 unique tokens: [',', '.', '20', '2016', 'a']...)\n",
      "Dictionary(100 unique tokens: [',', '.', '11', 'arabia', 'attacks']...)\n",
      "Dictionary(706 unique tokens: ['.', ':', 'committee', 'efforts', 'election']...)\n",
      "Dictionary(165 unique tokens: ['(', ')', '.', 'a', 'after']...)\n",
      "Dictionary(294 unique tokens: [\"'s\", ',', '.', '1997', '20']...)\n",
      "Dictionary(120 unique tokens: ['.', 'board', 'booking', 'chelsea', 'clinton']...)\n",
      "Dictionary(596 unique tokens: ['.', 'a', 'as', 'fickle', 'french']...)\n",
      "Dictionary(13 unique tokens: ['a', 'bottle…', 'flips', 'if', 'more']...)\n",
      "Dictionary(326 unique tokens: [',', '.', 'and', 'arrived', 'asia']...)\n",
      "Dictionary(356 unique tokens: [',', '.', 'and', 'anyone', 'appreciate']...)\n",
      "Dictionary(352 unique tokens: [\"'\", \"'orchestrate\", ',', '.', 'brain']...)\n",
      "Dictionary(545 unique tokens: [\"'s\", ',', '.', ':', 'a']...)\n",
      "Dictionary(147 unique tokens: [',', '.', 'a', 'anuradha', 'become']...)\n",
      "Dictionary(507 unique tokens: ['&', ',', '.', '?', 'a']...)\n",
      "Dictionary(48 unique tokens: [',', '.', ':', 'an', 'intellectual']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(234 unique tokens: [\"''\", \"'ve\", ',', '.', '...']...)\n",
      "Dictionary(176 unique tokens: [',', '.', 'a', 'acosta', 'advertising']...)\n",
      "Dictionary(553 unique tokens: [\"'\", \"'underwater\", '30', 'already', 'earth']...)\n",
      "Dictionary(165 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(180 unique tokens: ['.', 'after', 'cbs', 'charlie', 'co-host']...)\n",
      "Dictionary(177 unique tokens: ['#', '?', '@', 'als', 'andylenz']...)\n",
      "Dictionary(174 unique tokens: [',', 'business', 'insider', 'jacobs', 'sarah']...)\n",
      "Dictionary(176 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(381 unique tokens: ['more', '#', \"'re\", \"'s\", '.']...)\n",
      "Dictionary(59 unique tokens: [',', '.', 'a', 'allegedly', 'alphabet']...)\n",
      "Dictionary(140 unique tokens: [\"''\", ',', '.', '2-year-old', '``']...)\n",
      "Dictionary(109 unique tokens: [',', '.', 'administration', 'agenda', 'american']...)\n",
      "Dictionary(206 unique tokens: [',', '.', '2005', 'adviser', 'and']...)\n",
      "Dictionary(271 unique tokens: [',', '.', '19', 'a', 'ablaze']...)\n",
      "Dictionary(356 unique tokens: [',', '.', 'and', 'bills', 'calls']...)\n",
      "Dictionary(108 unique tokens: [\"'re\", '.', 'a', 'about', 'basis']...)\n",
      "Dictionary(213 unique tokens: [',', 'nikolaj', 'reuters', 'skydsgaard', '.']...)\n",
      "Dictionary(116 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(661 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(322 unique tokens: ['ban', 'order', 'possibly', 'revised', 'sign']...)\n",
      "Dictionary(488 unique tokens: [',', '.', 'a', 'be', 'behaviour']...)\n",
      "Dictionary(332 unique tokens: ['(', ')', '.', 'arabia', 'certainly']...)\n",
      "Dictionary(233 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(290 unique tokens: [\"''\", \"'s\", '.', '1994', '``']...)\n",
      "Dictionary(414 unique tokens: [':', 'a', 'also', 'and', 'as']...)\n",
      "Dictionary(82 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(190 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(555 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(241 unique tokens: [',', '.', '19-year-old', 'a', 'after']...)\n",
      "Dictionary(240 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(187 unique tokens: ['and', 'bill', 'care', 'compensation', 'for']...)\n",
      "Dictionary(706 unique tokens: ['more', ',', '.', '100', '31']...)\n",
      "Dictionary(242 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(214 unique tokens: [',', '.', 'adding', 'adopt', 'and']...)\n",
      "Dictionary(223 unique tokens: [',', '.', '1972', 'at', 'backflipped']...)\n",
      "Dictionary(426 unique tokens: [',', '.', 'a', 'address', 'addressing']...)\n",
      "Dictionary(276 unique tokens: ['embed', 'facebook', 'google', 'plus', 'twitter']...)\n",
      "Dictionary(467 unique tokens: [\"'s\", ',', '.', '1999', 'a']...)\n",
      "Dictionary(296 unique tokens: ['.', 'administration', 'bush', 'donald', 'former']...)\n",
      "Dictionary(139 unique tokens: [',', '.', 'a', 'and', 'announced']...)\n",
      "Dictionary(640 unique tokens: ['more', '.', '2017', 'a', 'at']...)\n",
      "Dictionary(109 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(433 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(245 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(339 unique tokens: [',', '.', 'despite', 'differences', 'extroverts']...)\n",
      "Dictionary(393 unique tokens: ['!', '(', ')', ',', '.']...)\n",
      "Dictionary(244 unique tokens: ['(', ')', ',', '.', 'ablaze']...)\n",
      "Dictionary(239 unique tokens: [\"'s\", '.', 'a', 'after', 'and']...)\n",
      "Dictionary(138 unique tokens: [',', '.', '6-year-old', 'a', 'alert']...)\n",
      "Dictionary(321 unique tokens: [',', '.', 'a', 'about', 'adults']...)\n",
      "Dictionary(297 unique tokens: [',', '.', '60', 'a', 'been']...)\n",
      "Dictionary(75 unique tokens: [',', '.', 'a', 'address', 'ag']...)\n",
      "Dictionary(296 unique tokens: [\"'s\", '(', ')', ',', '.â']...)\n",
      "Dictionary(53 unique tokens: [',', '.', '2015-16', 'after', 'always']...)\n",
      "Dictionary(128 unique tokens: ['.', 'action', 'annual', 'conference', 'conservative']...)\n",
      "Dictionary(60 unique tokens: [',', '.', 'allowed', 'beijing', 'beijing—before']...)\n",
      "Dictionary(324 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(341 unique tokens: [',', '.', '2017', 'association', 'at']...)\n",
      "Dictionary(408 unique tokens: [',', '.', 'a', 'after', 'all']...)\n",
      "Dictionary(230 unique tokens: [',', '.', 'dallas', 'deron', 'expressed']...)\n",
      "Dictionary(205 unique tokens: [',', '.', '45', ':', 'a']...)\n",
      "Dictionary(496 unique tokens: [',', '.', '13', 'a', 'after']...)\n",
      "Dictionary(76 unique tokens: ['and', 'emmanuel', 'jessica', 'ocbazghi', 'orwig']...)\n",
      "Dictionary(315 unique tokens: [',', '.', 'a', 'all', 'and']...)\n",
      "Dictionary(377 unique tokens: [\"'s\", ',', '.', 'a', 'aggressively']...)\n",
      "Dictionary(370 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(672 unique tokens: [\"'s\", ',', '.', 'a', 'airport']...)\n",
      "Dictionary(256 unique tokens: ['!', '(', ')', ',', '.']...)\n",
      "Dictionary(219 unique tokens: ['.', 'emily', 'for', 'has', 'no']...)\n",
      "Dictionary(76 unique tokens: ['&', '//t.co/ttabcwpf8f', ':', ';', 'amp']...)\n",
      "Dictionary(421 unique tokens: [',', '.', 'a', 'across', 'and']...)\n",
      "Dictionary(170 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(67 unique tokens: [',', '.', '1999', 'a', 'an']...)\n",
      "Dictionary(175 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(216 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(128 unique tokens: [\"'s\", \"'ve\", '(', ')', ',']...)\n",
      "Dictionary(624 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(263 unique tokens: ['.', '14', '2015', 'attack', 'bernardino']...)\n",
      "Dictionary(512 unique tokens: ['!', '(', ')', '.', 'accomplished']...)\n",
      "Dictionary(189 unique tokens: ['.', 'agent', 'an', 'attempt', 'career']...)\n",
      "Dictionary(587 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(40 unique tokens: ['14', '2017', 'feb', '-', '.']...)\n",
      "Dictionary(225 unique tokens: [\"'m\", \"'s\", \"'ve\", ',', '-']...)\n",
      "Dictionary(56 unique tokens: [',', '.', 'a', 'an', 'armed']...)\n",
      "Dictionary(305 unique tokens: ['$', ',', '.', '300', 'a']...)\n",
      "Dictionary(152 unique tokens: [\"'s\", ',', '.', 'a', 'album']...)\n",
      "Dictionary(449 unique tokens: [',', '.', '2', 'a', 'arrived']...)\n",
      "Dictionary(63 unique tokens: [\"'s\", '(', ')', ',', '.â']...)\n",
      "Dictionary(92 unique tokens: ['.', 'a', 'captured', 'car', 'caused']...)\n",
      "Dictionary(219 unique tokens: ['.', 'actions', 'agents', 'arrested', 'at']...)\n",
      "Dictionary(217 unique tokens: [',', '.', 'a', 'and', 'arguing']...)\n",
      "Dictionary(492 unique tokens: [\"'s\", \"'ve\", ',', '.', '10']...)\n",
      "Dictionary(501 unique tokens: [',', '.', '31-point', 'a', 'about']...)\n",
      "Dictionary(24 unique tokens: ['$', ',', '3.95', 'card', 'heavy']...)\n",
      "Dictionary(228 unique tokens: ['after', 'association', 'cambridge', 'centre', 'city']...)\n",
      "Dictionary(215 unique tokens: [',', '.', 'canada', 'cause', 'does']...)\n",
      "Dictionary(175 unique tokens: [\"''\", ',', '.', '19-year-old', '5,400']...)\n",
      "Dictionary(132 unique tokens: [\"'s\", ',', '.', 'a', 'agency']...)\n",
      "Dictionary(257 unique tokens: ['academic', 'and', 'chorus', 'david', 'developing']...)\n",
      "Dictionary(352 unique tokens: ['more', ',', '.', '9-year-old', 'a']...)\n",
      "Dictionary(156 unique tokens: ['!', '#', \"'ve\", '.', '//t.co/ezazglnldf']...)\n",
      "Dictionary(102 unique tokens: [',', '.', 'a', 'are', 'arrested']...)\n",
      "Dictionary(148 unique tokens: ['.', 'a', 'be', 'experts', 'future']...)\n",
      "Dictionary(412 unique tokens: ['.', '10,000', '111', 'ads', 'and']...)\n",
      "Dictionary(467 unique tokens: [',', '.', 'a', 'academics', 'analysis']...)\n",
      "Dictionary(184 unique tokens: ['--', '.', 'a', 'bank', 'contentious']...)\n",
      "Dictionary(242 unique tokens: [\"'s\", '.', 'aides', 'an', 'and']...)\n",
      "Dictionary(199 unique tokens: [',', '.', 'a', 'against', 'apart']...)\n",
      "Dictionary(370 unique tokens: [':', 'attorney', 'debate', 'general', 'jeff']...)\n",
      "Dictionary(210 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(321 unique tokens: [',', '.', '1', '100', '131,000']...)\n",
      "Dictionary(367 unique tokens: [',', '.', '...', '21', ':']...)\n",
      "Dictionary(291 unique tokens: [',', '1960', 'a', 'an', 'and']...)\n",
      "Dictionary(585 unique tokens: ['more', 'firefox', 'on', 'sports', 'try']...)\n",
      "Dictionary(175 unique tokens: [',', '.', 'a', 'and', 'announced']...)\n",
      "Dictionary(549 unique tokens: [',', '.', '2009', 'a', 'a.']...)\n",
      "Dictionary(90 unique tokens: [',', '.', 'behind', 'deserve', 'ears']...)\n",
      "Dictionary(169 unique tokens: ['!', '#', \"'\", \"'s\", \"'un-american\"]...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(175 unique tokens: [',', 'business', 'guerrasio', 'insider', 'jason']...)\n",
      "Dictionary(155 unique tokens: [',', '.', '9', 'a.m.', 'about']...)\n",
      "Dictionary(173 unique tokens: [\"'\", \"''\", \"'born\", \"'purposeful\", \"'re\"]...)\n",
      "Dictionary(210 unique tokens: [',', '.', 'a', 'against', 'anthony']...)\n",
      "Dictionary(114 unique tokens: ['.', 'on', 'or', 'says', 'she']...)\n",
      "Dictionary(264 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(126 unique tokens: [',', '2017', '31', '6:59', 'est']...)\n",
      "Dictionary(125 unique tokens: ['.', 'administration', 'back', 'has', 'house']...)\n",
      "Dictionary(194 unique tokens: [\"'s\", '.', 'a', 'advising', 'amending']...)\n",
      "Dictionary(440 unique tokens: [\"'s\", ',', '.', 'a', 'accompanied']...)\n",
      "Dictionary(169 unique tokens: [',', '.', 'alphabet', 'and', 'are']...)\n",
      "Dictionary(157 unique tokens: [\"'s\", ',', '.', 'a', 'all']...)\n",
      "Dictionary(57 unique tokens: [',', '.', 'and', 'are', 'at']...)\n",
      "Dictionary(437 unique tokens: [':', 'attorney', 'debate', 'general', 'jeff']...)\n",
      "Dictionary(649 unique tokens: [',', '.', '100', ':', ';']...)\n",
      "Dictionary(134 unique tokens: [',', '.', 'a', 'and', 'blasted']...)\n",
      "Dictionary(1001 unique tokens: ['firefox', 'news', 'on', 'try', 'yahoo']...)\n",
      "Dictionary(91 unique tokens: ['andy', 'kiersz', ',', '15:17', '27.01.2017']...)\n",
      "Dictionary(314 unique tokens: [\"'s\", ',', '.', 'at', 'crowd']...)\n",
      "Dictionary(271 unique tokens: ['shutterstock', \"'s\", ',', '.', 'are']...)\n",
      "Dictionary(294 unique tokens: [',', 'abuse', 'admitted', 'beaten', 'but']...)\n",
      "Dictionary(176 unique tokens: ['$', '&', \"'\", \"''\", \"'re\"]...)\n",
      "Dictionary(356 unique tokens: [',', 'a', 'academics', 'as', 'attract']...)\n",
      "Dictionary(604 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(135 unique tokens: [',', '.', 'and', 'beef', 'concerning']...)\n",
      "Dictionary(500 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(276 unique tokens: [',', '.', '19', '2', '200']...)\n",
      "Dictionary(283 unique tokens: ['after', 'agency', 'brothers', 'by', 'conning']...)\n",
      "Dictionary(290 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(474 unique tokens: [',', '.', '45th', 'a', 'across']...)\n",
      "Dictionary(530 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(552 unique tokens: [\"'s\", '(', ')', ',', '-']...)\n",
      "Dictionary(79 unique tokens: ['?', 'did', 'donald', 'just', 'office']...)\n",
      "Dictionary(571 unique tokens: [';', 'about', 'against', 'assault', 'describe']...)\n",
      "Dictionary(145 unique tokens: [\"'s\", ',', '.', '20', 'a']...)\n",
      "Dictionary(334 unique tokens: ['more', ',', '.', 'attend', 'caitlyn']...)\n",
      "Dictionary(96 unique tokens: [',', '.', '1', 'a', 'after']...)\n",
      "Dictionary(431 unique tokens: ['more', ',', '.', 'a', 'abc']...)\n",
      "Dictionary(370 unique tokens: [',', '.', 'analyst', 'army', 'barack']...)\n",
      "Dictionary(418 unique tokens: [',', '.', 'a', 'and', 'annals']...)\n",
      "Dictionary(474 unique tokens: ['more', ',', '.', 'announced', 'artist']...)\n",
      "Dictionary(410 unique tokens: [',', '.', 'badge', 'didn', 'doswell']...)\n",
      "Dictionary(333 unique tokens: [\"'s\", ':', ';', 'a', 'after']...)\n",
      "Dictionary(464 unique tokens: [',', '.', '18', 'a', 'act']...)\n",
      "Dictionary(420 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(108 unique tokens: [\"'s\", '.', 'a', 'acts', 'annual']...)\n",
      "Dictionary(242 unique tokens: [\"''\", '(', ')', '.', '``']...)\n",
      "Dictionary(286 unique tokens: ['a', 'begin', 'by', 'deal', 'denied']...)\n",
      "Dictionary(64 unique tokens: [',', '.', 'and', 'aquatic', 'asking']...)\n",
      "Dictionary(226 unique tokens: ['!', '#', \"'\", \"''\", \"'corrupt\"]...)\n",
      "Dictionary(128 unique tokens: [',', '.', 'a', 'agarwal', 'aims']...)\n",
      "Dictionary(183 unique tokens: [',', '.', 'a', 'according', 'asanas']...)\n",
      "Dictionary(221 unique tokens: ['.', 'a', 'and', 'ending', 'he']...)\n",
      "Dictionary(249 unique tokens: ['.', '15', 'a', 'admitted', 'career']...)\n",
      "Dictionary(176 unique tokens: [',', '.', 'are', 'be', 'but']...)\n",
      "Dictionary(195 unique tokens: [',', '.', 'and', 'announced', 'are']...)\n",
      "Dictionary(398 unique tokens: ['are', 'at', 'auschwitz', 'examining', 'experiments']...)\n",
      "Dictionary(257 unique tokens: [\"'s\", ',', '.', '27-run', 'a']...)\n",
      "Dictionary(290 unique tokens: [\"'s\", '.', '4', 'capital', 'celsius']...)\n",
      "Dictionary(59 unique tokens: ['.', 'a', 'after', 'attack', 'authorities']...)\n",
      "Dictionary(426 unique tokens: ['.', '20', '2016', ':', 'a']...)\n",
      "Dictionary(317 unique tokens: [',', '.', '17', 'clayton', 'community']...)\n",
      "Dictionary(298 unique tokens: [',', '.', 'a', 'actress', 'an']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.076320656"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = []\n",
    "for x in range (2000):\n",
    "    sim_val = gensim_sim(clickbait_train[x]['targetTitle'],clickbait_train[x]['targetParagraphs'])\n",
    "    if sim_val > 0:\n",
    "        aaa.append(sim_val)\n",
    "np.mean(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(328 unique tokens: [\"'s\", '.', '64gb', 'a', 'and']...)\n",
      "Dictionary(51 unique tokens: ['.', 'are', 'emerging', 'favor', 'markets']...)\n",
      "Dictionary(765 unique tokens: ['$', ',', '.', '1971', '3.50']...)\n",
      "Dictionary(460 unique tokens: [',', '.', 'a', 'after', 'agency']...)\n",
      "Dictionary(89 unique tokens: [',', '.', '12', 'a', 'after']...)\n",
      "Dictionary(49 unique tokens: [',', '.', '10', 'a', 'abroad']...)\n",
      "Dictionary(358 unique tokens: [\"'s\", ',', '.', 'a', 'agenda']...)\n",
      "Dictionary(204 unique tokens: [',', '.', 'a', 'and', 'attending']...)\n",
      "Dictionary(186 unique tokens: [',', '90', 'a', 'actually', 'believe']...)\n",
      "Dictionary(448 unique tokens: [',', 'dear', 'jerry', 'seinfeld', \"''\"]...)\n",
      "Dictionary(357 unique tokens: [\"'s\", 'actions', 'activity', 'anti-terror', 'chiefs']...)\n",
      "Dictionary(372 unique tokens: ['$', ',', '.', '100', 'a']...)\n",
      "Dictionary(384 unique tokens: ['.', 'against', 'alberto', 'allegations', 'american']...)\n",
      "Dictionary(168 unique tokens: [',', '.', 'a', 'and', 'armpits']...)\n",
      "Dictionary(502 unique tokens: [',', '.', 'a', 'afternoon', 'all-out']...)\n",
      "Dictionary(330 unique tokens: [',', '.', 'about', 'and', 'angeles']...)\n",
      "Dictionary(438 unique tokens: [',', 'accord', 'after', 'agreement', 'bailout']...)\n",
      "Dictionary(150 unique tokens: ['.', 'and', 'audrey', 'both', 'figo']...)\n",
      "Dictionary(520 unique tokens: [',', '.', 'a', 'accord', 'after']...)\n",
      "Dictionary(82 unique tokens: ['.', 'all', 'asstronauts', 'calling', ',']...)\n",
      "Dictionary(17 unique tokens: ['.', 'and', 'are', 'bffs', 'cutest']...)\n",
      "Dictionary(228 unique tokens: ['(', ')', '.', ':', 'astronaut']...)\n",
      "Dictionary(268 unique tokens: [',', '.', '14-year-old', 'a', 'administrative']...)\n",
      "Dictionary(303 unique tokens: [\"''\", '--', '.', '``', 'a']...)\n",
      "Dictionary(159 unique tokens: ['2016', 'e', 'euro', 'from', 'group']...)\n",
      "Dictionary(275 unique tokens: [',', '.', '11', '38', 'a']...)\n",
      "Dictionary(297 unique tokens: [\"'s\", ',', '.', 'a', 'abroad']...)\n",
      "Dictionary(265 unique tokens: [',', '.', 'a', 'academic', 'after']...)\n",
      "Dictionary(136 unique tokens: [',', '.', 'a', 'according', 'affidavit']...)\n",
      "Dictionary(299 unique tokens: [\"'\", '(', ')', ',', '-']...)\n",
      "Dictionary(341 unique tokens: [',', '.', '149', '320', 'airbus']...)\n",
      "Dictionary(169 unique tokens: [\"'roo\", '.', 'a', 'crushing', 'fame']...)\n",
      "Dictionary(257 unique tokens: [',', '.', 'a', 'airstrip', 'and']...)\n",
      "Dictionary(198 unique tokens: ['.', 'a', 'and', 'as', 'been']...)\n",
      "Dictionary(351 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(42 unique tokens: ['.', 'expansion', 'its', 'swift', 'unprecedented']...)\n",
      "Dictionary(261 unique tokens: ['.', 'a', 'after', 'announced', 'becoming']...)\n",
      "Dictionary(170 unique tokens: ['$', '.', '663', 'a', 'about']...)\n",
      "Dictionary(238 unique tokens: ['.', '38-year-old', 'a', 'astronaut', 'broken']...)\n",
      "Dictionary(317 unique tokens: ['(', ')', '.', 'a', 'and']...)\n",
      "Dictionary(60 unique tokens: [',', '.', 'and', 'are', 'bottle']...)\n",
      "Dictionary(158 unique tokens: [\"'s\", '.', '600', 'almost', 'atrocity']...)\n",
      "Dictionary(203 unique tokens: [',', '.', 'a', 'and', 'between']...)\n",
      "Dictionary(109 unique tokens: [\"'s\", '.', 'a', 'and', 'between']...)\n",
      "Dictionary(367 unique tokens: [',', '.', 'and', 'at', 'banned']...)\n",
      "Dictionary(780 unique tokens: [',', '.', 'a', 'about', 'acting']...)\n",
      "Dictionary(1005 unique tokens: [',', '.', 'afternoon', 'and', 'bursts']...)\n",
      "Dictionary(84 unique tokens: ['.', 'damn', 'hot', \"''\", ',']...)\n",
      "Dictionary(147 unique tokens: ['.', 'bill', 'clinton', 'elected', 'former']...)\n",
      "Dictionary(236 unique tokens: [',', 'against', 'backed', 'biggs', 'by']...)\n",
      "Dictionary(215 unique tokens: [\"'s\", '.', 'a', 'after', 'apparently']...)\n",
      "Dictionary(284 unique tokens: [\"'s\", ',', '--', '.', '2016']...)\n",
      "Dictionary(343 unique tokens: ['-', '.', '195', 'a', 'age']...)\n",
      "Dictionary(268 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(803 unique tokens: [\"'d\", \"'s\", ',', '.', '10-']...)\n",
      "Dictionary(142 unique tokens: [',', '.', 'another', 'at', 'descended']...)\n",
      "Dictionary(560 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(360 unique tokens: ['accuses', 'as', 'cameron', 'david', 'england']...)\n",
      "Dictionary(211 unique tokens: [\"''\", ',', '-', '.', '``']...)\n",
      "Dictionary(259 unique tokens: [\"'s\", ',', '.', '1987', '2012']...)\n",
      "Dictionary(128 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(809 unique tokens: ['despite', 'encircling', 'enjoyment', 'for', 'forces']...)\n",
      "Dictionary(256 unique tokens: ['.', 'a', 'aiding', 'and', 'at']...)\n",
      "Dictionary(294 unique tokens: [',', '.', '51-year-old', '?', 'a']...)\n",
      "Dictionary(79 unique tokens: [',', '.', 'accumulating', 'and', 'at']...)\n",
      "Dictionary(486 unique tokens: [\"'s\", ',', '.', '2', '2014']...)\n",
      "Dictionary(291 unique tokens: ['cat-and-mouse', 'could', 'help', 'in', 'rodents']...)\n",
      "Dictionary(336 unique tokens: [':', 'helen', 'photo', 'sloan/hbo', ',']...)\n",
      "Dictionary(956 unique tokens: [',', '.', 'a', 'according', 'against']...)\n",
      "Dictionary(114 unique tokens: [\"'s\", '.', 'a', 'air', 'an']...)\n",
      "Dictionary(246 unique tokens: ['$', \"'s\", ',', '.', '1']...)\n",
      "Dictionary(121 unique tokens: [',', '.', 'actor', 'and', 'arrested']...)\n",
      "Dictionary(305 unique tokens: ['.', 'actor', 'appeared', 'british', 'films']...)\n",
      "Dictionary(198 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(576 unique tokens: ['(', ')', '.', 'a', 'aided']...)\n",
      "Dictionary(574 unique tokens: ['.', 'about', 'actors', 'and', 'at']...)\n",
      "Dictionary(516 unique tokens: [\"'s\", ',', '--', '.', '/b']...)\n",
      "Dictionary(35 unique tokens: [',', '.', 'a', 'approval', 'bloomberg']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'a', 'address', 'awaits']...)\n",
      "Dictionary(253 unique tokens: ['.', 'a', 'and', 'appeals', 'battle']...)\n",
      "Dictionary(277 unique tokens: [\"'s\", '.', 'association', 'at', 'carrie']...)\n",
      "Dictionary(457 unique tokens: [\"'s\", ',', '.', ':', 'a']...)\n",
      "Dictionary(14 unique tokens: ['.', 'a', 'human', 'it', 'mess']...)\n",
      "Dictionary(165 unique tokens: [\"'s\", ',', '.', '10', '8']...)\n",
      "Dictionary(74 unique tokens: ['.', 'and', 'choose', 'did', 'didn']...)\n",
      "Dictionary(92 unique tokens: [\"'s\", ',', '.', 'a', 'approval']...)\n",
      "Dictionary(700 unique tokens: ['.', 'an', 'areas', 'care', 'consequences']...)\n",
      "Dictionary(193 unique tokens: [\"'\", \"'pleased\", 'and', 'be', 'happy']...)\n",
      "Dictionary(260 unique tokens: [',', 'absent', 'according', 'by', 'data']...)\n",
      "Dictionary(84 unique tokens: [',', '.', '20-year-old', 'a', 'athlete']...)\n",
      "Dictionary(216 unique tokens: [',', '.', 'a', 'adidas', 'and']...)\n",
      "Dictionary(388 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(168 unique tokens: [\"'s\", '15', 'ago', 'died', 'kevin']...)\n",
      "Dictionary(320 unique tokens: [\"''\", '(', ')', ',', '--']...)\n",
      "Dictionary(557 unique tokens: [',', '.', 'and', 'at', 'crew']...)\n",
      "Dictionary(180 unique tokens: ['photo', 'view', ',', '.', 'a']...)\n",
      "Dictionary(209 unique tokens: ['.', 'abductees', 'and', 'are', 'detailed']...)\n",
      "Dictionary(506 unique tokens: ['--', '.', 'champions', 'do', 'how']...)\n",
      "Dictionary(764 unique tokens: [',', '.', '26-year-old', '?', 'a']...)\n",
      "Dictionary(201 unique tokens: ['.', 'a', 'after', 'agreeing', 'an']...)\n",
      "Dictionary(198 unique tokens: [',', '.', '14-year-old', 'a', 'administrative']...)\n",
      "Dictionary(61 unique tokens: ['.', '2017', '3rd', 'app', 'april']...)\n",
      "Dictionary(410 unique tokens: [',', '.', 'a', 'are', 'as']...)\n",
      "Dictionary(72 unique tokens: [',', '.', 'a', 'and', 'costs']...)\n",
      "Dictionary(354 unique tokens: [',', '--', '.', '4', 'after']...)\n",
      "Dictionary(262 unique tokens: [\"'\", \"'be\", \"'s\", '.', 'americans']...)\n",
      "Dictionary(448 unique tokens: [',', 'dear', 'jerry', 'seinfeld', \"''\"]...)\n",
      "Dictionary(202 unique tokens: ['.', 'aggression', 'allies', 'and', 'began']...)\n",
      "Dictionary(380 unique tokens: [',', '.', 'a', 'america', 'and']...)\n",
      "Dictionary(301 unique tokens: [',', '.', 'a', 'agency', 'believes']...)\n",
      "Dictionary(343 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(391 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(280 unique tokens: [',', '2016', 'addresses', 'again', 'and']...)\n",
      "Dictionary(112 unique tokens: ['.', '2015', '5', ':', 'and']...)\n",
      "Dictionary(316 unique tokens: ['.', '200', 'and', 'are', 'back']...)\n",
      "Dictionary(30 unique tokens: [',', '.', 'a', 'afl', 'and']...)\n",
      "Dictionary(134 unique tokens: [',', '.', '12', '30', '41']...)\n",
      "Dictionary(622 unique tokens: [',', ':', '?', 'after', 'american']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(300 unique tokens: [\"'re\", '.', ':', 'are', 'following']...)\n",
      "Dictionary(382 unique tokens: ['(', ')', ',', '/', '8-9']...)\n",
      "Dictionary(495 unique tokens: ['.', 'being', 'empire', 'hand', 'his']...)\n",
      "Dictionary(60 unique tokens: ['.', 'all', 'at', 'found', 'local']...)\n",
      "Dictionary(99 unique tokens: ['(', ')', '.', '2015', '?']...)\n",
      "Dictionary(734 unique tokens: ['.', 'a', 'activity', 'allegedly', 'as']...)\n",
      "Dictionary(274 unique tokens: [\"''\", \"'s\", ',', '.', '1']...)\n",
      "Dictionary(661 unique tokens: ['.', 'a', 'as', 'bars', 'blair']...)\n",
      "Dictionary(210 unique tokens: [\"'s\", '.', 'and', 'artist', 'discovered']...)\n",
      "Dictionary(165 unique tokens: [\"'s\", ',', '.', 'for', 'friday']...)\n",
      "Dictionary(743 unique tokens: [',', '.', 'a', 'and', 'bare']...)\n",
      "Dictionary(83 unique tokens: [',', '--', '.', 'a', 'again']...)\n",
      "Dictionary(278 unique tokens: [\"'s\", '?', 'be', 'ceo', 'influential']...)\n",
      "Dictionary(324 unique tokens: ['(', ')', '.', ':', 'a']...)\n",
      "Dictionary(428 unique tokens: ['.', 'ebola', 'finished', 'is', \"n't\"]...)\n",
      "Dictionary(244 unique tokens: [\"''\", \"'s\", ',', '.', '/']...)\n",
      "Dictionary(166 unique tokens: [':', 'credit', 'dm/star', 'max/filmmagic', 'photo']...)\n",
      "Dictionary(86 unique tokens: ['(', ')', '@', 'a', 'bleacher']...)\n",
      "Dictionary(116 unique tokens: [\"'\", ',', '.', '18.5', '26.8']...)\n",
      "Dictionary(629 unique tokens: [',', '.', '82', 'a', 'ago']...)\n",
      "Dictionary(38 unique tokens: [',', '.', 'a', 'all', 'at']...)\n",
      "Dictionary(230 unique tokens: [\"'s\", ',', '.', 'a', 'as']...)\n",
      "Dictionary(187 unique tokens: ['share', 'this', 'with', \"''\", \"'s\"]...)\n",
      "Dictionary(580 unique tokens: [',', '.', '1,200-mile', 'afternoon', 'and']...)\n",
      "Dictionary(214 unique tokens: ['-', '--', '.', 'a', 'about']...)\n",
      "Dictionary(449 unique tokens: ['.', 'a', 'an', 'and', 'armed']...)\n",
      "Dictionary(207 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(375 unique tokens: [',', '.', 'add', 'and', 'been']...)\n",
      "Dictionary(91 unique tokens: [\"'s\", ',', '.', 'a', 'adidas']...)\n",
      "Dictionary(302 unique tokens: [',', '.', '69', 'a', 'as']...)\n",
      "Dictionary(1099 unique tokens: [',', '.', '103', '1908', '3-1']...)\n",
      "Dictionary(834 unique tokens: [',', '.', '6', 'allegedly', 'and']...)\n",
      "Dictionary(530 unique tokens: ['are', 'his', 'hunt', 'isn', 'learn']...)\n",
      "Dictionary(190 unique tokens: ['.', 'a', 'ambassador', 'been', 'by']...)\n",
      "Dictionary(129 unique tokens: [',', '.', 'a', 'again', 'cavaliers']...)\n",
      "Dictionary(301 unique tokens: [',', '.', 'a', 'an', 'assault']...)\n",
      "Dictionary(544 unique tokens: [',', '.', 'a', 'an', 'are']...)\n",
      "Dictionary(142 unique tokens: ['.', '1979', '21st', 'a', 'and']...)\n",
      "Dictionary(342 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(330 unique tokens: [',', '.', ':', 'all', 'an']...)\n",
      "Dictionary(158 unique tokens: ['.', 'a', 'and', 'beers', 'craft']...)\n",
      "Dictionary(621 unique tokens: [',', '.', '?', 'a', 'about']...)\n",
      "Dictionary(62 unique tokens: [',', '.', 'a', 'a.j.', 'and']...)\n",
      "Dictionary(218 unique tokens: [\"'s\", 'a', 'arm', 'ben', 'breaking']...)\n",
      "Dictionary(201 unique tokens: [\"'re\", '(', ')', ',', '.']...)\n",
      "Dictionary(232 unique tokens: [\"'s\", '(', ')', ',', '--']...)\n",
      "Dictionary(242 unique tokens: [\"''\", '.', '``', 'a', 'act']...)\n",
      "Dictionary(424 unique tokens: ['about', 'case', 'questions', 'race', 'raises']...)\n",
      "Dictionary(135 unique tokens: ['.', '1', '1975', 'a', 'appearance']...)\n",
      "Dictionary(576 unique tokens: ['(', ')', '.', 'a', 'aided']...)\n",
      "Dictionary(376 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(433 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(254 unique tokens: ['.', 'a', 'dellavedova', 'finals', 'having']...)\n",
      "Dictionary(110 unique tokens: [\"'s\", ',', '--', '.', 'a']...)\n",
      "Dictionary(255 unique tokens: [\"'\", \"'flying\", ',', '.', 'a']...)\n",
      "Dictionary(708 unique tokens: ['?', 'be', 'disrupt', 'disrupted', 'or']...)\n",
      "Dictionary(150 unique tokens: ['and', 'catch', 'could', 'cvs', 'have']...)\n",
      "Dictionary(333 unique tokens: ['(', ')', '.', '1.7', ':']...)\n",
      "Dictionary(126 unique tokens: ['.', '9', 'a', 'aimed', 'apple']...)\n",
      "Dictionary(308 unique tokens: ['.', 'anaconda', 'foundation', 'from', 'gray']...)\n",
      "Dictionary(94 unique tokens: ['2015', '21', 'aug', ',', '.']...)\n",
      "Dictionary(996 unique tokens: [\"'ve\", ',', '.', '25', 'about']...)\n",
      "Dictionary(152 unique tokens: [',', '.', 'a', 'about', 'and']...)\n",
      "Dictionary(621 unique tokens: [',', '.', 'darpa', 'don', 'if']...)\n",
      "Dictionary(221 unique tokens: ['.', 'a', 'albert', 'at', 'charity']...)\n",
      "Dictionary(179 unique tokens: [\"'s\", ',', '.', '26,482', ':']...)\n",
      "Dictionary(166 unique tokens: [',', '.', 'a', 'all', 'and']...)\n",
      "Dictionary(155 unique tokens: [',', '.', 'abandoned', 'along', 'an']...)\n",
      "Dictionary(65 unique tokens: [',', '.', ':', 'a', 'affected']...)\n",
      "Dictionary(69 unique tokens: ['.', 'a', 'bay', 'been', 'building']...)\n",
      "Dictionary(89 unique tokens: [',', '.', 'a', 'adorable', 'after']...)\n",
      "Dictionary(324 unique tokens: ['(', ')', '.', ':', 'and']...)\n",
      "Dictionary(144 unique tokens: [',', '?', 'a', 'angeles', 'better-looking']...)\n",
      "Dictionary(112 unique tokens: [\"'s\", ',', '.', 'ago', 'eighteen']...)\n",
      "Dictionary(306 unique tokens: ['.', 'a', 'agency', 'allegations', 'american']...)\n",
      "Dictionary(63 unique tokens: [',', '.', '2016', 'a', 'about']...)\n",
      "Dictionary(132 unique tokens: [\"'s\", ',', '.', ':', 'a']...)\n",
      "Dictionary(167 unique tokens: ['by', 'hope', 'nick', 'bbc', 'olympic']...)\n",
      "Dictionary(130 unique tokens: [\"'d\", \"'s\", ',', '.', 'a']...)\n",
      "Dictionary(90 unique tokens: [\"'d\", \"'s\", ',', '.', 'a']...)\n",
      "Dictionary(112 unique tokens: [\"'s\", ',', '--', '.', '2016']...)\n",
      "Dictionary(469 unique tokens: ['.', 'a', 'about', 'after', 'all']...)\n",
      "Dictionary(130 unique tokens: ['120,000', 'a', 'be', 'by', 'claim']...)\n",
      "Dictionary(247 unique tokens: [\"'\", \"'has\", \"'revolting\", ',', '2017']...)\n",
      "Dictionary(722 unique tokens: ['2015', 'draft', 'mlb', 'print', '.']...)\n",
      "Dictionary(455 unique tokens: [',', '.', 'a', 'about', 'ago']...)\n",
      "Dictionary(153 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(382 unique tokens: [\"'s\", ',', '.', ':', 'a']...)\n",
      "Dictionary(401 unique tokens: [\"'s\", '(', ')', '.', 'after']...)\n",
      "Dictionary(332 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(377 unique tokens: [\"''\", ',', '.', '``', 'american']...)\n",
      "Dictionary(168 unique tokens: [\"''\", '.', '19', '911', '``']...)\n",
      "Dictionary(446 unique tokens: [\"''\", \"'m\", \"'s\", ',', '.']...)\n",
      "Dictionary(357 unique tokens: [',', '.', '200', 'awkward', 'been']...)\n",
      "Dictionary(252 unique tokens: ['.', 'apple', 'at', 'conference', 'developers']...)\n",
      "Dictionary(320 unique tokens: ['?', 'are', 'best', 'ceos', 'the']...)\n",
      "Dictionary(55 unique tokens: [',', 'all', 'are', 'but', 'over']...)\n",
      "Dictionary(347 unique tokens: ['.', '50', 'after', 'as', 'bloody']...)\n",
      "Dictionary(190 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(67 unique tokens: ['content', 'main', 'skip', 'to', 'available']...)\n",
      "Dictionary(486 unique tokens: ['called', 'exposure', 'extended', 'for', 'friendship']...)\n",
      "Dictionary(14 unique tokens: ['.', 'and', 'galore', 'pant', 'skirt']...)\n",
      "Dictionary(79 unique tokens: [',', '.', 'and', 'bird', 'd']...)\n",
      "Dictionary(254 unique tokens: [\"'\", \"'zionists\", '.', 'a', 'after']...)\n",
      "Dictionary(357 unique tokens: [\"'\", \"'hostile\", 'a', 'against', 'conflict']...)\n",
      "Dictionary(409 unique tokens: ['(', ')', '.', '2016', 'a']...)\n",
      "Dictionary(125 unique tokens: [',', '.', '93', 'actor', 'after']...)\n",
      "Dictionary(50 unique tokens: ['.', 'a', 'at', 'buffett', 'but']...)\n",
      "Dictionary(420 unique tokens: [',', '.', 'a', 'about', 'and']...)\n",
      "Dictionary(343 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(573 unique tokens: [',', '.', 'affiliated', 'and', 'battles']...)\n",
      "Dictionary(150 unique tokens: ['alerted', 'any', 'appliances', 'be', 'can']...)\n",
      "Dictionary(63 unique tokens: ['.', 'kids', 'learn', 'need', 'somehow']...)\n",
      "Dictionary(238 unique tokens: [',', '.', '2.7', 'a', 'after']...)\n",
      "Dictionary(317 unique tokens: [',', '.', 'a', 'and', 'at']...)\n",
      "Dictionary(501 unique tokens: [',', '.', 'asked', 'brightest', 'came']...)\n",
      "Dictionary(137 unique tokens: [',', '.', '12', '3', '40']...)\n",
      "Dictionary(294 unique tokens: ['biker', 'charges', 'cleared', 'detective', 'in']...)\n",
      "Dictionary(250 unique tokens: ['and', 'anxious', 'back', 'british', 'daughter']...)\n",
      "Dictionary(245 unique tokens: [',', '.', 'alternative', 'be', 'could']...)\n",
      "Dictionary(266 unique tokens: [\"'s\", ',', '.', 'all', 'and']...)\n",
      "Dictionary(173 unique tokens: [',', '.', 'against', 'alongside', 'american']...)\n",
      "Dictionary(339 unique tokens: [',', '.', 'a', 'be', 'court']...)\n",
      "Dictionary(81 unique tokens: [',', '.', '6-inch', 'a', 'as']...)\n",
      "Dictionary(70 unique tokens: ['.', 'a', 'and', 'bb', 'christmas']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(512 unique tokens: [',', '.', 'a', 'administration', 'against']...)\n",
      "Dictionary(115 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(267 unique tokens: [',', '.', 'actor', 'as', 'barry']...)\n",
      "Dictionary(358 unique tokens: [\"'s\", ',', '.', 'a', 'agenda']...)\n",
      "Dictionary(137 unique tokens: ['.', 'against', 'basketball', 'guy', 'had']...)\n",
      "Dictionary(251 unique tokens: [\"'s\", '.', 'an', 'armored', 'dallas']...)\n",
      "Dictionary(575 unique tokens: [',', '.', '2000s', '73', 'a']...)\n",
      "Dictionary(208 unique tokens: [',', '.', '14,609', '21', '72']...)\n",
      "Dictionary(149 unique tokens: ['.', '69', 'died', 'dusty', 'he']...)\n",
      "Dictionary(188 unique tokens: ['.', 'a', 'been', 'britain', 'campaign']...)\n",
      "Dictionary(180 unique tokens: [\"'\", \"'nail\", '.', 'and', 'be']...)\n",
      "Dictionary(96 unique tokens: [\"'s\", ',', 'a', 'bites', 'boulder']...)\n",
      "Dictionary(478 unique tokens: [',', '.', '7-year-old', '90s', 'a']...)\n",
      "Dictionary(432 unique tokens: [\"'s\", ',', '--', '.', '34']...)\n",
      "Dictionary(124 unique tokens: ['.', 'about', 'asked', 'behind', 'black']...)\n",
      "Dictionary(327 unique tokens: ['!', ',', '.', 'any', 'anything']...)\n",
      "Dictionary(200 unique tokens: ['.', 'are', 'as', 'being', 'conduct']...)\n",
      "Dictionary(194 unique tokens: ['a', 'and', 'behalf', 'campaign', 'done']...)\n",
      "Dictionary(200 unique tokens: ['activist', 'at', 'centre', 'currently', 'ethnicity']...)\n",
      "Dictionary(568 unique tokens: [',', '.', ':', 'a', 'advances']...)\n",
      "Dictionary(189 unique tokens: ['30-year-old', 'a', 'able', 'an', 'be']...)\n",
      "Dictionary(161 unique tokens: [',', '.', 'a', 'but', 'crash']...)\n",
      "Dictionary(159 unique tokens: [\"'s\", ',', '.', 'a', 'ahead']...)\n",
      "Dictionary(405 unique tokens: [',', '.', 'almost', 'and', 'comfy']...)\n",
      "Dictionary(275 unique tokens: [',', '.', '150', '9525', 'a']...)\n",
      "Dictionary(86 unique tokens: ['(', ')', ',', '.', '13']...)\n",
      "Dictionary(152 unique tokens: [\"'s\", '-', '.', 'baby', 'deer']...)\n",
      "Dictionary(32 unique tokens: [\"''\", ',', '.', ':', '``']...)\n",
      "Dictionary(126 unique tokens: ['.', '23.5', '6.5', 'averaging', 'cavaliers']...)\n",
      "Dictionary(151 unique tokens: [',', '.', '17', '1st', 'a']...)\n",
      "Dictionary(493 unique tokens: ['.', 'and', 'car', 'certain', 'city']...)\n",
      "Dictionary(105 unique tokens: ['a', 'are', 'been', 'belgian', 'by']...)\n",
      "Dictionary(283 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(342 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(206 unique tokens: [',', '.', 'a', 'activist', 'along']...)\n",
      "Dictionary(318 unique tokens: [\"''\", ',', '--', '.', '73']...)\n",
      "Dictionary(272 unique tokens: ['-', '.', 'a', 'are', 'as']...)\n",
      "Dictionary(287 unique tokens: [',', '.', '2022', 'a', 'already']...)\n",
      "Dictionary(433 unique tokens: [',', '.', '?', 'about', 'admit']...)\n",
      "Dictionary(197 unique tokens: ['.', '17-year-old', 'a', 'an', 'as']...)\n",
      "Dictionary(103 unique tokens: ['.', 'a', 'against', 'an', 'and']...)\n",
      "Dictionary(12 unique tokens: ['.', 'british', 'covered', 'got', 'have']...)\n",
      "Dictionary(203 unique tokens: ['10,000', 'a', 'additional', 'almost', 'an']...)\n",
      "Dictionary(517 unique tokens: [',', '.', ':', 'a', 'agrees']...)\n",
      "Dictionary(123 unique tokens: ['.', 'amtrak', 'an', 'crash', 'engineer']...)\n",
      "Dictionary(41 unique tokens: ['%', ',', '.', '4.9', 'a']...)\n",
      "Dictionary(30 unique tokens: [\"'s\", '.', 'based', 'broadview', 'by']...)\n",
      "Dictionary(66 unique tokens: [',', '.', 'a', 'before', 'beijing-backed']...)\n",
      "Dictionary(576 unique tokens: ['(', ')', '.', 'a', 'aided']...)\n",
      "Dictionary(6 unique tokens: ['abc', 'coverage', 'from', 'live', 'news']...)\n",
      "Dictionary(81 unique tokens: ['.', 'a', 'before', 'being', 'bouquet']...)\n",
      "Dictionary(218 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(366 unique tokens: [',', '.', '16', 'a', 'accused']...)\n",
      "Dictionary(32 unique tokens: ['.', '8-bit-animated', 'adventure', 'alan', 'an']...)\n",
      "Dictionary(442 unique tokens: [',', 'a', 'and', 'as', 'charm']...)\n",
      "Dictionary(136 unique tokens: [\"'s\", ',', '.', 'balls', 'bay']...)\n",
      "Dictionary(322 unique tokens: ['a', 'after', 'chelsea', 'costa', 'diego']...)\n",
      "Dictionary(150 unique tokens: ['.', 'andy', 'browns', 'changing', 'cleveland']...)\n",
      "Dictionary(389 unique tokens: [',', '.', '140', '450', 'ago']...)\n",
      "Dictionary(67 unique tokens: [',', '.', 'again', 'all', 'and']...)\n",
      "Dictionary(329 unique tokens: [\"'s\", ',', '.', 'alike', 'all']...)\n",
      "Dictionary(265 unique tokens: [\"''\", '.', '``', 'a', 'alex']...)\n",
      "Dictionary(577 unique tokens: [',', '.', '6', 'a', 'about']...)\n",
      "Dictionary(8 unique tokens: ['.', 'are', 'available', 'currently', 'no']...)\n",
      "Dictionary(297 unique tokens: ['.', 'a', 'be', 'fifa', 'flop']...)\n",
      "Dictionary(202 unique tokens: ['(', ')', 'abc', 'adds', 'appeared']...)\n",
      "Dictionary(221 unique tokens: ['.', '100', 'a', 'after', 'been']...)\n",
      "Dictionary(593 unique tokens: [\"'\", ',', '.', 'a', 'angeles']...)\n",
      "Dictionary(126 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(685 unique tokens: [',', 'a', 'about', 'before', 'controversy']...)\n",
      "Dictionary(165 unique tokens: ['//t.co/hpgdkuepvp', '//t.co/p9xar6sqna', '124+', ':', 'black']...)\n",
      "Dictionary(67 unique tokens: [',', '.', 'a', 'agency', 'aimed']...)\n",
      "Dictionary(1407 unique tokens: [',', 'after', 'an', 'captured', 'confrontation']...)\n",
      "Dictionary(75 unique tokens: [',', '.', 'a', 'after', 'back']...)\n",
      "Dictionary(119 unique tokens: [',', '.', 'a', 'alleged', 'arrested']...)\n",
      "Dictionary(64 unique tokens: [',', '.', '51-year-old', 'been', 'break']...)\n",
      "Dictionary(580 unique tokens: ['.', 'have', 'receipts', 'the', 'we']...)\n",
      "Dictionary(400 unique tokens: [\"'s\", 'a', 'championship', 'cut', 'down']...)\n",
      "Dictionary(474 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(106 unique tokens: ['.', '2', 'due', 'for', 'in']...)\n",
      "Dictionary(377 unique tokens: ['.', '25-year-old', 'a', 'afford', 'ca']...)\n",
      "Dictionary(259 unique tokens: [',', '.', '2-1', '3', '3-2']...)\n",
      "Dictionary(515 unique tokens: [',', '.', 'about', 'at', 'care']...)\n",
      "Dictionary(220 unique tokens: [',', 'a', 'at', 'capital', 'end']...)\n",
      "Dictionary(285 unique tokens: [',', '.', 'about', 'but', 'crust']...)\n",
      "Dictionary(541 unique tokens: ['.', 'a', 'already', 'and', 'away']...)\n",
      "Dictionary(556 unique tokens: [',', '.', 'been', 'cairo', 'dead']...)\n",
      "Dictionary(100 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(316 unique tokens: [\"'s\", ',', '--', '.', 'alien-like']...)\n",
      "Dictionary(441 unique tokens: [',', '.', '15', 'aged', 'already']...)\n",
      "Dictionary(518 unique tokens: [',', '.', 'a', 'arena', 'at']...)\n",
      "Dictionary(150 unique tokens: [',', '.', 'a', 'and', 'bizarre']...)\n",
      "Dictionary(149 unique tokens: [\"''\", ',', '.', '``', 'advantage']...)\n",
      "Dictionary(265 unique tokens: [\"''\", '.', '``', 'a', 'activist']...)\n",
      "Dictionary(457 unique tokens: ['?', 'a', 'feel', 'hamster', 'how']...)\n",
      "Dictionary(15 unique tokens: ['.', 'actors', 'be', 'brand-new', 'girls']...)\n",
      "Dictionary(639 unique tokens: [',', '.', ':', 'a', 'about']...)\n",
      "Dictionary(512 unique tokens: [',', '.', '.…', '30', ';']...)\n",
      "Dictionary(226 unique tokens: [',', '.', '103-82', '4', 'a']...)\n",
      "Dictionary(458 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(25 unique tokens: [\"'s\", ',', '.', '2', 'also']...)\n",
      "Dictionary(29 unique tokens: [',', '.', 'and', 'at', 'begin']...)\n",
      "Dictionary(421 unique tokens: ['.', 'a', 'among', 'and', 'but']...)\n",
      "Dictionary(146 unique tokens: [\"'s\", '.', '2016', 'but', 'diagnosis']...)\n",
      "Dictionary(651 unique tokens: [',', '.', 'a', 'ability', 'and']...)\n",
      "Dictionary(115 unique tokens: [\"'s\", ',', '.', 'a', 'adidas']...)\n",
      "Dictionary(670 unique tokens: [',', '.', 'a', 'ago', 'amir']...)\n",
      "Dictionary(186 unique tokens: ['(', ')', ',', '.', '432']...)\n",
      "Dictionary(248 unique tokens: [',', '.', 'a', 'already', 'be']...)\n",
      "Dictionary(257 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(62 unique tokens: [\"'s\", '.', '4g', 'action', 'as']...)\n",
      "Dictionary(324 unique tokens: [',', 'a', 'an', 'and', 'break']...)\n",
      "Dictionary(386 unique tokens: ['a', 'align', 'be', 'biggest', 'can']...)\n",
      "Dictionary(382 unique tokens: ['.', 'a', 'after', 'are', 'battle']...)\n",
      "Dictionary(381 unique tokens: ['.', 'cute', 'much', 'so', ',']...)\n",
      "Dictionary(513 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(181 unique tokens: [\"'re\", ',', '--', '.', '3-d']...)\n",
      "Dictionary(256 unique tokens: [',', '.', 'an', 'andrew', 'are']...)\n",
      "Dictionary(557 unique tokens: [',', '.', '12', '2-1', '40']...)\n",
      "Dictionary(88 unique tokens: ['.', 'a', 'act', 'beautiful', 'bravery']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(685 unique tokens: [',', '.', 'a', 'actor', 'and']...)\n",
      "Dictionary(326 unique tokens: [',', '.', 'a', 'baby', 'carrying']...)\n",
      "Dictionary(166 unique tokens: [',', '.', 'a', 'almost', 'aspect']...)\n",
      "Dictionary(389 unique tokens: [\"'s\", ',', '.', 'a', 'always']...)\n",
      "Dictionary(331 unique tokens: [',', '.', 'after', 'an', 'announcing']...)\n",
      "Dictionary(250 unique tokens: ['after', 'airport', 'at', 'charles', 'child']...)\n",
      "Dictionary(246 unique tokens: ['.', 'at', 'comes', 'fast', 'life']...)\n",
      "Dictionary(415 unique tokens: [\"''\", '.', ':', '``', 'a']...)\n",
      "Dictionary(75 unique tokens: ['.', 'a', 'actor', 'and', 'baby']...)\n",
      "Dictionary(452 unique tokens: ['%', \"'s\", \"'ve\", ',', '.']...)\n",
      "Dictionary(99 unique tokens: ['(', ')', '@', 'a', 'by']...)\n",
      "Dictionary(178 unique tokens: ['.', 'and', 'barbie', 'be', 'follow']...)\n",
      "Dictionary(290 unique tokens: ['.', '75', 'and', 'appear', 'at']...)\n",
      "Dictionary(215 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(75 unique tokens: ['.', 'a', 'and', 'at', 'brought']...)\n",
      "Dictionary(61 unique tokens: ['.', 'a', 'about', 'and', 'battle']...)\n",
      "Dictionary(68 unique tokens: ['.', 'a', 'air', 'get', 'just']...)\n",
      "Dictionary(657 unique tokens: ['%', \"'s\", ',', '.', '97']...)\n",
      "Dictionary(295 unique tokens: ['1964', 'april', 'alcoholics', 'anonymous', '.']...)\n",
      "Dictionary(59 unique tokens: ['.', 'and', 'day', 'every', 'for']...)\n",
      "Dictionary(149 unique tokens: ['.', '9', 'battery', 'ios', 'iphone']...)\n",
      "Dictionary(500 unique tokens: [',', '.', 'alleged', 'at', 'bay']...)\n",
      "Dictionary(524 unique tokens: [',', '.', 'a', 'ago', 'announced']...)\n",
      "Dictionary(127 unique tokens: ['.', 'how', 'it', 'know', 'like']...)\n",
      "Dictionary(644 unique tokens: [',', 'address', 'bitterly', 'budge', 'but']...)\n",
      "Dictionary(37 unique tokens: [',', '.', 'a', 'cronut', 'dausage']...)\n",
      "Dictionary(1407 unique tokens: [',', 'after', 'an', 'captured', 'confrontation']...)\n",
      "Dictionary(516 unique tokens: ['.', 'a', 'bear', 'can', 'do']...)\n",
      "Dictionary(243 unique tokens: [\"'s\", '.', '4-year-old', 'a', 'account']...)\n",
      "Dictionary(335 unique tokens: ['.', 'back', 'is', 'jack', \"'s\"]...)\n",
      "Dictionary(459 unique tokens: ['agents', 'an', 'been', 'countries', 'forced']...)\n",
      "Dictionary(188 unique tokens: [',', '.', '1', 'a', 'all']...)\n",
      "Dictionary(335 unique tokens: [',', '.', 'a', 'and', 'announced']...)\n",
      "Dictionary(111 unique tokens: [\"'\", \"'bizarre\", 'copy', 'not', 'people']...)\n",
      "Dictionary(8 unique tokens: ['.', 'are', 'available', 'currently', 'no']...)\n",
      "Dictionary(247 unique tokens: [\"'\", \"'has\", \"'revolting\", ',', '2017']...)\n",
      "Dictionary(220 unique tokens: ['share', 'this', 'with', '.', 'a']...)\n",
      "Dictionary(150 unique tokens: ['.', 'bryce', 'dallas', 'dino', 'find']...)\n",
      "Dictionary(523 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(322 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(212 unique tokens: [\"'s\", ',', '.', 'a', 'angeles']...)\n",
      "Dictionary(764 unique tokens: [',', '.', '26-year-old', '?', 'a']...)\n",
      "Dictionary(100 unique tokens: ['$', \"'s\", ',', '.', '2.74']...)\n",
      "Dictionary(339 unique tokens: ['(', ')', ',', '.', 'are']...)\n",
      "Dictionary(250 unique tokens: [\"'s\", '.', 'bbc', 'been', 'by']...)\n",
      "Dictionary(157 unique tokens: ['$', \"'\", \"'s\", ',', '.']...)\n",
      "Dictionary(269 unique tokens: ['.', 'a', 'an', 'bizarre', 'country']...)\n",
      "Dictionary(455 unique tokens: [',', '.', 'a', 'company', 'family']...)\n",
      "Dictionary(402 unique tokens: [',', '--', '.', 'away', 'cup']...)\n",
      "Dictionary(32 unique tokens: ['.', '25', 'a', 'and', 'climate']...)\n",
      "Dictionary(344 unique tokens: ['.', ':', 'a', 'article', 'as']...)\n",
      "Dictionary(324 unique tokens: [',', '--', '.', 'a', 'administration']...)\n",
      "Dictionary(183 unique tokens: [':', 'academic', 'bellevue', 'boost', 'emotional']...)\n",
      "Dictionary(395 unique tokens: [',', '.', 'a', 'almost', 'andrew']...)\n",
      "Dictionary(397 unique tokens: [\"'\", \"''\", '.', '``', 'a']...)\n",
      "Dictionary(162 unique tokens: ['.', 'a', 'accused', 'and', 'bodies']...)\n",
      "Dictionary(363 unique tokens: [',', '.', ':', 'a', 'all']...)\n",
      "Dictionary(81 unique tokens: [',', '.', '20', 'afghan', 'after']...)\n",
      "Dictionary(529 unique tokens: [',', '.', 'acknowledged', 'administration', 'airplanes']...)\n",
      "Dictionary(352 unique tokens: [\"'s\", ',', '.', '22', 'and']...)\n",
      "Dictionary(59 unique tokens: [',', '.', 'about', 'advantage', 'as']...)\n",
      "Dictionary(235 unique tokens: [',', '67p', 'comet', 'craft', 'distant']...)\n",
      "Dictionary(279 unique tokens: [\"'\", \"'human\", ',', '.', 'appearance']...)\n",
      "Dictionary(565 unique tokens: [',', '.', 'and', 'are', 'constantly']...)\n",
      "Dictionary(242 unique tokens: [\"'s\", ',', '.', '102', '1938']...)\n",
      "Dictionary(479 unique tokens: ['.', 'a', 'bill', 'even', 'families']...)\n",
      "Dictionary(352 unique tokens: [\"'s\", ',', '.', '22', 'and']...)\n",
      "Dictionary(341 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(419 unique tokens: [',', '.', '12-year-old', 'a', 'after']...)\n",
      "Dictionary(268 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(1407 unique tokens: [',', 'after', 'an', 'captured', 'confrontation']...)\n",
      "Dictionary(200 unique tokens: ['$', ',', '.', '100,000', 'a']...)\n",
      "Dictionary(193 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(147 unique tokens: [\"'s\", ',', '.', '2012', 'a']...)\n",
      "Dictionary(187 unique tokens: ['share', 'this', 'with', \"''\", \"'s\"]...)\n",
      "Dictionary(63 unique tokens: ['.', 'a', 'an', 'at', 'bankers']...)\n",
      "Dictionary(259 unique tokens: [\"'s\", '.', 'astronaut', 'board', 'capsule']...)\n",
      "Dictionary(746 unique tokens: ['.', 'allow', 'any', 'enter', 'following']...)\n",
      "Dictionary(414 unique tokens: ['anne', 'as', 'books', 'concealing', 'crime']...)\n",
      "Dictionary(164 unique tokens: [\"''\", \"'s\", ',', '.', '10']...)\n",
      "Dictionary(107 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(234 unique tokens: ['.', '2015', 'a', 'big', 'change']...)\n",
      "Dictionary(283 unique tokens: [',', '.', 'a', 'brain', 'cel']...)\n",
      "Dictionary(592 unique tokens: [\"'s\", ',', '.', 'days', 'exactly']...)\n",
      "Dictionary(229 unique tokens: [',', '.', 'a', 'and', 'been']...)\n",
      "Dictionary(172 unique tokens: [\"'s\", ',', '.', 'bars', 'behind']...)\n",
      "Dictionary(415 unique tokens: ['.', '16', 'a', 'about', 'arrived']...)\n",
      "Dictionary(346 unique tokens: [',', '.', 'a', 'as', 'at']...)\n",
      "Dictionary(595 unique tokens: [',', '.', 'a', 'and', 'anti-harassment']...)\n",
      "Dictionary(12 unique tokens: ['.', 'all', 'cutest', 'of', 'robin']...)\n",
      "Dictionary(150 unique tokens: ['!', ',', '.', 'actually', 'adds']...)\n",
      "Dictionary(124 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(94 unique tokens: [',', '.', 'and', 'baseball', 'example']...)\n",
      "Dictionary(104 unique tokens: [',', '.', 'back', 'be', 'bulldogs']...)\n",
      "Dictionary(544 unique tokens: [',', '.', 'a', 'additional', 'against']...)\n",
      "Dictionary(345 unique tokens: [\"''\", '(', ')', '.', '``']...)\n",
      "Dictionary(51 unique tokens: [\"'s\", ',', '.', 'a', 'auto']...)\n",
      "Dictionary(593 unique tokens: [',', '.', 'an', 'aspirational', 'attend']...)\n",
      "Dictionary(301 unique tokens: ['(', ')', ',', '.', '2.1']...)\n",
      "Dictionary(73 unique tokens: [',', '.', 'a', 'advice', 'an']...)\n",
      "Dictionary(186 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(14 unique tokens: ['.', 'better', 'boring', 'cereal', 'is']...)\n",
      "Dictionary(426 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(193 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(325 unique tokens: ['and', 'day', 'ethan', 'fled', 'for']...)\n",
      "Dictionary(248 unique tokens: [\"'s\", ',', 'animals', 'city', 'could']...)\n",
      "Dictionary(250 unique tokens: ['.', 'a', 'achieve', 'at', 'bake']...)\n",
      "Dictionary(203 unique tokens: ['10,000', 'a', 'additional', 'almost', 'an']...)\n",
      "Dictionary(284 unique tokens: ['.', 'army', 'been', 'by', 'confirm']...)\n",
      "Dictionary(65 unique tokens: [',', '.', 'a', 'age', 'an']...)\n",
      "Dictionary(233 unique tokens: [\"'s\", '.', 'a', 'agents', 'at']...)\n",
      "Dictionary(327 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(273 unique tokens: [\"''\", '-style', '.', '``', 'a']...)\n",
      "Dictionary(339 unique tokens: [\"'\", \"''\", \"'90s/early\", ',', '.']...)\n",
      "Dictionary(286 unique tokens: [',', '.', 'a', 'according', 'admitted']...)\n",
      "Dictionary(133 unique tokens: ['distractingly', 'female', 'fight', 'photos', 'scientists']...)\n",
      "Dictionary(425 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(225 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(251 unique tokens: ['.', 'after', 'banned', 'been', 'being']...)\n",
      "Dictionary(309 unique tokens: [',', '.', 'a', 'an', 'attempting']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(463 unique tokens: [\"''\", '-', '?', '``', 'asylum']...)\n",
      "Dictionary(122 unique tokens: [\"'\", '.', 'best', 'collins', 'former']...)\n",
      "Dictionary(259 unique tokens: [\"''\", '.', '``', 'a', 'abused']...)\n",
      "Dictionary(219 unique tokens: [',', '2012', 'and', 'as', 'ayotte']...)\n",
      "Dictionary(289 unique tokens: [',', '.', '2-year-old', 'a', 'and']...)\n",
      "Dictionary(202 unique tokens: [\"''\", \"'re\", \"'s\", ',', '.']...)\n",
      "Dictionary(1229 unique tokens: ['.', '2', 'a', 'be', 'but']...)\n",
      "Dictionary(356 unique tokens: [\"''\", '.', '``', 'a', 'around']...)\n",
      "Dictionary(180 unique tokens: [\"''\", ',', '.', '81', '``']...)\n",
      "Dictionary(76 unique tokens: [',', '.', '13-year-old', 'a', 'and']...)\n",
      "Dictionary(96 unique tokens: [',', '.', 'announced', 'as', 'ceo']...)\n",
      "Dictionary(634 unique tokens: ['(', ')', ',', '.', '1970s']...)\n",
      "Dictionary(448 unique tokens: [',', '.', '24', '37,000', 'a']...)\n",
      "Dictionary(146 unique tokens: [',', '.', 'a', 'and', 'backyards']...)\n",
      "Dictionary(54 unique tokens: ['$', ',', '.', '580-a-month', ':']...)\n",
      "Dictionary(53 unique tokens: [\"'d\", \"'ll\", \"'s\", ',', '--']...)\n",
      "Dictionary(232 unique tokens: [',', '.', 'a', 'angst', 'awards']...)\n",
      "Dictionary(537 unique tokens: [',', '.', 'after', 'as', 'board']...)\n",
      "Dictionary(332 unique tokens: [\"'s\", ',', '.', '10', '2009']...)\n",
      "Dictionary(142 unique tokens: [',', '.', 'a', 'arizona', 'bankrate.com']...)\n",
      "Dictionary(238 unique tokens: [\"'s\", ',', '.', '12', 'after']...)\n",
      "Dictionary(217 unique tokens: ['american', 'at', 'crown', 'end', 'likely']...)\n",
      "Dictionary(56 unique tokens: [',', '.', 'an', 'authority', 'berth']...)\n",
      "Dictionary(359 unique tokens: [',', '.', 'a', 'abuse', 'abusive']...)\n",
      "Dictionary(90 unique tokens: ['(', ')', '?', 'awesome', 'being']...)\n",
      "Dictionary(252 unique tokens: [\"'s\", ',', '.', '3', '4']...)\n",
      "Dictionary(62 unique tokens: [',', '.', 'a', 'according', 'affairs']...)\n",
      "Dictionary(477 unique tokens: [\"'s\", ',', '.', 'about', 'an']...)\n",
      "Dictionary(230 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(341 unique tokens: ['.', 'a', 'alaska', 'and', 'at']...)\n",
      "Dictionary(38 unique tokens: [',', '.', '1.5-year-old', 'a', 'air']...)\n",
      "Dictionary(42 unique tokens: ['.', 'for', 'his', 'in', 'industry']...)\n",
      "Dictionary(89 unique tokens: [',', '.', 'a', 'adorable', 'after']...)\n",
      "Dictionary(345 unique tokens: ['.', '227', '4.5lbs', 'a', 'african']...)\n",
      "Dictionary(318 unique tokens: [\"''\", ',', '--', '.', '73']...)\n",
      "Dictionary(444 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(526 unique tokens: [',', '.', 'and', 'chief', 'co-founder']...)\n",
      "Dictionary(251 unique tokens: ['.', 'a', 'banning', 'been', 'criticised']...)\n",
      "Dictionary(315 unique tokens: ['(', ')', ',', '.', 'and']...)\n",
      "Dictionary(426 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(59 unique tokens: ['.', 'a', 'about', 'activist', 'as']...)\n",
      "Dictionary(488 unique tokens: [',', '.', 'advisers', 'and', 'blair']...)\n",
      "Dictionary(240 unique tokens: ['.', 'a', 'baby', 'before', 'cooking']...)\n",
      "Dictionary(370 unique tokens: [',', '.', '24', 'a', 'afternoon']...)\n",
      "Dictionary(119 unique tokens: ['.', 'a', 'blond', 'chris', 'ghostbusters']...)\n",
      "Dictionary(500 unique tokens: ['(', ')', ',', '.', '14-year-old']...)\n",
      "Dictionary(207 unique tokens: ['.', '2020', '26', 'a', 'addition']...)\n",
      "Dictionary(124 unique tokens: [\"'s\", '.', 'a', 'and', 'bbc']...)\n",
      "Dictionary(73 unique tokens: ['.', 'bearings', 'get', 'here', 'let']...)\n",
      "Dictionary(116 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(286 unique tokens: ['.', 'a', 'adorable', 'and', 'approached']...)\n",
      "Dictionary(193 unique tokens: [\"'ll\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(417 unique tokens: [',', '.', 'a', 'about', 'again']...)\n",
      "Dictionary(63 unique tokens: [',', '.', '6-foot-8', 'a', 'almost']...)\n",
      "Dictionary(197 unique tokens: ['.', 'a', 'armstrong', 'at', 'charity']...)\n",
      "Dictionary(234 unique tokens: [',', '.', '10,000', '140-character', 'allowing']...)\n",
      "Dictionary(302 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(8 unique tokens: ['.', 'are', 'available', 'currently', 'no']...)\n",
      "Dictionary(33 unique tokens: [',', '.', '728', 'a', 'according']...)\n",
      "Dictionary(484 unique tokens: ['.', 'afraid', 'are', 'been', 'campaigning']...)\n",
      "Dictionary(308 unique tokens: [',', 'among', 'and', 'british', 'burton']...)\n",
      "Dictionary(321 unique tokens: ['affront', 'an', 'ban', 'bodies', 'do']...)\n",
      "Dictionary(1310 unique tokens: [',', '.', 'and', 'any', 'bbc']...)\n",
      "Dictionary(504 unique tokens: [\"'s\", ',', '--', '.', 'appreciative']...)\n",
      "Dictionary(492 unique tokens: ['.', 'about', 'is', 'post', 'probably']...)\n",
      "Dictionary(47 unique tokens: ['confused…', 'i', 'kind', 'm', 'of']...)\n",
      "Dictionary(345 unique tokens: ['after', 'back', 'call', 'comes', 'divorce']...)\n",
      "Dictionary(147 unique tokens: [',', '.', 'a', 'after', 'andy']...)\n",
      "Dictionary(221 unique tokens: [',', '.', 'abc', 'attorney', 'c.']...)\n",
      "Dictionary(142 unique tokens: [\"''\", ',', '.', '``', 'are']...)\n",
      "Dictionary(525 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(205 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(174 unique tokens: [',', '.', '40', 'a', 'after']...)\n",
      "Dictionary(538 unique tokens: [',', '.', '4', 'a', 'and']...)\n",
      "Dictionary(59 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(176 unique tokens: ['$', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(768 unique tokens: [',', '.', 'a', 'abused', 'afternoon']...)\n",
      "Dictionary(355 unique tokens: [',', '.', 'a', 'are', 'as']...)\n",
      "Dictionary(47 unique tokens: ['a', 'as', 'at', 'e', 'england']...)\n",
      "Dictionary(150 unique tokens: ['.', 'an', 'barack', 'calling', 'has']...)\n",
      "Dictionary(254 unique tokens: [\"'s\", '.', '21st', 'a', 'and']...)\n",
      "Dictionary(247 unique tokens: ['-', '.', 'and', 'as', 'bombs']...)\n",
      "Dictionary(238 unique tokens: ['(', ')', ',', '.', '2016']...)\n",
      "Dictionary(24 unique tokens: ['.', 'are', 'cards', 'coming', 'dads']...)\n",
      "Dictionary(157 unique tokens: ['.', 'a', 'administrators', 'after', 'and']...)\n",
      "Dictionary(226 unique tokens: [\"'s\", '.', 'a', 'and', 'been']...)\n",
      "Dictionary(427 unique tokens: ['.', 'administration', 'citizens', 'dead', 'declares']...)\n",
      "Dictionary(319 unique tokens: [',', '.', 'about', 'and', 'as']...)\n",
      "Dictionary(168 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(109 unique tokens: ['about', 'comments', 'crying', 'female', 'in']...)\n",
      "Dictionary(49 unique tokens: [',', '.', 'a', 'administration', 'against']...)\n",
      "Dictionary(253 unique tokens: [\"'\", \"'did\", '.', 'are', 'because']...)\n",
      "Dictionary(404 unique tokens: [\"'ve\", ',', '.', '12-year-old', '2014']...)\n",
      "Dictionary(224 unique tokens: ['.', '33,000', 'an', 'and', 'babies']...)\n",
      "Dictionary(178 unique tokens: [',', '.', 'a', 'alaska', 'an']...)\n",
      "Dictionary(65 unique tokens: [',', '.', '3,000-mile', 'a', 'after']...)\n",
      "Dictionary(47 unique tokens: ['!', 'be', 'for', 'he', 'll']...)\n",
      "Dictionary(553 unique tokens: [',', '.', 'a', 'adorn', 'artist']...)\n",
      "Dictionary(198 unique tokens: [\"'s\", '.', 'a', 'and', 'college']...)\n",
      "Dictionary(406 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(250 unique tokens: ['.', 'a', 'are', 'attack', 'dentists']...)\n",
      "Dictionary(307 unique tokens: ['.', '11:50', 'et', 'p.m', 'updated']...)\n",
      "Dictionary(408 unique tokens: ['.', '9', 'for', 'ios', 'is']...)\n",
      "Dictionary(424 unique tokens: [',', '.', '1903', '1977', 'a']...)\n",
      "Dictionary(355 unique tokens: ['.', 'a', 'are', 'as', 'britain']...)\n",
      "Dictionary(400 unique tokens: ['.', 'a', 'aboard', 'added', 'caused']...)\n",
      "Dictionary(118 unique tokens: [\"''\", \"'s\", ',', '--', '.']...)\n",
      "Dictionary(343 unique tokens: [\"'s\", ',', '-', '.', '11']...)\n",
      "Dictionary(366 unique tokens: [',', '.', '16', 'a', 'accused']...)\n",
      "Dictionary(241 unique tokens: [\"'\", \"'s\", ',', '.', '11th']...)\n",
      "Dictionary(115 unique tokens: ['(', ')', '.', 'an', 'bathrooms']...)\n",
      "Dictionary(70 unique tokens: [\"''\", \"'s\", ',', '.', '1953']...)\n",
      "Dictionary(165 unique tokens: ['.', 'and', 'are', 'audiences', 'because']...)\n",
      "Dictionary(250 unique tokens: [\"'s\", '(', ')', ',', '-']...)\n",
      "Dictionary(78 unique tokens: [',', '.', 'air', 'bud', 'eat']...)\n",
      "Dictionary(393 unique tokens: ['.', 'an', 'and', 'any', 'around-the-clock']...)\n",
      "Dictionary(520 unique tokens: ['.', 'a', 'ago', 'almost', 'as']...)\n",
      "Dictionary(428 unique tokens: ['43', 'after', 'almost', 'for', 'harrowing']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(52 unique tokens: ['!', \"'s\", ',', '.', '2015']...)\n",
      "Dictionary(736 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(256 unique tokens: ['$', '--', '.', '663', 'a']...)\n",
      "Dictionary(314 unique tokens: [',', '.', 'a', 'abc', 'acknowledged']...)\n",
      "Dictionary(157 unique tokens: ['a', 'caused', 'has', 'huge', 'media']...)\n",
      "Dictionary(29 unique tokens: [',', '.', 'a', 'and', 'at']...)\n",
      "Dictionary(85 unique tokens: ['can', 'enter', 'who', '.', 'agencies']...)\n",
      "Dictionary(257 unique tokens: [',', 'aka', 'amriki', 'boston-area', 'charged']...)\n",
      "Dictionary(229 unique tokens: [\"'s\", ',', '.', '2015', 'a']...)\n",
      "Dictionary(146 unique tokens: [\"'s\", '.', 'a', 'after', 'airlines']...)\n",
      "Dictionary(500 unique tokens: ['(', ')', ',', '.', '14-year-old']...)\n",
      "Dictionary(132 unique tokens: ['#', ',', '.', 'because', 'behind']...)\n",
      "Dictionary(70 unique tokens: ['.', ':', 'be', 'can', 'celebs']...)\n",
      "Dictionary(437 unique tokens: [',', '.', '2005', 'a', 'after']...)\n",
      "Dictionary(462 unique tokens: [',', '.', ':', 'ancient', 'and']...)\n",
      "Dictionary(500 unique tokens: ['(', ')', ',', '.', '14-year-old']...)\n",
      "Dictionary(154 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(38 unique tokens: ['chicago', '.', 'airline', 'are', 'cattle']...)\n",
      "Dictionary(83 unique tokens: ['.', 'a', 'biggest', 'casual', 'enjoying']...)\n",
      "Dictionary(387 unique tokens: [',', '.', 'a', 'adequately', 'airline']...)\n",
      "Dictionary(358 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(52 unique tokens: [',', '.', 'advertisers', 'an', 'and']...)\n",
      "Dictionary(66 unique tokens: [',', '.', 'a', 'amid', 'an']...)\n",
      "Dictionary(326 unique tokens: [',', '2014', 'and', 'at', 'auckland']...)\n",
      "Dictionary(175 unique tokens: [',', '10,000', 'be', 'but', 'can']...)\n",
      "Dictionary(417 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(360 unique tokens: [\"'s\", 'does', 'for', 'men', 'money']...)\n",
      "Dictionary(267 unique tokens: [',', '.', 'a', 'already', 'and']...)\n",
      "Dictionary(166 unique tokens: ['%', ',', '.', '90', 'a']...)\n",
      "Dictionary(47 unique tokens: ['!', 'be', 'for', 'he', 'll']...)\n",
      "Dictionary(383 unique tokens: [\"'s\", ',', '.', 'a', 'blast']...)\n",
      "Dictionary(370 unique tokens: [',', '.', ':', 'a', 'an']...)\n",
      "Dictionary(366 unique tokens: [',', '.', '16', 'a', 'accused']...)\n",
      "Dictionary(189 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(216 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(300 unique tokens: ['.', 'a', 'abroad', 'and', 'been']...)\n",
      "Dictionary(406 unique tokens: [',', '--', '1996', '?', 'all']...)\n",
      "Dictionary(142 unique tokens: [',', '.', '24', 'a', 'airlines']...)\n",
      "Dictionary(73 unique tokens: [',', '--', '.', 'are', 'can']...)\n",
      "Dictionary(169 unique tokens: [\"'s\", 'attempt', 'feature', 'is', 'latest']...)\n",
      "Dictionary(139 unique tokens: [',', '.', 'after', 'apple', 'august']...)\n",
      "Dictionary(43 unique tokens: [',', '.', 'a', 'chase', 'it']...)\n",
      "Dictionary(258 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(368 unique tokens: ['conservative', 'convention', 'court', 'european', 'government']...)\n",
      "Dictionary(293 unique tokens: [',', '.', ':', 'a', 'and']...)\n",
      "Dictionary(275 unique tokens: ['-', '.', 'a', 'alive', 'and']...)\n",
      "Dictionary(468 unique tokens: [\"'s\", ',', '-', '.', 'a']...)\n",
      "Dictionary(271 unique tokens: ['(', ')', ',', '-', '.']...)\n",
      "Dictionary(448 unique tokens: [',', 'manitoba', 'winnipeg', '.', 'a']...)\n",
      "Dictionary(359 unique tokens: [\"''\", '.', '11', '``', 'been']...)\n",
      "Dictionary(439 unique tokens: [':', 'analysis', 'are', 'before', 'but']...)\n",
      "Dictionary(308 unique tokens: ['.', 'a', 'busted', 'case', 'encounters']...)\n",
      "Dictionary(235 unique tokens: [\"'s\", '.', 'a', 'after', 'controversial']...)\n",
      "Dictionary(598 unique tokens: [',', '.', '68-year-old', 'and', 'at']...)\n",
      "Dictionary(155 unique tokens: [\"''\", \"'s\", ',', '.', '2008']...)\n",
      "Dictionary(267 unique tokens: [\"'s\", ',', '.', 'aims', 'an']...)\n",
      "Dictionary(124 unique tokens: ['60th', 'after', 'called', 'club', 'died']...)\n",
      "Dictionary(289 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(263 unique tokens: ['.', '4', 'a', 'behind-the-basket', 'camera']...)\n",
      "Dictionary(233 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(286 unique tokens: ['(', ')', '.', 'a', 'adoption']...)\n",
      "Dictionary(1407 unique tokens: [',', 'after', 'an', 'captured', 'confrontation']...)\n",
      "Dictionary(291 unique tokens: ['.', 'is', 'revenge', 'so', 'sometimes']...)\n",
      "Dictionary(1009 unique tokens: [',', '.', '2014', '26', 'a']...)\n",
      "Dictionary(427 unique tokens: ['.', '3', ':', 'article', 'black']...)\n",
      "Dictionary(270 unique tokens: [',', 'by', 'concerns', 'cooke', 'death']...)\n",
      "Dictionary(13 unique tokens: [',', '.', 'care', 'clap', 'don']...)\n",
      "Dictionary(174 unique tokens: ['.', '2015', 'achievement', 'added', 'against']...)\n",
      "Dictionary(166 unique tokens: [',', '.', '2018', ':', 'a']...)\n",
      "Dictionary(55 unique tokens: ['.', ':', 'appear', 'assets', 'be']...)\n",
      "Dictionary(290 unique tokens: ['?', 'artificial', 'be', 'could', 'intelligence']...)\n",
      "Dictionary(722 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(479 unique tokens: [',', '.', '12-year-old', 'a', 'against']...)\n",
      "Dictionary(146 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(166 unique tokens: [',', '.', 'a', 'all', 'and']...)\n",
      "Dictionary(16 unique tokens: [',', '.', 'any', 'honestly', 'it']...)\n",
      "Dictionary(283 unique tokens: ['.', 'collections', 'ever', 'is', 'it']...)\n",
      "Dictionary(88 unique tokens: ['.', 'an', 'football', 'friday', 'from']...)\n",
      "Dictionary(192 unique tokens: [\"'s\", ',', '.', 'a', 'before']...)\n",
      "Dictionary(345 unique tokens: ['after', 'back', 'call', 'comes', 'divorce']...)\n",
      "Dictionary(375 unique tokens: ['.', '4:40', ':', 'et', 'p.m']...)\n",
      "Dictionary(19 unique tokens: ['.', '1970s', '?', 'click', 'cnn']...)\n",
      "Dictionary(116 unique tokens: [',', '.', '1', 'a', 'addressed']...)\n",
      "Dictionary(232 unique tokens: ['.', 'a', 'aims', 'ambitious', 'an']...)\n",
      "Dictionary(1857 unique tokens: [',', '.', 'along', 'and', 'as']...)\n",
      "Dictionary(522 unique tokens: ['(', ')', '.', '2016', ':']...)\n",
      "Dictionary(189 unique tokens: [',', '.', 'a', 'and', 'bedroom']...)\n",
      "Dictionary(1457 unique tokens: [\"''\", '--', '.', '2014', '3']...)\n",
      "Dictionary(239 unique tokens: ['--', '.', '200', '93', 'after']...)\n",
      "Dictionary(111 unique tokens: ['.', 'a', 'easy', 'friend', 'goodbye']...)\n",
      "Dictionary(76 unique tokens: [\"''\", '.', '``', 'a', 'apply']...)\n",
      "Dictionary(382 unique tokens: [',', '.', '400', 'a', 'against']...)\n",
      "Dictionary(272 unique tokens: [',', '.', '14', '140', 'among']...)\n",
      "Dictionary(457 unique tokens: [',', '.', 'and', 'been', 'can']...)\n",
      "Dictionary(104 unique tokens: [',', '.', '?', 'an', 'and']...)\n",
      "Dictionary(576 unique tokens: [\"'s\", ',', '.', '5', 'a']...)\n",
      "Dictionary(396 unique tokens: [\"''\", ',', '--', '.', '``']...)\n",
      "Dictionary(106 unique tokens: ['.', 'feeeeeeeeeels', 'the', ',', 'a']...)\n",
      "Dictionary(71 unique tokens: [',', '.', 'a', 'activity', 'ahold']...)\n",
      "Dictionary(131 unique tokens: ['2', 'a', 'about', 'actor', 'adapt']...)\n",
      "Dictionary(412 unique tokens: [',', '.', '80', 'a', 'angeles']...)\n",
      "Dictionary(466 unique tokens: ['(', ')', '.', 'and', 'architect']...)\n",
      "Dictionary(357 unique tokens: [\"'s\", ',', '.', 'a', 'but']...)\n",
      "Dictionary(507 unique tokens: [',', '.', '12', ':', 'a']...)\n",
      "Dictionary(135 unique tokens: ['share', 'this', 'with', \"'s\", ',']...)\n",
      "Dictionary(1030 unique tokens: ['.', 'a', 'for', 'gent', 'handbook']...)\n",
      "Dictionary(52 unique tokens: ['!', \"'s\", ',', '.', '2015']...)\n",
      "Dictionary(479 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(920 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(138 unique tokens: ['.', 'animals', 'are', 'by', 'instantly']...)\n",
      "Dictionary(342 unique tokens: ['$', \"'s\", ',', '.', '/']...)\n",
      "Dictionary(232 unique tokens: [',', '.', 'a', 'apparently', 'but']...)\n",
      "Dictionary(203 unique tokens: ['10,000', 'a', 'additional', 'almost', 'an']...)\n",
      "Dictionary(269 unique tokens: ['.', 'a', 'after', 'an', 'and']...)\n",
      "Dictionary(935 unique tokens: [',', '13', '398', 'after', 'at']...)\n",
      "Dictionary(176 unique tokens: ['.', '2015', '4', 'a', 'after']...)\n",
      "Dictionary(452 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(425 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(55 unique tokens: ['.', 'a', 'help', 'inc.', 'is']...)\n",
      "Dictionary(118 unique tokens: [',', '.', '26-year-old', 'a', 'as']...)\n",
      "Dictionary(164 unique tokens: [',', '.', '26', 'after', 'and']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1243 unique tokens: [',', '.', 'a', 'as', 'back']...)\n",
      "Dictionary(247 unique tokens: ['and', 'candidates', 'conference', 'experiences', 'for']...)\n",
      "Dictionary(366 unique tokens: [',', '.', '16', 'a', 'accused']...)\n",
      "Dictionary(95 unique tokens: ['.', 'as', 'been', 'bernthal', 'cast']...)\n",
      "Dictionary(590 unique tokens: [',', '.', 'a', 'actor', 'all']...)\n",
      "Dictionary(64 unique tokens: ['#', '-', '.', '2', ':']...)\n",
      "Dictionary(374 unique tokens: [',', '.', 'a', 'accusing', 'an']...)\n",
      "Dictionary(417 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(305 unique tokens: [',', '11', '2015', 'by', 'june']...)\n",
      "Dictionary(525 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(477 unique tokens: [\"'s\", ',', '.', 'about', 'an']...)\n",
      "Dictionary(287 unique tokens: [\"'s\", '.', 'also', 'been', 'brutality']...)\n",
      "Dictionary(119 unique tokens: [\"'s\", '.', 'already', 'at', 'be']...)\n",
      "Dictionary(242 unique tokens: [',', '.', 'against', 'airline', 'alps']...)\n",
      "Dictionary(367 unique tokens: [\"'s\", '.', '2011', 'a', 'america']...)\n",
      "Dictionary(113 unique tokens: [',', '--', '?', 'and', 'brody']...)\n",
      "Dictionary(573 unique tokens: [',', 'approaching', 'culture', 'depictions', 'fast']...)\n",
      "Dictionary(324 unique tokens: [',', '.', 'a', 'albert', 'angola']...)\n",
      "Dictionary(220 unique tokens: [',', '.', '2015', 'among', 'and']...)\n",
      "Dictionary(160 unique tokens: ['.', '19-year-old', 'a', 'and', 'as']...)\n",
      "Dictionary(373 unique tokens: [',', '.', '7', '9', '?']...)\n",
      "Dictionary(382 unique tokens: [\"'s\", '.', 'country', 'culture', 'experiencing']...)\n",
      "Dictionary(369 unique tokens: [\"'\", \"'s\", ',', '.', '2-1']...)\n",
      "Dictionary(203 unique tokens: [\"'s\", '.', 'airbus', 'at', 'bright']...)\n",
      "Dictionary(171 unique tokens: ['0-0', 'changes', 'drew', 'form', 'hodgson']...)\n",
      "Dictionary(569 unique tokens: ['(', ')', ':', 'all', 'be']...)\n",
      "Dictionary(259 unique tokens: [\"'s\", ',', '.', '800th', ':']...)\n",
      "Dictionary(184 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(559 unique tokens: [',', '.', '150mph', '24-hour', '60']...)\n",
      "Dictionary(387 unique tokens: [',', '.', 'a', 'appearance', 'as']...)\n",
      "Dictionary(170 unique tokens: ['.', 'armour', 'best', 'grant', 'has']...)\n",
      "Dictionary(1407 unique tokens: [',', 'after', 'an', 'captured', 'confrontation']...)\n",
      "Dictionary(419 unique tokens: [\"'s\", ',', '.', 'all', 'and']...)\n",
      "Dictionary(576 unique tokens: ['(', ')', '.', 'a', 'aided']...)\n",
      "Dictionary(140 unique tokens: ['.', 'a', 'alabama', 'campus', 'coach']...)\n",
      "Dictionary(23 unique tokens: ['.', 'any', 'at', 'dependent', 'i']...)\n",
      "Dictionary(96 unique tokens: [\"'\", ',', '.', '49ers', '?']...)\n",
      "Dictionary(261 unique tokens: [',', '.', '4-month-old', 'a', 'baby']...)\n",
      "Dictionary(47 unique tokens: ['a', 'as', 'at', 'e', 'england']...)\n",
      "Dictionary(546 unique tokens: ['!', ',', '.', 'and', 'are']...)\n",
      "Dictionary(116 unique tokens: ['?', 'changing', 'did', 'his', 'is']...)\n",
      "Dictionary(375 unique tokens: [',', '.', 'add', 'and', 'been']...)\n",
      "Dictionary(257 unique tokens: ['by', ',', '.', 'considered', 'court']...)\n",
      "Dictionary(24 unique tokens: [',', '.', '11', 'alani', 'among']...)\n",
      "Dictionary(157 unique tokens: ['.', 'girls', 'meet', 'new', 'powerpuff']...)\n",
      "Dictionary(116 unique tokens: [',', '.', '2015', '4', 'a']...)\n",
      "Dictionary(129 unique tokens: [\"'s\", ':', '?', 'always', 'better']...)\n",
      "Dictionary(261 unique tokens: [',', '.', '12,490', '153', 'a']...)\n",
      "Dictionary(698 unique tokens: [',', '.', 'a', 'and', 'at']...)\n",
      "Dictionary(151 unique tokens: ['.', 'a', 'cat', 'crocodiles', 'deliberately']...)\n",
      "Dictionary(668 unique tokens: ['(', ')', ',', ':', 'ap']...)\n",
      "Dictionary(264 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(402 unique tokens: [\"''\", \"'s\", ',', '--', '.']...)\n",
      "Dictionary(619 unique tokens: [',', '.', 'a', 'ahead', 'an']...)\n",
      "Dictionary(379 unique tokens: [\"''\", '(', ')', '.', '``']...)\n",
      "Dictionary(104 unique tokens: ['.', 'contract', 'estée', 'is', 'jenner']...)\n",
      "Dictionary(469 unique tokens: ['.', 'a', 'about', 'after', 'all']...)\n",
      "Dictionary(343 unique tokens: [',', '--', '.', '2014', 'a']...)\n",
      "Dictionary(14 unique tokens: ['.', 'at', 'best', 'correction', 'its']...)\n",
      "Dictionary(45 unique tokens: [\"'s\", ',', '.', 'against', 'and']...)\n",
      "Dictionary(184 unique tokens: [',', '.', 'a', 'accompanied', 'across']...)\n",
      "Dictionary(216 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(289 unique tokens: ['.', 'and', 'anything', 'avoid', 'be']...)\n",
      "Dictionary(135 unique tokens: [',', '.', '52', 'age', 'at']...)\n",
      "Dictionary(181 unique tokens: [',', '.', '1-year-old', '4', '59-year-old']...)\n",
      "Dictionary(771 unique tokens: ['.', 'a', 'england', 'finals', 'gathered']...)\n",
      "Dictionary(205 unique tokens: [',', '.', '9', 'across', 'admissions']...)\n",
      "Dictionary(288 unique tokens: [\"'\", '.', 'bikers', 'comfort', 'food']...)\n",
      "Dictionary(30 unique tokens: [',', '.', 'a', 'adorn', 'artist']...)\n",
      "Dictionary(443 unique tokens: [',', '.', 'a', 'according', 'after']...)\n",
      "Dictionary(23 unique tokens: ['.', 'and', 'antonio', 'art', 'artist']...)\n",
      "Dictionary(297 unique tokens: [',', '.', 'a', 'abc', 'after']...)\n",
      "Dictionary(405 unique tokens: ['$', \"''\", ',', '-', '.']...)\n",
      "Dictionary(67 unique tokens: ['.', 'afternoon', 'an', 'appear', 'bank']...)\n",
      "Dictionary(321 unique tokens: ['affront', 'an', 'ban', 'bodies', 'do']...)\n",
      "Dictionary(8 unique tokens: ['.', 'and', 'pose', ',', '2017']...)\n",
      "Dictionary(436 unique tokens: ['a', 'affordable', 'buyers', 'does', 'few']...)\n",
      "Dictionary(484 unique tokens: [',', '.', 'a', 'against', 'assaulted']...)\n",
      "Dictionary(265 unique tokens: [\"''\", '.', '``', 'a', 'activist']...)\n",
      "Dictionary(604 unique tokens: [',', '.', '40', 'a', 'a.']...)\n",
      "Dictionary(430 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(617 unique tokens: ['.', 'aziz', 'help', 'here', 'is']...)\n",
      "Dictionary(297 unique tokens: ['.', 'at', 'biology', 'class', 'colorado']...)\n",
      "Dictionary(302 unique tokens: ['share', 'this', 'with', '.', 'appointed']...)\n",
      "Dictionary(186 unique tokens: [',', '.', 'a', 'became', 'conditions']...)\n",
      "Dictionary(172 unique tokens: ['(', ')', ',', '.', '6-3']...)\n",
      "Dictionary(13 unique tokens: ['.', 'fast', 'grow', 'so', 'they']...)\n",
      "Dictionary(336 unique tokens: [',', '.', 'a', 'again', 'battle']...)\n",
      "Dictionary(151 unique tokens: [\"'ll\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(202 unique tokens: ['(', ')', '.', 'fanboy', 'hollywood']...)\n",
      "Dictionary(228 unique tokens: ['.', '911', 'a', 'and', 'anguish']...)\n",
      "Dictionary(407 unique tokens: [\"'\", \"'dark\", ',', '-', '.']...)\n",
      "Dictionary(356 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(643 unique tokens: [',', '.', '?', 'and', 'anticipation']...)\n",
      "Dictionary(106 unique tokens: [\"'\", \"'ll\", \"'s\", ',', '.']...)\n",
      "Dictionary(742 unique tokens: [',', '.', '16', '1922', 'about']...)\n",
      "Dictionary(199 unique tokens: [\"''\", ',', '.', '``', 'and']...)\n",
      "Dictionary(439 unique tokens: [',', '.', '13th', '5-0', 'career']...)\n",
      "Dictionary(482 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(968 unique tokens: [':', 'cocotos', 'for', 'illustration', 'news']...)\n",
      "Dictionary(321 unique tokens: [\"'s\", '(', ')', ',', '--']...)\n",
      "Dictionary(225 unique tokens: ['photo', 'view', '.', 'a', 'after']...)\n",
      "Dictionary(133 unique tokens: ['.', 'absolute', 'an', 'basketball', 'beast']...)\n",
      "Dictionary(176 unique tokens: [\"'s\", '.', '200,000', 'a', 'after']...)\n",
      "Dictionary(250 unique tokens: ['$', ',', '.', '21st', '70bn']...)\n",
      "Dictionary(103 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(328 unique tokens: ['photo', 'view', ',', '.', 'babies']...)\n",
      "Dictionary(249 unique tokens: ['.', 'a', 'basketball', 'big', 'cedar']...)\n",
      "Dictionary(279 unique tokens: ['(', ')', '.', ':', 'a']...)\n",
      "Dictionary(469 unique tokens: ['(', ')', '.', ':', 'about']...)\n",
      "Dictionary(80 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(169 unique tokens: [\"''\", ',', '.', '30-year-old', '``']...)\n",
      "Dictionary(150 unique tokens: ['.', 'a', 'agenda', 'ahead', 'barack']...)\n",
      "Dictionary(301 unique tokens: [';', 'a', 'advice', 'affect', 'an']...)\n",
      "Dictionary(170 unique tokens: ['%', '500,000,000,000', 'after', 'at', 'bank']...)\n",
      "Dictionary(1240 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(143 unique tokens: ['.', 'a', 'an', 'apt', 'as']...)\n",
      "Dictionary(395 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(206 unique tokens: [',', 'a', 'found', 'has', 'is']...)\n",
      "Dictionary(160 unique tokens: ['.', 'and', 'are', 'court', 'decides']...)\n",
      "Dictionary(229 unique tokens: [\"'s\", ',', '.', 'accustomed', 'all']...)\n",
      "Dictionary(241 unique tokens: [\"'s\", ',', '--', '.', 'a']...)\n",
      "Dictionary(139 unique tokens: [\"'s\", ',', '.', 'a', 'anaconda-deer']...)\n",
      "Dictionary(383 unique tokens: ['.', '?', 'a', 'actually', 'as']...)\n",
      "Dictionary(429 unique tokens: [\"''\", \"'s\", ',', '--', '.']...)\n",
      "Dictionary(300 unique tokens: ['(', ')', '.', '/facebook', ':']...)\n",
      "Dictionary(51 unique tokens: ['.', '50,000', 'a', 'as', 'eliminate']...)\n",
      "Dictionary(93 unique tokens: [\"'s\", ',', '.', 'affairs', 'and']...)\n",
      "Dictionary(150 unique tokens: [',', '.', '140-character', ':', 'a']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(329 unique tokens: ['!', ',', '.', '?', 'a']...)\n",
      "Dictionary(57 unique tokens: ['.', 'a', 'episode', 'fourth', 'fresh']...)\n",
      "Dictionary(163 unique tokens: ['.', ':', 'a', 'airlines', 'be']...)\n",
      "Dictionary(238 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(349 unique tokens: [\"'\", \"'interstellar\", \"'jack\", \"'spooks\", \"'the\"]...)\n",
      "Dictionary(30 unique tokens: [',', '.', 'behind', 'drake', 'isn']...)\n",
      "Dictionary(111 unique tokens: [',', '.', ':', 'a', 'all']...)\n",
      "Dictionary(2 unique tokens: ['featured', 'matches'])\n",
      "Dictionary(474 unique tokens: [',', '.', 'a', 'administration', 'allies']...)\n",
      "Dictionary(211 unique tokens: [',', '.', '2012', '2013', 'a']...)\n",
      "Dictionary(77 unique tokens: [',', '.', '8th', 'a', 'as']...)\n",
      "Dictionary(835 unique tokens: ['.', 'an', 'exotic', 'feel', 'i']...)\n",
      "Dictionary(705 unique tokens: ['.', 'absence', 'and', 'are', 'as']...)\n",
      "Dictionary(2 unique tokens: ['featured', 'matches'])\n",
      "Dictionary(255 unique tokens: ['photo', 'view', ',', '.', '43']...)\n",
      "Dictionary(590 unique tokens: [',', '.', 'a', 'actor', 'all']...)\n",
      "Dictionary(306 unique tokens: [\"''\", \"'s\", ',', '.', '?']...)\n",
      "Dictionary(106 unique tokens: [\"''\", ',', '.', '2011', '2013']...)\n",
      "Dictionary(979 unique tokens: [',', '.', 'accident', 'an', 'british']...)\n",
      "Dictionary(400 unique tokens: [',', '.', '17', 'and', 'at']...)\n",
      "Dictionary(244 unique tokens: [\"'s\", '.', 'a', 'criminal', 'gang']...)\n",
      "Dictionary(102 unique tokens: [\"'s\", ',', '.', '...', '2015']...)\n",
      "Dictionary(397 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(146 unique tokens: [\"'s\", ',', '.', '9', ':']...)\n",
      "Dictionary(243 unique tokens: ['.', '?', 'a', 'but', 'create']...)\n",
      "Dictionary(356 unique tokens: ['--', '.', 'a', 'badawi', 'been']...)\n",
      "Dictionary(368 unique tokens: ['.', 'a', 'around', 'bringing', 'from']...)\n",
      "Dictionary(300 unique tokens: [\"'s\", ',', '.', '11', '11:04']...)\n",
      "Dictionary(316 unique tokens: ['!', ',', '.', '?', 'a']...)\n",
      "Dictionary(90 unique tokens: ['!', ',', ':', 'a', 'about']...)\n",
      "Dictionary(1116 unique tokens: [\"'s\", ',', ':', '?', 'a']...)\n",
      "Dictionary(792 unique tokens: ['.', '14-year-old', 'a', 'after', 'arrested']...)\n",
      "Dictionary(216 unique tokens: [',', '.', '18-year-old', 'as', 'barrett']...)\n",
      "Dictionary(174 unique tokens: ['.', '2016', 'a', 'against', 'association']...)\n",
      "Dictionary(214 unique tokens: [',', '.', 'a', 'amount', 'and']...)\n",
      "Dictionary(280 unique tokens: ['.', 'a', 'after', 'against', 'at']...)\n",
      "Dictionary(205 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(512 unique tokens: [\"'s\", '.', 'a', 'bitcoin', 'hard']...)\n",
      "Dictionary(203 unique tokens: ['10,000', 'a', 'additional', 'almost', 'an']...)\n",
      "Dictionary(11 unique tokens: ['bruce', 'courtesy', 'ofâ', 'shapiro', 'video']...)\n",
      "Dictionary(165 unique tokens: [',', 'accused', 'airport', 'alean', 'andres']...)\n",
      "Dictionary(294 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(844 unique tokens: [',', 'all', 'by', 'happened', 'kurt']...)\n",
      "Dictionary(925 unique tokens: ['by', 'eli', 'epstein', 'infiniti', 'presented']...)\n",
      "Dictionary(254 unique tokens: [',', '.', '93', 'a', 'actor']...)\n",
      "Dictionary(45 unique tokens: ['disabled', 'is', 'javascript', 'enable', 'need']...)\n",
      "Dictionary(66 unique tokens: [',', '.', 'about', 'activist', 'and']...)\n",
      "Dictionary(112 unique tokens: [',', '.', 'angeles', 'are', 'as']...)\n",
      "Dictionary(585 unique tokens: [\"'\", \"''\", \"'jurassic\", ',', '.']...)\n",
      "Dictionary(352 unique tokens: ['.', 'a', 'abdel-fattah', 'after', 'away']...)\n",
      "Dictionary(428 unique tokens: [',', '.', 'according', 'and', 'but']...)\n",
      "Dictionary(87 unique tokens: [',', '.', '21st', 'as', 'century']...)\n",
      "Dictionary(136 unique tokens: ['.', 'a', 'ahmad', 'design', 'elements']...)\n",
      "Dictionary(134 unique tokens: [\"'re\", ',', '.', 'alone', 'draymond']...)\n",
      "Dictionary(8 unique tokens: ['.', 'are', 'available', 'currently', 'no']...)\n",
      "Dictionary(321 unique tokens: ['.', 'a', 'as', 'can', 'dollars']...)\n",
      "Dictionary(577 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(311 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(736 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(134 unique tokens: ['.', 'a', 'an', 'elderly', 'gone']...)\n",
      "Dictionary(23 unique tokens: [',', 'about', 'adventure-minded', 'big', 'consider']...)\n",
      "Dictionary(99 unique tokens: [',', '.', 'a', 'everyone', 'for']...)\n",
      "Dictionary(140 unique tokens: ['$', '.', '15', 'an', 'angeles']...)\n",
      "Dictionary(256 unique tokens: [\"'\", ',', '--', '.', 'against']...)\n",
      "Dictionary(210 unique tokens: ['.', '17', 'a', 'after', 'at']...)\n",
      "Dictionary(406 unique tokens: [',', '.', '2003', 'a', 'accept']...)\n",
      "Dictionary(206 unique tokens: ['$', '&', \"'s\", ',', '.']...)\n",
      "Dictionary(104 unique tokens: ['a', 'alcohol', 'allege', 'azerbaijan', 'been']...)\n",
      "Dictionary(140 unique tokens: [',', '.', 'and', 'are', 'be']...)\n",
      "Dictionary(375 unique tokens: [',', '.', '12', '2010', 'a']...)\n",
      "Dictionary(241 unique tokens: [\"'\", \"'worrying\", '.', 'a', 'all']...)\n",
      "Dictionary(193 unique tokens: [\"'s\", ',', '.', 'a', 'able']...)\n",
      "Dictionary(388 unique tokens: ['by', '.', 'a', 'and', 'big']...)\n",
      "Dictionary(569 unique tokens: ['(', ')', ',', '21', 'a']...)\n",
      "Dictionary(147 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(484 unique tokens: [\"'\", \"'permit-to-purchase\", \"'s\", ',', 'a']...)\n",
      "Dictionary(147 unique tokens: [',', '.', 'a', 'and', 'angry']...)\n",
      "Dictionary(269 unique tokens: [',', '.', 'a', 'an', 'arrested']...)\n",
      "Dictionary(445 unique tokens: ['--', '.', '2013', 'a', 'after']...)\n",
      "Dictionary(79 unique tokens: [\"'s\", ',', '.', 'a', 'american']...)\n",
      "Dictionary(376 unique tokens: ['.', '100', 'around', 'being', 'border']...)\n",
      "Dictionary(337 unique tokens: ['alasdair', 'by', 'lamont', ',', 'at']...)\n",
      "Dictionary(171 unique tokens: ['(', ')', '.', 'a', 'able']...)\n",
      "Dictionary(445 unique tokens: ['from', 'life', 'the', 'topic', '15/06/15']...)\n",
      "Dictionary(118 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(198 unique tokens: [\"'\", \"'90\", ',', '.', '10']...)\n",
      "Dictionary(81 unique tokens: ['.', 'compensation', 'employee', 'for', 'in']...)\n",
      "Dictionary(164 unique tokens: ['share', 'this', 'with', ',', '.']...)\n",
      "Dictionary(314 unique tokens: ['.', '/', '2013', 'a', 'and']...)\n",
      "Dictionary(332 unique tokens: [',', '.', 'a', 'ambushed', 'and']...)\n",
      "Dictionary(209 unique tokens: [\"'s\", '.', '3', 'a', 'angola']...)\n",
      "Dictionary(82 unique tokens: ['//t.co/0njbviyguj', ':', 'cool', 'flag', 'georgia']...)\n",
      "Dictionary(250 unique tokens: ['.', '3', '75-year-old', 'a', 'admitted']...)\n",
      "Dictionary(180 unique tokens: [',', '.', 'a', 'and', 'antoinette']...)\n",
      "Dictionary(323 unique tokens: [',', '.', '400', 'a', 'accepted']...)\n",
      "Dictionary(139 unique tokens: ['.', '73', 'a', 'active', 'administration']...)\n",
      "Dictionary(577 unique tokens: [',', '.', '30-mile', '90-foot', 'a']...)\n",
      "Dictionary(17 unique tokens: ['.', 'are', 'at', 'be', 'celebrities']...)\n",
      "Dictionary(67 unique tokens: [\"'s\", ',', '.', 'along', 'and']...)\n",
      "Dictionary(355 unique tokens: [',', '-', '.', 'a', 'among']...)\n",
      "Dictionary(65 unique tokens: [',', '.', '10', 'a', 'and']...)\n",
      "Dictionary(326 unique tokens: [\"'s\", ',', '.', '12', 'a']...)\n",
      "Dictionary(158 unique tokens: ['a', 'at', 'be', 'bridge', 'computerised']...)\n",
      "Dictionary(61 unique tokens: [',', '.', 'amid', 'defense', 'discussing']...)\n",
      "Dictionary(69 unique tokens: ['.', 'is', 'it', 'kids', 'like']...)\n",
      "Dictionary(147 unique tokens: ['.', '95-93', 'a', 'cavaliers', 'cleveland']...)\n",
      "Dictionary(539 unique tokens: [',', '.', '5,000', 'a', 'aggression']...)\n",
      "Dictionary(292 unique tokens: [',', '.', 'a', 'act', 'affordable']...)\n",
      "Dictionary(424 unique tokens: ['.', 'are', 'dads', 'in', \"''\"]...)\n",
      "Dictionary(605 unique tokens: [',', '.', ':', 'a', 'after']...)\n",
      "Dictionary(37 unique tokens: ['cleveland', ',', '.', 'coming', 'finals']...)\n",
      "Dictionary(278 unique tokens: [\"'s\", ',', '.', '38', '55']...)\n",
      "Dictionary(351 unique tokens: [\"'s\", ',', '.', '7', 'an']...)\n",
      "Dictionary(155 unique tokens: [',', '50', 'and', 'are', 'around']...)\n",
      "Dictionary(602 unique tokens: [',', '.', 'and', 'azerbaijani', 'baku']...)\n",
      "Dictionary(617 unique tokens: ['.', 'aziz', 'help', 'here', 'is']...)\n",
      "Dictionary(257 unique tokens: ['.', '3-mile', 'a', 'ago', 'area']...)\n",
      "Dictionary(207 unique tokens: [\"'s\", ',', '.', '30', 'a']...)\n",
      "Dictionary(192 unique tokens: [\"'s\", ',', '.', '13', 'after']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(275 unique tokens: [',', '.', ':', 'a', 'air']...)\n",
      "Dictionary(160 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(434 unique tokens: ['.', 'businesses', 'city', 'disabled', 'financiers']...)\n",
      "Dictionary(118 unique tokens: ['brought', 'chicago', 'on', 'rapper', 'saturday']...)\n",
      "Dictionary(470 unique tokens: [\"'s\", \"'ve\", '(', ')', '.']...)\n",
      "Dictionary(278 unique tokens: ['from', 'life', 'the', 'topic', '11/06/15']...)\n",
      "Dictionary(211 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(542 unique tokens: ['(', ')', '.', 'a', 'administrative']...)\n",
      "Dictionary(88 unique tokens: ['?', 'be', 'could', 'jennifer', 'lawrence']...)\n",
      "Dictionary(86 unique tokens: [\"'s\", ',', '.', 'about', 'apple']...)\n",
      "Dictionary(133 unique tokens: [',', '.', '2014', '60,000', '9']...)\n",
      "Dictionary(639 unique tokens: [',', '.', 'birds', 'broken', 'by']...)\n",
      "Dictionary(413 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(123 unique tokens: [',', '.', 'according', 'am', 'an']...)\n",
      "Dictionary(171 unique tokens: ['.', '?', 'and', 'by', 'color']...)\n",
      "Dictionary(162 unique tokens: [\"''\", ',', '--', '.', '``']...)\n",
      "Dictionary(399 unique tokens: ['(', ')', '.', 'a', 'arrest']...)\n",
      "Dictionary(164 unique tokens: ['.', '...', 'bright', 'ceres', 'even']...)\n",
      "Dictionary(623 unique tokens: ['!', '(', ')', ',', '.']...)\n",
      "Dictionary(49 unique tokens: [',', '.', 'a', 'ads', 'against']...)\n",
      "Dictionary(203 unique tokens: [\"'s\", ',', '.', 'a', 'amassed']...)\n",
      "Dictionary(202 unique tokens: ['.', '30,000', 'a', 'almost', 'asking']...)\n",
      "Dictionary(26 unique tokens: [',', '.', 'a', 'according', 'airport']...)\n",
      "Dictionary(142 unique tokens: [\"'s\", ',', '.', 'a', 'accidentally']...)\n",
      "Dictionary(98 unique tokens: [',', '.', 'about', 'all', 'draft']...)\n",
      "Dictionary(420 unique tokens: ['.', 'because', 'but', 'chance', 'gurkhas']...)\n",
      "Dictionary(86 unique tokens: [\"'s\", ',', '.', 'a', 'about']...)\n",
      "Dictionary(51 unique tokens: ['&', ',', '.', '12', 'agreement']...)\n",
      "Dictionary(246 unique tokens: ['$', \"'s\", ',', '.', '1']...)\n",
      "Dictionary(316 unique tokens: [',', 'about', 'and', 'answer', 'answered']...)\n",
      "Dictionary(201 unique tokens: ['a', 'be', 'blame', 'cats', 'found']...)\n",
      "Dictionary(261 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(247 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(307 unique tokens: [',', '.', '2012', 'a', 'become']...)\n",
      "Dictionary(516 unique tokens: ['.', 'airlines', 'and', 'as', 'booking']...)\n",
      "Dictionary(52 unique tokens: ['!', \"'s\", ',', '.', '2015']...)\n",
      "Dictionary(479 unique tokens: ['.', 'ahead', 'crying', 'crying…', 'i']...)\n",
      "Dictionary(201 unique tokens: [',', '.', '2015', 'after', 'cities']...)\n",
      "Dictionary(207 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(381 unique tokens: [\"'\", \"'really\", ',', '.', '2014']...)\n",
      "Dictionary(56 unique tokens: [',', '.', '40', 'a', 'abed']...)\n",
      "Dictionary(34 unique tokens: ['!', 'it', 'obvious', 'omg', 's']...)\n",
      "Dictionary(55 unique tokens: [',', 'follow', 'i', 'lead', 'where']...)\n",
      "Dictionary(216 unique tokens: ['(', ')', '.', 'a', 'and']...)\n",
      "Dictionary(765 unique tokens: ['$', ',', '.', '1971', '3.50']...)\n",
      "Dictionary(189 unique tokens: [',', '.', '10', '10-man', '14th']...)\n",
      "Dictionary(92 unique tokens: ['.', ':', 'alert', 'can', 'chrome']...)\n",
      "Dictionary(207 unique tokens: [',', '.', 'advised', 'ai', 'an']...)\n",
      "Dictionary(168 unique tokens: [\"'s\", ',', '.', 'a', 'african']...)\n",
      "Dictionary(380 unique tokens: [',', '.', 'a', 'afloat', 'an']...)\n",
      "Dictionary(197 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(856 unique tokens: [',', '.', ':', 'a', 'asked']...)\n",
      "Dictionary(22 unique tokens: [',', '.', 'arizona', 'city', 'council']...)\n",
      "Dictionary(205 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(717 unique tokens: [',', '--', '.', 'a', 'affixed']...)\n",
      "Dictionary(144 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(26 unique tokens: ['(', ')', '.', 'all', 'at']...)\n",
      "Dictionary(272 unique tokens: ['a', 'and', 'audi', 'be', 'between']...)\n",
      "Dictionary(323 unique tokens: [\"'s\", ',', '.', '/', 'abaca']...)\n",
      "Dictionary(199 unique tokens: ['by', 'foley', 'kaye', ',', '.']...)\n",
      "Dictionary(425 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(23 unique tokens: [',', '--', '.', 'a', 'across']...)\n",
      "Dictionary(268 unique tokens: [',', '.', 'a', 'according', 'adult']...)\n",
      "Dictionary(165 unique tokens: [\"'s\", ',', 'and', 'battle', 'between']...)\n",
      "Dictionary(372 unique tokens: [',', '.', 'a', 'and', 'ask']...)\n",
      "Dictionary(194 unique tokens: [',', '76', '92', 'adopt', 'after']...)\n",
      "Dictionary(216 unique tokens: ['(', ')', '.', 'a', 'and']...)\n",
      "Dictionary(115 unique tokens: ['share', 'this', 'with', '.', 'andy']...)\n",
      "Dictionary(510 unique tokens: ['-', '.', '5', 'a', 'and']...)\n",
      "Dictionary(115 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(332 unique tokens: [',', '.', 'a', 'after', 'an']...)\n",
      "Dictionary(85 unique tokens: [\"'\", ',', '.', '2015', '4']...)\n",
      "Dictionary(105 unique tokens: [\"'s\", ',', '.', '2', '?']...)\n",
      "Dictionary(1005 unique tokens: [',', '.', 'afternoon', 'and', 'bursts']...)\n",
      "Dictionary(545 unique tokens: ['.', 'a', 'do', 'fitness', 'for']...)\n",
      "Dictionary(8 unique tokens: ['.', 'are', 'available', 'currently', 'no']...)\n",
      "Dictionary(569 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(146 unique tokens: ['.', 'appropriately', 'epic', 'finale', 'games']...)\n",
      "Dictionary(195 unique tokens: ['.', 'a', 'being', 'brazen', 'break']...)\n",
      "Dictionary(269 unique tokens: [',', '60m', 'after', 'and', 'are']...)\n",
      "Dictionary(58 unique tokens: [',', '.', '14-year-old', 'a', 'administrative']...)\n",
      "Dictionary(52 unique tokens: [',', '.', 'before', 'bringing', 'cable-tv']...)\n",
      "Dictionary(174 unique tokens: [\"'s\", '.', 'a', 'aretha', 'channeled']...)\n",
      "Dictionary(382 unique tokens: [\"'re\", ',', '.', 'a', 'again']...)\n",
      "Dictionary(34 unique tokens: [',', '.', 'a', 'about', 'and']...)\n",
      "Dictionary(27 unique tokens: [\"'s\", '.', '1', '50', 'annual']...)\n",
      "Dictionary(252 unique tokens: [\"'re\", '?', 'do', 'far', 'how']...)\n",
      "Dictionary(102 unique tokens: [',', '.', 'at', 'company', 'consumer']...)\n",
      "Dictionary(68 unique tokens: ['.', 'adorable', 'and', 'being', 'exhausting']...)\n",
      "Dictionary(459 unique tokens: ['.', 'a', 'after', 'and', 'are']...)\n",
      "Dictionary(129 unique tokens: [\"'s\", ',', '.', '?', 'a']...)\n",
      "Dictionary(542 unique tokens: [',', '--', '.', 'a', 'bay']...)\n",
      "Dictionary(242 unique tokens: [',', '.', 'being', 'breast', 'cancer']...)\n",
      "Dictionary(114 unique tokens: [\"'s\", '.', 'been', 'cues', 'daughters']...)\n",
      "Dictionary(141 unique tokens: ['country', 'first', 'for', 'goals', 'his']...)\n",
      "Dictionary(297 unique tokens: ['-', '.', 'a', 'bad', 'behaviour']...)\n",
      "Dictionary(172 unique tokens: [\"'\", \"'structural\", 'about', 'concerned', 'creating']...)\n",
      "Dictionary(160 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(499 unique tokens: [',', '.', 'a', 'accused', 'again']...)\n",
      "Dictionary(406 unique tokens: [',', '.', 'a', 'and', 'be']...)\n",
      "Dictionary(15 unique tokens: ['and', 'cold', 'drafty…', 'gets', 'i']...)\n",
      "Dictionary(134 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(9 unique tokens: ['#', '.', 'squadgoals', 'caters', ',']...)\n",
      "Dictionary(241 unique tokens: [\"''\", '.', '``', 'a', 'against']...)\n",
      "Dictionary(292 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(116 unique tokens: ['.', '18', 'a', 'after', 'coast']...)\n",
      "Dictionary(105 unique tokens: ['--', '.', 'a', 'according', 'anyway']...)\n",
      "Dictionary(400 unique tokens: [',', '.', '40', 'across', 'adrenaline']...)\n",
      "Dictionary(272 unique tokens: ['also', 'and', 'banks', 'branches', 'calls']...)\n",
      "Dictionary(101 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(108 unique tokens: ['.', ':', 'a', 'arriving', 'at']...)\n",
      "Dictionary(146 unique tokens: ['.', ':', 'a', 'avoid', 'don']...)\n",
      "Dictionary(545 unique tokens: [',', '.', 'a', 'build', 'coup']...)\n",
      "Dictionary(258 unique tokens: ['--', '.', '22-year-old', 'a', 'being']...)\n",
      "Dictionary(182 unique tokens: ['.', '12', 'a', 'after', 'arrested']...)\n",
      "Dictionary(118 unique tokens: ['.', 'a', 'angeles', 'angels', 'big']...)\n",
      "Dictionary(298 unique tokens: [',', '10', 'be', 'blamed', 'buffalo']...)\n",
      "Dictionary(52 unique tokens: ['!', \"'s\", ',', '.', '2015']...)\n",
      "Dictionary(26 unique tokens: ['(', ')', '.', 'catcher', 'crosby']...)\n",
      "Dictionary(461 unique tokens: ['.', 'a', 'and', 'apiece', 'are']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(140 unique tokens: ['.', '37', 'american', 'became', 'crown']...)\n",
      "Dictionary(166 unique tokens: [',', '.', 'and', 'by', 'crossing']...)\n",
      "Dictionary(68 unique tokens: ['.', 'business', 'contest', 'ever', 'first']...)\n",
      "Dictionary(269 unique tokens: ['.', '10', '21', 'african', 'august']...)\n",
      "Dictionary(400 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(430 unique tokens: [\"'s\", \"'ve\", ',', '.', 'a']...)\n",
      "Dictionary(250 unique tokens: [',', '.', '115,000', '1975', '2004']...)\n",
      "Dictionary(709 unique tokens: [',', '.', 'a', 'admission', 'and']...)\n",
      "Dictionary(99 unique tokens: [\"'s\", ',', '.', '...', 'another']...)\n",
      "Dictionary(480 unique tokens: [\"'s\", '.', 'a', 'as', 'here']...)\n",
      "Dictionary(36 unique tokens: ['!', ',', 'canines', 'coffee', 'love']...)\n",
      "Dictionary(354 unique tokens: [',', '.', 'abortion', 'abortions', 'all']...)\n",
      "Dictionary(115 unique tokens: ['(', ')', ',', '-', '.']...)\n",
      "Dictionary(389 unique tokens: [\"'re\", \"'s\", ',', '.', '...']...)\n",
      "Dictionary(465 unique tokens: [',', '.', '400', 'a', 'after']...)\n",
      "Dictionary(171 unique tokens: [',', 'and', 'are', 'as', 'big']...)\n",
      "Dictionary(205 unique tokens: [\"'\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(332 unique tokens: [',', '.', 'a', 'according', 'and']...)\n",
      "Dictionary(299 unique tokens: ['>', \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(312 unique tokens: [',', '.', 'a', 'about', 'american']...)\n",
      "Dictionary(352 unique tokens: ['.', 'a', 'abdel-fattah', 'after', 'away']...)\n",
      "Dictionary(131 unique tokens: [',', '.', '16', 'aduba', 'and']...)\n",
      "Dictionary(370 unique tokens: [',', '.', ':', 'a', 'an']...)\n",
      "Dictionary(45 unique tokens: ['.', 'a', 'master', 'thespian', 'true']...)\n",
      "Dictionary(416 unique tokens: [',', '.', 'a', 'blew', 'bomber']...)\n",
      "Dictionary(279 unique tokens: [',', '.', 'a', 'according', 'aiding']...)\n",
      "Dictionary(238 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(391 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(322 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(244 unique tokens: [',', '.', 'a', 'ceremony', 'colombian']...)\n",
      "Dictionary(358 unique tokens: [\"'s\", ',', '.', 'a', 'agenda']...)\n",
      "Dictionary(245 unique tokens: [',', '.', 'and', 'be', 'being']...)\n",
      "Dictionary(231 unique tokens: [\"'\", \"'doreen\", \"'re\", 'and', 'can']...)\n",
      "Dictionary(717 unique tokens: [\"'s\", ',', '.', 'advance', 'agreement']...)\n",
      "Dictionary(8 unique tokens: ['.', 'and', 'pose', ',', '2017']...)\n",
      "Dictionary(629 unique tokens: [',', '.', 'alerts', 'all', 'american']...)\n",
      "Dictionary(256 unique tokens: [\"'s\", ',', '.', '12:57', '13']...)\n",
      "Dictionary(266 unique tokens: ['.', 'a', 'after', 'against', 'day']...)\n",
      "Dictionary(207 unique tokens: ['.', '12', '7', 'a', 'announced']...)\n",
      "Dictionary(672 unique tokens: [',', '.', '1984', 'and', 'around']...)\n",
      "Dictionary(321 unique tokens: [',', '.', 'a', 'all', 'at']...)\n",
      "Dictionary(588 unique tokens: [',', '.', '1969', '80', 'a']...)\n",
      "Dictionary(115 unique tokens: [',', '.', '10', ':', 'an']...)\n",
      "Dictionary(211 unique tokens: [\"'\", \"''\", \"'s\", ',', '--']...)\n",
      "Dictionary(441 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(445 unique tokens: [',', '.', '24/7', ':', 'a']...)\n",
      "Dictionary(188 unique tokens: ['.', 'a', 'admitted', 'against', 'and']...)\n",
      "Dictionary(316 unique tokens: [\"'s\", ',', '--', '.', 'alien-like']...)\n",
      "Dictionary(180 unique tokens: [\"'\", \"'nail\", '.', 'and', 'be']...)\n",
      "Dictionary(224 unique tokens: ['by', 'cornwell', 'susan', '(', ')']...)\n",
      "Dictionary(345 unique tokens: ['after', 'back', 'call', 'comes', 'divorce']...)\n",
      "Dictionary(225 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(374 unique tokens: ['.', 'a', 'about', 'like', 'list']...)\n",
      "Dictionary(135 unique tokens: ['$', \"'s\", '.', '100,000', 'and']...)\n",
      "Dictionary(200 unique tokens: [',', 'all', 'art', 'battel', 'courtesy']...)\n",
      "Dictionary(95 unique tokens: [\"'s\", ',', '.', '10', 'again']...)\n",
      "Dictionary(624 unique tokens: [',', '.', '2016', 'a', 'accept']...)\n",
      "Dictionary(195 unique tokens: [',', '.', 'a', 'and', 'assailants']...)\n",
      "Dictionary(326 unique tokens: [',', '.', 'an', 'but', 'cup']...)\n",
      "Dictionary(186 unique tokens: [',', '.', 'a', 'after', 'allegedly']...)\n",
      "Dictionary(533 unique tokens: [',', '.', 'albany', 'all', 'and']...)\n",
      "Dictionary(540 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(479 unique tokens: [',', '.', '45', ':', 'adjective']...)\n",
      "Dictionary(71 unique tokens: ['12', '2016', 'apr', \"'s\", ',']...)\n",
      "Dictionary(72 unique tokens: [',', '.', ':', 'a', 'an']...)\n",
      "Dictionary(364 unique tokens: [',', '.', ':', 'all', 'and']...)\n",
      "Dictionary(96 unique tokens: ['.', 'a', 'actions', 'and', 'at']...)\n",
      "Dictionary(244 unique tokens: [\"'s\", '.', 'and', 'big', 'black']...)\n",
      "Dictionary(340 unique tokens: [',', '.', 'a', 'academics', 'analysis']...)\n",
      "Dictionary(172 unique tokens: [\"'s\", '.', 'a', 'after', 'an']...)\n",
      "Dictionary(113 unique tokens: ['but', 'certainty', 'company', 'deal', 'dispose']...)\n",
      "Dictionary(368 unique tokens: ['.', 'a', 'around', 'bringing', 'from']...)\n",
      "Dictionary(309 unique tokens: [',', '.', 'a', 'an', 'attempting']...)\n",
      "Dictionary(40 unique tokens: ['.', 'bel-air', 'down', 'flip-turned', 'fresh']...)\n",
      "Dictionary(507 unique tokens: [',', '.', ':', 'aired', 'finale']...)\n",
      "Dictionary(278 unique tokens: [',', '.', 'applying', 'are', 'authorities']...)\n",
      "Dictionary(184 unique tokens: ['by', 'foley', 'kaye', ',', '.']...)\n",
      "Dictionary(663 unique tokens: [':', 'and', 'andy', 'by', 'dan']...)\n",
      "Dictionary(107 unique tokens: ['.', 'a', 'actually', 'better', 'can']...)\n",
      "Dictionary(457 unique tokens: ['.', 'a', 'alpine', 'an', 'and']...)\n",
      "Dictionary(24 unique tokens: [',', '.', '11', 'alani', 'among']...)\n",
      "Dictionary(246 unique tokens: ['.', 'be', 'bliss', 'can', 'ignorance']...)\n",
      "Dictionary(186 unique tokens: [\"'\", '.', 'around', 'australian', 'cruisin']...)\n",
      "Dictionary(338 unique tokens: ['by', 'mcnulty', 'phil', 'chief', 'football']...)\n",
      "Dictionary(185 unique tokens: [',', '.', 'acting', 'also', 'an']...)\n",
      "Dictionary(532 unique tokens: [',', 'and', 'business', 'but', 'calls']...)\n",
      "Dictionary(278 unique tokens: [\"'\", \"'s\", ',', '.', 'a']...)\n",
      "Dictionary(470 unique tokens: ['accessories', 'and', 'barnett', 'bold', 'british']...)\n",
      "Dictionary(38 unique tokens: [',', '.', '1.5-year-old', 'a', 'air']...)\n",
      "Dictionary(117 unique tokens: ['?', 'and', 'bread', 'cut', 'has']...)\n",
      "Dictionary(50 unique tokens: ['$', '.', '1.60', 'a', 'about']...)\n",
      "Dictionary(214 unique tokens: [',', '.', 'and', 'at', 'car']...)\n",
      "Dictionary(246 unique tokens: [\"'s\", \"'ve\", ',', '.', 'it']...)\n",
      "Dictionary(532 unique tokens: [',', '.', 'a', 'ant', 'as']...)\n",
      "Dictionary(321 unique tokens: [',', '.', '30', 'a', 'advocates']...)\n",
      "Dictionary(38 unique tokens: [',', '.', 'a', 'all', 'at']...)\n",
      "Dictionary(283 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(385 unique tokens: ['.', '12,000', 'don', 'have', 'island']...)\n",
      "Dictionary(105 unique tokens: [\"'s\", '.', 'a', 'ballgame', 'brawl']...)\n",
      "Dictionary(185 unique tokens: ['.', 'a', 'amazing', 'brain', 'brazil']...)\n",
      "Dictionary(215 unique tokens: [\"'s\", '--', '.', 'a', 'age']...)\n",
      "Dictionary(385 unique tokens: [',', '.', '22', '30', 'adult']...)\n",
      "Dictionary(264 unique tokens: [\"'s\", '.', '3', '3-2', 'and']...)\n",
      "Dictionary(225 unique tokens: [\"''\", ',', '.', '``', 'a']...)\n",
      "Dictionary(31 unique tokens: [\"'s\", ',', '.', 'a', 'became']...)\n",
      "Dictionary(371 unique tokens: ['.', '/', '4c', 'alfa', 'an']...)\n",
      "Dictionary(496 unique tokens: ['a', 'as', 'believe', 'cache', 'couldn']...)\n",
      "Dictionary(552 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(358 unique tokens: [',', '.', 'a', 'across', 'allegedly']...)\n",
      "Dictionary(188 unique tokens: [',', '2-year-old', '4-year-old', ':', 'care']...)\n",
      "Dictionary(203 unique tokens: ['$', \"'s\", ',', '.', '420,000']...)\n",
      "Dictionary(60 unique tokens: [',', '.', 'a', 'ago', 'airbus']...)\n",
      "Dictionary(354 unique tokens: [',', '.', 'a', 'accounts', 'after']...)\n",
      "Dictionary(267 unique tokens: [\"'\", \"'sleeping\", '.', '2,000-year-old', 'an']...)\n",
      "Dictionary(654 unique tokens: ['.', 'a', 'acceptance', 'advocates', 'americans']...)\n",
      "Dictionary(322 unique tokens: [',', '.', 'around', 'california', 'cool']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'also', 'and', 'co.']...)\n",
      "Dictionary(25 unique tokens: [',', '.', 'and', 'between', 'college-']...)\n",
      "Dictionary(97 unique tokens: ['.', 'gopro', 'greatness', 'is', 'pure']...)\n",
      "Dictionary(244 unique tokens: ['.', 'a', 'came', 'championship', 'home']...)\n",
      "Dictionary(230 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(128 unique tokens: ['?', 'discovered', 'have', 'how', 'many']...)\n",
      "Dictionary(358 unique tokens: [\"'s\", ',', '.', '76-year-old', ':']...)\n",
      "Dictionary(165 unique tokens: ['.', ':', 'a', 'be', 'cold']...)\n",
      "Dictionary(134 unique tokens: [',', '.', 'a', 'about', 'also']...)\n",
      "Dictionary(297 unique tokens: [',', '.', 'a', 'abc', 'after']...)\n",
      "Dictionary(327 unique tokens: [',', '.', '2012', '30-year-old', '5-year-old']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(341 unique tokens: [',', '.', 'a', 'and', 'apartments']...)\n",
      "Dictionary(256 unique tokens: [\"'s\", ',', '.', '12:57', '13']...)\n",
      "Dictionary(212 unique tokens: [',', '.', ':', 'a', 'according']...)\n",
      "Dictionary(81 unique tokens: [\"''\", '.', '``', 'after', 'are']...)\n",
      "Dictionary(567 unique tokens: [',', '.', '14', '2011', ':']...)\n",
      "Dictionary(315 unique tokens: [\"'s\", ',', '.', '1', 'and']...)\n",
      "Dictionary(123 unique tokens: ['$', '(', ')', '-', '.']...)\n",
      "Dictionary(218 unique tokens: ['(', ')', ',', '-', '.']...)\n",
      "Dictionary(281 unique tokens: ['andrew', 'by', 'm.', 'seaman', '(']...)\n",
      "Dictionary(3 unique tokens: ['associated', 'by', 'press'])\n",
      "Dictionary(49 unique tokens: [\"''\", \"'s\", '.', '``', 'a']...)\n",
      "Dictionary(698 unique tokens: [',', '.', 'a', 'and', 'at']...)\n",
      "Dictionary(165 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(387 unique tokens: ['.', 'a', 'abused', 'allegations', 'boy']...)\n",
      "Dictionary(360 unique tokens: [',', '.', 'a', 'announced', 'become']...)\n",
      "Dictionary(316 unique tokens: [\"'\", ',', '.', '88', 'a']...)\n",
      "Dictionary(70 unique tokens: [',', '.', 'a', 'an', 'answer']...)\n",
      "Dictionary(230 unique tokens: ['--', '.', '?', 'a', 'answer']...)\n",
      "Dictionary(90 unique tokens: ['.', 'a', 'and', 'back', 'because']...)\n",
      "Dictionary(59 unique tokens: ['?', 'a', 'as', 'can', 'future']...)\n",
      "Dictionary(224 unique tokens: ['-', '.', '15th', '1949', '86-year-old']...)\n",
      "Dictionary(254 unique tokens: [\"'s\", ',', '.', 'a', 'accidentally']...)\n",
      "Dictionary(92 unique tokens: ['.', '2017', '3rd', 'app', 'april']...)\n",
      "Dictionary(402 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(130 unique tokens: ['.', 'after', 'allegedly', 'and', 'both']...)\n",
      "Dictionary(145 unique tokens: ['.', 'a', 'admitted', 'as', 'barack']...)\n",
      "Dictionary(167 unique tokens: ['and', 'between', 'constant', 'incident', 'intimidation']...)\n",
      "Dictionary(251 unique tokens: ['a', 'ago', 'at', 'dip', 'existence']...)\n",
      "Dictionary(134 unique tokens: [\"'s\", ',', '.', 'bit', 'for']...)\n",
      "Dictionary(134 unique tokens: ['.', 'a', 'an', 'elderly', 'gone']...)\n",
      "Dictionary(283 unique tokens: [\"'ve\", '(', ')', ',', '.']...)\n",
      "Dictionary(45 unique tokens: [',', '.', 'a', 'abortion', 'ambulatory']...)\n",
      "Dictionary(611 unique tokens: [',', '.', 'ago', 'almost', 'and']...)\n",
      "Dictionary(353 unique tokens: [\"'s\", ',', '.', '5', 'a']...)\n",
      "Dictionary(352 unique tokens: [',', '.', '1500', 'a', 'accident']...)\n",
      "Dictionary(127 unique tokens: ['.', 'abilities', 'brought', 'duke', 'his']...)\n",
      "Dictionary(332 unique tokens: [\"''\", '--', '.', '``', 'a']...)\n",
      "Dictionary(185 unique tokens: [',', '.', 'acting', 'also', 'an']...)\n",
      "Dictionary(79 unique tokens: [',', '.', '18', '1815', '200']...)\n",
      "Dictionary(569 unique tokens: [\"''\", ',', '.', '...', ':']...)\n",
      "Dictionary(153 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(285 unique tokens: [\"''\", \"'s\", '(', ')', '.']...)\n",
      "Dictionary(247 unique tokens: ['(', ')', '.', '13', ':']...)\n",
      "Dictionary(277 unique tokens: [',', '.', 'and', 'anti-harassment', 'ceo']...)\n",
      "Dictionary(128 unique tokens: ['.', '2017', '3rd', 'app', 'april']...)\n",
      "Dictionary(347 unique tokens: [\"'s\", '.', 'a', 'anâ', 'barack']...)\n",
      "Dictionary(648 unique tokens: [',', '.', 'a', 'and', 'couples']...)\n",
      "Dictionary(184 unique tokens: [',', 'after', 'bandaged', 'completing', 'fighters']...)\n",
      "Dictionary(189 unique tokens: ['%', ',', '.', '2015', '3.9']...)\n",
      "Dictionary(465 unique tokens: [',', '.', '38', ':', 'admire']...)\n",
      "Dictionary(409 unique tokens: [',', '.', 'a', 'about', 'alberto']...)\n",
      "Dictionary(283 unique tokens: [',', '.', 'ampleforth', 'and', 'college']...)\n",
      "Dictionary(158 unique tokens: [\"''\", '.', '``', 'a', 'do']...)\n",
      "Dictionary(54 unique tokens: [',', '.', 'accelerate', 'action', 'against']...)\n",
      "Dictionary(331 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(102 unique tokens: ['.', 'all-rounder', 'as', 'called', 'chris']...)\n",
      "Dictionary(372 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(82 unique tokens: [',', '.', '1', '12th', 'american']...)\n",
      "Dictionary(474 unique tokens: ['.', 'a', 'after', 'as', 'back']...)\n",
      "Dictionary(103 unique tokens: [',', '.', 'a', 'acted', 'american']...)\n",
      "Dictionary(262 unique tokens: [\"'\", \"'gascoigne\", 'and', 'back', 'career']...)\n",
      "Dictionary(402 unique tokens: ['!', \"'s\", '.', '22', 'appears']...)\n",
      "Dictionary(143 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(555 unique tokens: [\"'s\", ',', '.', 'a', 'again']...)\n",
      "Dictionary(256 unique tokens: [\"'s\", ',', '.', '12:57', '13']...)\n",
      "Dictionary(635 unique tokens: [\"'s\", ',', '.', 'abroad', 'behaviour']...)\n",
      "Dictionary(1005 unique tokens: [',', '.', 'afternoon', 'and', 'bursts']...)\n",
      "Dictionary(242 unique tokens: [',', 'as', 'bishop', 'campaign', 'campus']...)\n",
      "Dictionary(141 unique tokens: [',', '.', 'a', 'breaking', 'britton']...)\n",
      "Dictionary(257 unique tokens: [',', '.', '21st', '84-year-old', 'according']...)\n",
      "Dictionary(71 unique tokens: ['$', '.', '100', '2014', '46']...)\n",
      "Dictionary(417 unique tokens: [\"'ve\", ',', '.', 'a', 'able']...)\n",
      "Dictionary(235 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(420 unique tokens: ['(', ')', '.', '3rd', ':']...)\n",
      "Dictionary(342 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(456 unique tokens: ['.', ':', 'accomplished', 'are', 'bush']...)\n",
      "Dictionary(352 unique tokens: [\"'s\", ',', '.', '22', 'and']...)\n",
      "Dictionary(66 unique tokens: [',', '.', 'buy', 'cat', 'do']...)\n",
      "Dictionary(273 unique tokens: ['.', 'a', 'an', 'appellate', 'case']...)\n",
      "Dictionary(408 unique tokens: [',', '.', '27', '42', '5am']...)\n",
      "Dictionary(582 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(479 unique tokens: [',', '.', '12-year-old', 'a', 'against']...)\n",
      "Dictionary(163 unique tokens: [',', '.', '12', ':', 'effect']...)\n",
      "Dictionary(171 unique tokens: [\"'s\", '.', 'comedy', 'comes', 'day']...)\n",
      "Dictionary(1005 unique tokens: [',', '.', 'afternoon', 'and', 'bursts']...)\n",
      "Dictionary(417 unique tokens: [\"'s\", '.', 'a', 'after', 'become']...)\n",
      "Dictionary(474 unique tokens: [\"'s\", ',', '.', 'a', 'act']...)\n",
      "Dictionary(481 unique tokens: [',', 'a', 'as', 'chosen', 'come']...)\n",
      "Dictionary(483 unique tokens: [',', '.', 'a', 'all', 'and']...)\n",
      "Dictionary(82 unique tokens: [',', '.', 'a', 'adorable', 'an']...)\n",
      "Dictionary(270 unique tokens: [\"'s\", '.', '10', 'a', 'after']...)\n",
      "Dictionary(593 unique tokens: [',', '.', ':', 'a', 'a.']...)\n",
      "Dictionary(163 unique tokens: ['.', 'a', 'and', 'biryani', 'cemented']...)\n",
      "Dictionary(319 unique tokens: ['more', 'brian', 'by', 'prowse-gany', ',']...)\n",
      "Dictionary(226 unique tokens: [',', '.', 'abroad', 'and', 'another']...)\n",
      "Dictionary(133 unique tokens: [',', '.', '?', 'a', 'before']...)\n",
      "Dictionary(705 unique tokens: ['(', ')', ',', '.', '2']...)\n",
      "Dictionary(340 unique tokens: ['.', '70-year-old', 'a', 'after', 'against']...)\n",
      "Dictionary(379 unique tokens: ['.', 'a', 'animals', 'coast', 'during']...)\n",
      "Dictionary(655 unique tokens: ['.', '@', 'about', 'been', 'consistent']...)\n",
      "Dictionary(456 unique tokens: ['.', 'administered', 'after', 'an', 'and']...)\n",
      "Dictionary(506 unique tokens: [',', '.', 'a', 'accused', 'and']...)\n",
      "Dictionary(551 unique tokens: [',', '.', 'a', 'and', 'anita']...)\n",
      "Dictionary(40 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(412 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(31 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(97 unique tokens: [',', '.', '59', 'a', 'after']...)\n",
      "Dictionary(755 unique tokens: [',', '2017', '26', '9:36', 'apr']...)\n",
      "Dictionary(416 unique tokens: [\"'s\", ',', '.', 'a', 'after']...)\n",
      "Dictionary(207 unique tokens: ['$', \"'\", \"''\", \"'a\", \"'defeat\"]...)\n",
      "Dictionary(151 unique tokens: [',', '.', '``', 'a', 'are']...)\n",
      "Dictionary(298 unique tokens: [',', '100,000', 'a', 'acid', 'bleeding']...)\n",
      "Dictionary(192 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(208 unique tokens: ['airways', 'cathay', 'pacific', \"'re\", '(']...)\n",
      "Dictionary(321 unique tokens: ['advertisement', ',', '2017', '26', 'april']...)\n",
      "Dictionary(380 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(47 unique tokens: ['2017', '26', 'apr', ',', '-']...)\n",
      "Dictionary(267 unique tokens: [\"'re\", \"'s\", ',', '.', '100']...)\n",
      "Dictionary(244 unique tokens: [\"''\", '.', '``', 'against', 'all-out']...)\n",
      "Dictionary(218 unique tokens: ['$', '(', ')', '.', '14']...)\n",
      "Dictionary(118 unique tokens: ['(', ')', ',', '.', 'and']...)\n",
      "Dictionary(569 unique tokens: ['more', ',', '.', 'a', 'absurd']...)\n",
      "Dictionary(50 unique tokens: [',', '.', '2012', 'a', 'accelerating']...)\n",
      "Dictionary(97 unique tokens: ['.', 'a', 'after', 'airlines', 'an']...)\n",
      "Dictionary(229 unique tokens: ['accused', 'agency', 'australian', 'authorities', 'bolster']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(173 unique tokens: [',', '.', 'aadhaar-like', 'amid', 'ani']...)\n",
      "Dictionary(182 unique tokens: [',', '.', 'a', 'about', 'after']...)\n",
      "Dictionary(65 unique tokens: [',', '.', 'a', 'announced', 'attending']...)\n",
      "Dictionary(413 unique tokens: [',', '.', 'about', 'better', 'come']...)\n",
      "Dictionary(71 unique tokens: [',', '.', 'addicts', 'burden', 'centers']...)\n",
      "Dictionary(105 unique tokens: ['(', ')', ',', '.', '16']...)\n",
      "Dictionary(253 unique tokens: [',', '--', '.', 'common', 'department']...)\n",
      "Dictionary(211 unique tokens: [',', 'business', 'dennis', 'green', 'insider']...)\n",
      "Dictionary(275 unique tokens: [',', '.', 'a', 'about', 'all']...)\n",
      "Dictionary(193 unique tokens: [',', '?', 'a', 'bit', 'ca']...)\n",
      "Dictionary(391 unique tokens: ['$', '(', ')', ',', '--']...)\n",
      "Dictionary(68 unique tokens: [',', '.', 'a', 'after', 'amid']...)\n",
      "Dictionary(394 unique tokens: [',', '.', 'a', 'app', 'bathroom']...)\n",
      "Dictionary(494 unique tokens: [',', '.', '40', 'a', 'african']...)\n",
      "Dictionary(192 unique tokens: ['.', 'a', 'adventure', 'ago', 'englishman']...)\n",
      "Dictionary(239 unique tokens: ['!', 'success', '.', 'been', 'box']...)\n",
      "Dictionary(193 unique tokens: [',', '--', '.', 'a', 'after']...)\n",
      "Dictionary(65 unique tokens: [',', '.', 'a', 'aaron', 'according']...)\n",
      "Dictionary(59 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(308 unique tokens: ['.', 'a', 'an', 'cost', 'failure']...)\n",
      "Dictionary(657 unique tokens: ['more', ',', '.', '21', 'a']...)\n",
      "Dictionary(257 unique tokens: [',', '.', '64', 'and', 'are']...)\n",
      "Dictionary(567 unique tokens: [',', '.', '100', 'a', 'advisers']...)\n",
      "Dictionary(180 unique tokens: [',', '.', 'and', 'been', 'clarke']...)\n",
      "Dictionary(66 unique tokens: [',', '.', '2,174', '3,200', '83']...)\n",
      "Dictionary(312 unique tokens: [',', '.', '1973', 'a', 'along']...)\n",
      "Dictionary(312 unique tokens: [',', '.', '23-year-old', 'a', 'action']...)\n",
      "Dictionary(303 unique tokens: [\"'s\", ',', '.', 'a', 'but']...)\n",
      "Dictionary(385 unique tokens: ['!', \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(179 unique tokens: [':', '?', 'are', 'body', 'does']...)\n",
      "Dictionary(87 unique tokens: [',', 'business', 'insider', 'steven', 'tweedie']...)\n",
      "Dictionary(548 unique tokens: [',', '.', 'a', 'able', 'at']...)\n",
      "Dictionary(756 unique tokens: [\"'s\", ',', '.', 'a', 'at']...)\n",
      "Dictionary(567 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(370 unique tokens: ['.', '8', 'a', 'election', 'general']...)\n",
      "Dictionary(204 unique tokens: [',', '.', 'and', 'as', 'days']...)\n",
      "Dictionary(75 unique tokens: [',', 'business', 'gillett', 'insider', 'rachel']...)\n",
      "Dictionary(149 unique tokens: [',', '.', 'a', 'around', 'away']...)\n",
      "Dictionary(322 unique tokens: [\"'s\", ',', '.', '13,000', 'a']...)\n",
      "Dictionary(177 unique tokens: [',', '.', 'and', 'barack', 'bill']...)\n",
      "Dictionary(284 unique tokens: ['(', ')', ',', '.', 'an']...)\n",
      "Dictionary(1058 unique tokens: [\"''\", \"'s\", ',', '.', '23']...)\n",
      "Dictionary(85 unique tokens: ['.', 'a', 'air', 'all', 'and']...)\n",
      "Dictionary(189 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(249 unique tokens: [',', '.', 'and', 'atlanta', 'between']...)\n",
      "Dictionary(457 unique tokens: [',', '.', '12', '1997', '31']...)\n",
      "Dictionary(266 unique tokens: ['(', ')', '.', ':', 'ahead']...)\n",
      "Dictionary(422 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(253 unique tokens: ['.', 'a', 'administration', 'already', 'and']...)\n",
      "Dictionary(244 unique tokens: [',', '.', 'a', 'after', 'base']...)\n",
      "Dictionary(499 unique tokens: [',', '.', 'a', 'an', 'awarded']...)\n",
      "Dictionary(438 unique tokens: [',', '.', 'andes', 'are', 'argentina']...)\n",
      "Dictionary(193 unique tokens: [',', '.', 'a', 'all', 'better']...)\n",
      "Dictionary(250 unique tokens: [',', '.', 'a', 'all', 'allah']...)\n",
      "Dictionary(130 unique tokens: [',', '.', '60,000', 'a', 'all']...)\n",
      "Dictionary(183 unique tokens: ['25', 'days', 'girl', 'in', 'little']...)\n",
      "Dictionary(219 unique tokens: ['afp', ',', '.', '16', '2017']...)\n",
      "Dictionary(416 unique tokens: [\"''\", \"'s\", ',', '.', '1991']...)\n",
      "Dictionary(439 unique tokens: [',', '.', 'a', 'americans', 'away']...)\n",
      "Dictionary(213 unique tokens: ['sections', 'stories', 'top', 'watch', '.']...)\n",
      "Dictionary(1377 unique tokens: [\"'s\", '.', '?', 'and', 'california']...)\n",
      "Dictionary(335 unique tokens: [',', '.', '16', 'a', 'after']...)\n",
      "Dictionary(235 unique tokens: ['.', 'a', 'are', 'big', 'boogers']...)\n",
      "Dictionary(132 unique tokens: [',', 'acuna', 'business', 'carrie', 'guerrasio']...)\n",
      "Dictionary(183 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(20 unique tokens: [':', 'now', 'playing', 'mainvideo.hed', '{']...)\n",
      "Dictionary(434 unique tokens: [',', '.', '40', ':', '?']...)\n",
      "Dictionary(532 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(802 unique tokens: [',', '.', 'a', 'amount', 'and']...)\n",
      "Dictionary(567 unique tokens: ['more', '.', 'a', 'as', 'could']...)\n",
      "Dictionary(276 unique tokens: ['.', 'a', 'airport', 'claims', 'flight']...)\n",
      "Dictionary(390 unique tokens: [',', 'are', 'around', 'burning', 'celebrate']...)\n",
      "Dictionary(123 unique tokens: [',', '.', 'a', 'and', 'baltimore']...)\n",
      "Dictionary(40 unique tokens: ['.', '40-year-old', 'a', 'an', 'apartment']...)\n",
      "Dictionary(108 unique tokens: [\"'s\", '.', 'aboutâ', 'album', 'damn.â']...)\n",
      "Dictionary(195 unique tokens: [',', '.', 'a', 'about', 'after']...)\n",
      "Dictionary(202 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(297 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(304 unique tokens: [',', '.', 'a', 'are', 'bank']...)\n",
      "Dictionary(140 unique tokens: ['(', ')', '.', 'a', 'bill']...)\n",
      "Dictionary(67 unique tokens: [',', '.', '20', 'a', 'about']...)\n",
      "Dictionary(356 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(150 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(912 unique tokens: [\"'\", \"'100\", '.', 'a', 'al-assad']...)\n",
      "Dictionary(207 unique tokens: [',', '.', 'according', 'adrian', 'fired']...)\n",
      "Dictionary(523 unique tokens: [',', '.', 'any', 'beach', 'before']...)\n",
      "Dictionary(544 unique tokens: [\"''\", ',', '.', '1919', ':']...)\n",
      "Dictionary(230 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(324 unique tokens: [',', '.', 'a', 'aging', 'and']...)\n",
      "Dictionary(148 unique tokens: ['.', 'a', 'announcement', 'casting', 'change']...)\n",
      "Dictionary(464 unique tokens: [',', '.', 'a', 'and', 'being']...)\n",
      "Dictionary(373 unique tokens: [\"''\", '.', '``', 'a', 'after']...)\n",
      "Dictionary(209 unique tokens: ['.', '2017', 'and', 'bold', 'comes']...)\n",
      "Dictionary(332 unique tokens: [':', 'acker/bloomberg', 'daniel', 'photographer', ',']...)\n",
      "Dictionary(374 unique tokens: [\"''\", \"'s\", ',', ':', '?']...)\n",
      "Dictionary(105 unique tokens: [',', 'antonio', 'business', 'insider', 'villas-boas']...)\n",
      "Dictionary(213 unique tokens: [',', '.', 'a', 'after', 'ajit']...)\n",
      "Dictionary(331 unique tokens: ['.', '//www.facebook.com/nbcnews', '//www.snapchat.com/add/nbcnews', '75', ':']...)\n",
      "Dictionary(526 unique tokens: ['$', \"'\", \"'s\", ',', '.']...)\n",
      "Dictionary(71 unique tokens: ['david', 'ibekwe', ',', '.', '12']...)\n",
      "Dictionary(166 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(661 unique tokens: [',', '.', 'a', 'after', 'airline']...)\n",
      "Dictionary(213 unique tokens: [',', '.', '2016', 'a', 'american']...)\n",
      "Dictionary(218 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(65 unique tokens: ['.', 'adolf', 'after', 'been', 'chemical']...)\n",
      "Dictionary(150 unique tokens: [',', 'business', 'garber', 'insider', 'jonathan']...)\n",
      "Dictionary(526 unique tokens: [',', '.', 'a', 'according', 'china']...)\n",
      "Dictionary(166 unique tokens: [\"''\", '.', '``', 'a', 'actor']...)\n",
      "Dictionary(416 unique tokens: ['already', 'and', 'as', 'bills', 'brexit-effect']...)\n",
      "Dictionary(307 unique tokens: [',', '10', '11:03', '2017', 'apr']...)\n",
      "Dictionary(624 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(96 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(130 unique tokens: [\"'s\", ',', '.', '30', 'a']...)\n",
      "Dictionary(343 unique tokens: ['about', 'admission', 'behind', 'defence', 'donald']...)\n",
      "Dictionary(74 unique tokens: [',', 'business', 'cork', 'gaines', 'insider']...)\n",
      "Dictionary(64 unique tokens: [',', '--', '.', 'an', 'apparent']...)\n",
      "Dictionary(211 unique tokens: [',', '-', '.', 'a', 'affair']...)\n",
      "Dictionary(281 unique tokens: [',', '.', '1', 'a', 'about']...)\n",
      "Dictionary(64 unique tokens: [',', '.', 'a', 'air', 'and']...)\n",
      "Dictionary(260 unique tokens: [',', '.', 'a', 'about', 'ammon']...)\n",
      "Dictionary(221 unique tokens: [\"'s\", ',', '.', 'and', 'anxious']...)\n",
      "Dictionary(205 unique tokens: [',', '.', '50,000', 'a', 'annual']...)\n",
      "Dictionary(347 unique tokens: ['more', ',', '.', '30', '50']...)\n",
      "Dictionary(217 unique tokens: [',', '.', 'agreed', 'airstrikes', 'and']...)\n",
      "Dictionary(369 unique tokens: ['.', '11-year-old', 'a', 'after', 'an']...)\n",
      "Dictionary(59 unique tokens: [',', '.', ':', 'a', 'air']...)\n",
      "Dictionary(293 unique tokens: ['.', 'a', 'attorney', 'baltimore', 'concerns']...)\n",
      "Dictionary(112 unique tokens: [',', '.', 'a', 'and', 'but']...)\n",
      "Dictionary(358 unique tokens: [',', '.', '2017', 'and', 'are']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(173 unique tokens: [',', '.', 'a', 'and', 'bear']...)\n",
      "Dictionary(160 unique tokens: [',', '.', 'a', 'after', 'an']...)\n",
      "Dictionary(585 unique tokens: ['more', ',', '.', 'about', 'apprehensive']...)\n",
      "Dictionary(546 unique tokens: [',', '.', 'a', 'against', 'allegations']...)\n",
      "Dictionary(401 unique tokens: [\"''\", ',', '-', '.', '22-year']...)\n",
      "Dictionary(378 unique tokens: [',', '.', 'a', 'alongside', 'an']...)\n",
      "Dictionary(126 unique tokens: ['.', 'a', 'accident', 'and', 'at']...)\n",
      "Dictionary(348 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(266 unique tokens: ['&', \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(540 unique tokens: ['.', '?', 'a', 'all', 'an']...)\n",
      "Dictionary(186 unique tokens: ['.', '30', 'a', 'before', 'call']...)\n",
      "Dictionary(270 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(229 unique tokens: ['.', 'a', 'against', 'airfield', 'at']...)\n",
      "Dictionary(313 unique tokens: [',', '.', 'a', 'airbase', 'an']...)\n",
      "Dictionary(510 unique tokens: [':', 'chris', 'cillizza', 'from', ',']...)\n",
      "Dictionary(164 unique tokens: ['.', 'a', 'adviser', 'after', 'andrea']...)\n",
      "Dictionary(503 unique tokens: ['.', 'across', 'baseball', 'begin', 'items']...)\n",
      "Dictionary(283 unique tokens: ['.', '10', '100', '1s', '3']...)\n",
      "Dictionary(267 unique tokens: ['$', ',', '110', '19', 'a']...)\n",
      "Dictionary(436 unique tokens: [\"'s\", ',', '.', 'a', 'around']...)\n",
      "Dictionary(439 unique tokens: [',', '.', 'a', 'adjunct', 'also']...)\n",
      "Dictionary(81 unique tokens: [',', 'business', 'cork', 'gaines', 'insider']...)\n",
      "Dictionary(143 unique tokens: ['.', 'a', 'advertisement', 'and', 'apologized']...)\n",
      "Dictionary(365 unique tokens: [',', '.', 'a', 'advertisement', 'after']...)\n",
      "Dictionary(43 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(169 unique tokens: [\"''\", \"'s\", ',', '--', '.']...)\n",
      "Dictionary(247 unique tokens: [\"'s\", ',', '.', '1948', '200']...)\n",
      "Dictionary(292 unique tokens: [',', '.', '2019', 'after', 'allowed']...)\n",
      "Dictionary(329 unique tokens: [',', '.', 'and', 'boost', 'brain']...)\n",
      "Dictionary(216 unique tokens: [',', '.', 'a', 'across', 'border']...)\n",
      "Dictionary(217 unique tokens: [',', '.', 'a', 'accredited', 'african']...)\n",
      "Dictionary(297 unique tokens: [\"'s\", '.', 'a', 'been', 'couple']...)\n",
      "Dictionary(190 unique tokens: ['#', ',', '.', 'about', 'advisor']...)\n",
      "Dictionary(121 unique tokens: [',', '.', 'a', 'according', 'after']...)\n",
      "Dictionary(345 unique tokens: [',', '--', '.', 'away', 'end']...)\n",
      "Dictionary(462 unique tokens: ['more', ',', '.', 'a', 'and']...)\n",
      "Dictionary(134 unique tokens: ['and-live', 'course', 'four', 'in-person', 'offers']...)\n",
      "Dictionary(233 unique tokens: ['and', 'assaults', 'baker', 'caroline', 'craigavon']...)\n",
      "Dictionary(177 unique tokens: ['.', '1.5', 'a', 'adobe', 'aims']...)\n",
      "Dictionary(230 unique tokens: [',', 'business', 'garfield', 'insider', 'leanna']...)\n",
      "Dictionary(935 unique tokens: ['-', '.', '14', '51', 'a']...)\n",
      "Dictionary(497 unique tokens: ['more', ',', '.', 'a', 'ballot']...)\n",
      "Dictionary(57 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(91 unique tokens: ['(', ')', 'bloomberg', 'casts', 'lots']...)\n",
      "Dictionary(191 unique tokens: ['&', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(213 unique tokens: [',', 'business', 'insider', 'sheth', 'sonam']...)\n",
      "Dictionary(502 unique tokens: [\"''\", \"'re\", \"'s\", ',', '.']...)\n",
      "Dictionary(429 unique tokens: [',', '.', 'a', 'animation', 'april']...)\n",
      "Dictionary(178 unique tokens: [\"'\", \"'s\", ',', '.', 'an']...)\n",
      "Dictionary(218 unique tokens: ['analysis', 'areas', 'be', 'deprived', 'formula']...)\n",
      "Dictionary(133 unique tokens: [',', '.', 'french', 'german', 'italian']...)\n",
      "Dictionary(468 unique tokens: ['(', ')', ',', '.', ':']...)\n",
      "Dictionary(361 unique tokens: ['more', '(', ')', ',', '.']...)\n",
      "Dictionary(226 unique tokens: [',', '.', 'a', 'activity', 'by']...)\n",
      "Dictionary(164 unique tokens: [\"'s\", ',', '.', '21-3', '28-3']...)\n",
      "Dictionary(71 unique tokens: ['%', ',', '.', '2', 'a']...)\n",
      "Dictionary(189 unique tokens: ['.', 'a', 'across', 'alleged', 'an']...)\n",
      "Dictionary(109 unique tokens: [',', '.', 'adviser', 'america', 'an']...)\n",
      "Dictionary(145 unique tokens: [',', '.', 'a', 'after', 'alabama']...)\n",
      "Dictionary(226 unique tokens: ['.', 'april', 'came', 'day', 'early']...)\n",
      "Dictionary(172 unique tokens: ['.', 'administration', 'and', 'appear', 'be']...)\n",
      "Dictionary(392 unique tokens: ['.', 'a', 'an', 'and', 'ban']...)\n",
      "Dictionary(420 unique tokens: ['.', '10', 'be', 'contact', 'for']...)\n",
      "Dictionary(523 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(90 unique tokens: ['(', ')', ',', '.', 'across']...)\n",
      "Dictionary(39 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(184 unique tokens: ['.', 'a', 'city', 'creates', 'devin']...)\n",
      "Dictionary(401 unique tokens: [\"'s\", '.', '2017', 'and', 'bad-mouthing']...)\n",
      "Dictionary(222 unique tokens: ['!', '#', '&', \"'\", \"''\"]...)\n",
      "Dictionary(457 unique tokens: [',', '.', '?', 'again', 'and']...)\n",
      "Dictionary(107 unique tokens: [',', '.', 'among', 'and', 'are']...)\n",
      "Dictionary(89 unique tokens: [',', '.', '...', '1995', '53']...)\n",
      "Dictionary(495 unique tokens: [',', '.', '28', 'a', 'against']...)\n",
      "Dictionary(136 unique tokens: ['.', 'a', 'absolutely', 'adviser', 'at']...)\n",
      "Dictionary(209 unique tokens: ['&', ',', '.', 'a', 'blondie']...)\n",
      "Dictionary(27 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(433 unique tokens: [',', '.', 'a', 'about', 'achievement']...)\n",
      "Dictionary(469 unique tokens: ['more', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(324 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(63 unique tokens: [\"'s\", '.', 'after', 'airport', 'an']...)\n",
      "Dictionary(283 unique tokens: [',', '.', 'and', 'at', 'chelsea']...)\n",
      "Dictionary(174 unique tokens: ['20', 'from', 'has', 'in', 'last']...)\n",
      "Dictionary(139 unique tokens: [',', '.', 'academy', 'accept', 'according']...)\n",
      "Dictionary(237 unique tokens: [',', 'business', 'insider', 'madeline', 'stone']...)\n",
      "Dictionary(84 unique tokens: [',', '.', '2019', '50', 'article']...)\n",
      "Dictionary(417 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(195 unique tokens: ['.', 'all', 'feel', 'kinds', 'nostalgic']...)\n",
      "Dictionary(125 unique tokens: ['.', 'after', 'and', 'attacked', 'breaking']...)\n",
      "Dictionary(261 unique tokens: [',', '.', '200', 'a', 'and']...)\n",
      "Dictionary(258 unique tokens: [\"'re\", '.', '12-sided', 'a', 'british']...)\n",
      "Dictionary(397 unique tokens: [\"'s\", '.', 'a', 'and', 'big-city']...)\n",
      "Dictionary(122 unique tokens: ['.', 'a', 'all', 'and', 'be']...)\n",
      "Dictionary(193 unique tokens: [',', '.', 'a', 'account', 'behind']...)\n",
      "Dictionary(183 unique tokens: ['!', 'success', '.', 'been', 'box']...)\n",
      "Dictionary(204 unique tokens: [',', '.', 'a', 'about', 'america']...)\n",
      "Dictionary(272 unique tokens: [',', '.', 'and', 'appear', 'at']...)\n",
      "Dictionary(101 unique tokens: [',', '.', 'a', 'about', 'and']...)\n",
      "Dictionary(206 unique tokens: ['barack', 'criticised', 'for', 'game', 'his']...)\n",
      "Dictionary(219 unique tokens: ['(', ')', '.', 'and', 'caucus']...)\n",
      "Dictionary(2229 unique tokens: [',', '?', 'after', 'can', 'capitol']...)\n",
      "Dictionary(510 unique tokens: ['--', '.', 'all', 'giggled', 'in']...)\n",
      "Dictionary(156 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(135 unique tokens: [',', '.', 'a', 'after', 'all']...)\n",
      "Dictionary(177 unique tokens: ['.', 'create', 'in', 'log', 'or']...)\n",
      "Dictionary(542 unique tokens: [',', '.', 'a', 'along', 'an']...)\n",
      "Dictionary(67 unique tokens: [',', '.', ':', 'a', 'and']...)\n",
      "Dictionary(338 unique tokens: [',', '.', 'against', 'allowing', 'and']...)\n",
      "Dictionary(258 unique tokens: ['.', 'a', 'and', 'been', 'chris']...)\n",
      "Dictionary(240 unique tokens: ['after', 'became', 'bill', 'clear', 'end']...)\n",
      "Dictionary(497 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(178 unique tokens: [',', '.', 'a', 'alive', 'and']...)\n",
      "Dictionary(122 unique tokens: [\"'\", \"'dirty\", 'broadcast', 'calls', 'mayor']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'a', 'appearing', 'at']...)\n",
      "Dictionary(178 unique tokens: [',', '.', 'a', 'after', 'bill']...)\n",
      "Dictionary(1262 unique tokens: ['.', 'a', 'be', 'biggest', 'canadians']...)\n",
      "Dictionary(141 unique tokens: ['.', 'a', 'about', 'diver', 'fearless']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(499 unique tokens: ['.', 'a', 'and', 'are', 'backing']...)\n",
      "Dictionary(564 unique tokens: [',', '.', 'a', 'against', 'allegations']...)\n",
      "Dictionary(324 unique tokens: [',', '.', 'any', 'away', 'been']...)\n",
      "Dictionary(819 unique tokens: ['accidentally', 'donated', 'dress', 'for', 'husband']...)\n",
      "Dictionary(118 unique tokens: [',', 'business', 'insider', 'jethro', 'nededog']...)\n",
      "Dictionary(144 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(249 unique tokens: ['.', 'a', 'and', 'are', 'be']...)\n",
      "Dictionary(1191 unique tokens: [\"''\", \"'s\", ',', '.', '?']...)\n",
      "Dictionary(73 unique tokens: [',', '.', 'enough', 'for', 'funds']...)\n",
      "Dictionary(237 unique tokens: ['#', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(155 unique tokens: [\"'s\", '.', 'a', 'back', 'been']...)\n",
      "Dictionary(431 unique tokens: [',', '.', ';', 'a', 'agrawal']...)\n",
      "Dictionary(412 unique tokens: [',', 'about', 'bartending', 'breaking', 'cleaning']...)\n",
      "Dictionary(138 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(974 unique tokens: [\"'s\", ',', '.', '40', 'and']...)\n",
      "Dictionary(217 unique tokens: [',', '.', '2,755', 'a', 'american']...)\n",
      "Dictionary(274 unique tokens: ['(', ')', ',', '.', '2016']...)\n",
      "Dictionary(133 unique tokens: [\"''\", '(', ')', ',', '.â']...)\n",
      "Dictionary(306 unique tokens: [',', 'business', 'insider', 'matt', 'turner']...)\n",
      "Dictionary(255 unique tokens: [',', '.', 'a', 'again', 'answer']...)\n",
      "Dictionary(20 unique tokens: [',', '.', 'a', 'apart', 'at']...)\n",
      "Dictionary(75 unique tokens: [',', '.', 'a', 'across', 'advance']...)\n",
      "Dictionary(193 unique tokens: ['--', '.', '15-year-old', 'a', 'appeared']...)\n",
      "Dictionary(246 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(163 unique tokens: [',', '.', 'a', 'an', 'claiming']...)\n",
      "Dictionary(80 unique tokens: [',', 'business', 'greg', 'hoffman', 'insider']...)\n",
      "Dictionary(90 unique tokens: [',', '.', 'don', 'gotcha', 'hurt']...)\n",
      "Dictionary(321 unique tokens: [',', '.', 'a', 'airlines', 'and']...)\n",
      "Dictionary(183 unique tokens: [\"'s\", '.', '30', 'a', 'and']...)\n",
      "Dictionary(161 unique tokens: ['(', ')', '.', '31', 'allowing']...)\n",
      "Dictionary(69 unique tokens: [',', '.', '1930s', 'about', 'allen']...)\n",
      "Dictionary(426 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(538 unique tokens: [\"'s\", 'a', 'and', 'briefing', 'day']...)\n",
      "Dictionary(104 unique tokens: [\"'s\", ',', '.', 'a', 'can']...)\n",
      "Dictionary(232 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(124 unique tokens: [',', '.', 'a', 'about', 'and']...)\n",
      "Dictionary(56 unique tokens: ['.', '2016', 'agency', 'an', 'and']...)\n",
      "Dictionary(109 unique tokens: [',', '.', 'a', 'abc', 'ap']...)\n",
      "Dictionary(255 unique tokens: [',', 'and', 'appear', 'april', 'are']...)\n",
      "Dictionary(318 unique tokens: [',', '.', 'a', 'also', 'and']...)\n",
      "Dictionary(270 unique tokens: [',', '.', 'a', 'about', 'according']...)\n",
      "Dictionary(662 unique tokens: [',', '.', 'administration', 'africa', 'against']...)\n",
      "Dictionary(119 unique tokens: [\"'s\", ',', '.', 'a', 'an']...)\n",
      "Dictionary(190 unique tokens: ['.', 'are', 'for', 'half-brother', 'have']...)\n",
      "Dictionary(216 unique tokens: [',', '.', 'a', 'and', 'be']...)\n",
      "Dictionary(603 unique tokens: ['(', ')', '4', 'all', 'atlantic']...)\n",
      "Dictionary(184 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(406 unique tokens: ['?', 'absolutely', 'and', 'be', 'can']...)\n",
      "Dictionary(222 unique tokens: [\"'s\", '.', 'an', 'been', 'clegg']...)\n",
      "Dictionary(262 unique tokens: [\"''\", '(', ')', '.', '``']...)\n",
      "Dictionary(94 unique tokens: ['.', '138', 'and', 'as', 'coming']...)\n",
      "Dictionary(187 unique tokens: ['.', 'a', 'against', 'be', 'by']...)\n",
      "Dictionary(398 unique tokens: [',', '.', '@', 'all', 'and']...)\n",
      "Dictionary(122 unique tokens: ['.', 'a', 'after', 'and', 'ashwin']...)\n",
      "Dictionary(185 unique tokens: [',', 'benjamin', 'business', 'insider', 'zhang']...)\n",
      "Dictionary(163 unique tokens: ['.', 'a', 'and', 'as', 'become']...)\n",
      "Dictionary(97 unique tokens: [\"'s\", ':', 'cheese', 'chuck', 'e.']...)\n",
      "Dictionary(19 unique tokens: [',', '/feature/retirement-checkup', ':', '=', 'channel_2']...)\n",
      "Dictionary(203 unique tokens: [':', 'notes', 'takes', '.', '1']...)\n",
      "Dictionary(70 unique tokens: [\"'re\", \"'ve\", ',', '.', ':']...)\n",
      "Dictionary(205 unique tokens: [\"'s\", '.', 'a', 'are', 'asylum-seekers']...)\n",
      "Dictionary(331 unique tokens: [',', '.', 'again', 'anything', 'as']...)\n",
      "Dictionary(454 unique tokens: [',', '.', '?', 'a', 'and']...)\n",
      "Dictionary(530 unique tokens: ['more', '.', ':', 'about', 'advice']...)\n",
      "Dictionary(246 unique tokens: ['&', \"''\", \"'d\", \"'re\", \"'s\"]...)\n",
      "Dictionary(371 unique tokens: [',', '.', '20', 'a', 'ago']...)\n",
      "Dictionary(509 unique tokens: ['more', 'finance', 'firefox', 'on', 'try']...)\n",
      "Dictionary(200 unique tokens: [\"'s\", ',', '.', 'a', 'and']...)\n",
      "Dictionary(357 unique tokens: [',', '.', 'against', 'avoid', 'bankrupt']...)\n",
      "Dictionary(217 unique tokens: [',', '.', 'a', 'and', 'armstrong']...)\n",
      "Dictionary(94 unique tokens: [\"'s\", ',', '.', '2017', 'alongside']...)\n",
      "Dictionary(654 unique tokens: ['and', 'by', 'joel', 'kotkin', 'mark']...)\n",
      "Dictionary(122 unique tokens: ['.', 'advance', 'bill', 'budget', 'care']...)\n",
      "Dictionary(306 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(233 unique tokens: [\"'s\", '.', 'a', 'another', 'been']...)\n",
      "Dictionary(316 unique tokens: [\"'s\", '.', 'a', 'ai', 'behind']...)\n",
      "Dictionary(376 unique tokens: [',', '--', '.', '2010', 'activists']...)\n",
      "Dictionary(354 unique tokens: [',', '.', 'ago', 'aid', 'and']...)\n",
      "Dictionary(327 unique tokens: [\"'s\", ',', '.', '2', ':']...)\n",
      "Dictionary(276 unique tokens: [\"'\", ',', '.', '10', 'a']...)\n",
      "Dictionary(225 unique tokens: [\"'s\", ',', '.', 'a', 'about']...)\n",
      "Dictionary(176 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(212 unique tokens: [',', '.', 'because', 'but', 'dark']...)\n",
      "Dictionary(46 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(87 unique tokens: ['.', 'all', 'allman', 'canceled', 'dates']...)\n",
      "Dictionary(43 unique tokens: [\"'s\", ':', 'at', 'city', 'live']...)\n",
      "Dictionary(180 unique tokens: ['.', 'just', 'me', 'trust', 'are']...)\n",
      "Dictionary(379 unique tokens: [',', '.', 'a', 'analysis', 'behind']...)\n",
      "Dictionary(169 unique tokens: ['#', \"'s\", ',', '.', 'and']...)\n",
      "Dictionary(441 unique tokens: [',', '.', 'a', 'about', 'administration']...)\n",
      "Dictionary(294 unique tokens: ['european', 'have', 'headwear', 'in', 'moved']...)\n",
      "Dictionary(492 unique tokens: [',', '.', '2000', 'bulls', 'didn']...)\n",
      "Dictionary(378 unique tokens: [',', '.', 'a', 'all', 'and']...)\n",
      "Dictionary(26 unique tokens: ['?', 'deal-breaker', 's', 'what', 'your']...)\n",
      "Dictionary(402 unique tokens: ['.', 'a', 'as', 'at', 'blamed']...)\n",
      "Dictionary(364 unique tokens: ['.', 'and', 'another', 'before', 'eu']...)\n",
      "Dictionary(264 unique tokens: [',', '2.5m', 'across', 'bank', 'canada']...)\n",
      "Dictionary(75 unique tokens: ['$', '(', ')', ',', '.']...)\n",
      "Dictionary(378 unique tokens: ['.', '34c', 'a', 'dubbed', 'for']...)\n",
      "Dictionary(118 unique tokens: [\"''\", '(', ')', ',', '.']...)\n",
      "Dictionary(138 unique tokens: ['entertainment', 'from', 'the', 'topic', '13/03/17']...)\n",
      "Dictionary(264 unique tokens: [\"'s\", '.', 'before', 'body', 'brother']...)\n",
      "Dictionary(1104 unique tokens: ['bracket', 'breakdowns', ',', '.', ':']...)\n",
      "Dictionary(285 unique tokens: [',', '.', 'a', 'attention', 'better-funded']...)\n",
      "Dictionary(364 unique tokens: ['15-year', 'a', 'and', 'axed', 'chiefs']...)\n",
      "Dictionary(88 unique tokens: ['#', '//t.co/m0q9i3qaji', '//t.co/yyxcm8sn5v', ':', 'ahead']...)\n",
      "Dictionary(207 unique tokens: ['.', '100', 'a', 'and', 'are']...)\n",
      "Dictionary(206 unique tokens: [\"'s\", '.', '18', 'a', 'biggest']...)\n",
      "Dictionary(333 unique tokens: [\"''\", '.', '``', 'a', 'and']...)\n",
      "Dictionary(286 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(846 unique tokens: ['(', ')', 'civita', 'courtesy', 'della']...)\n",
      "Dictionary(286 unique tokens: [',', '.', 'adults', 'doctors', 'first']...)\n",
      "Dictionary(190 unique tokens: [\"'s\", ',', '.', 'a', 'aide']...)\n",
      "Dictionary(675 unique tokens: [',', '.', '2017', 'bubble', 'clouded']...)\n",
      "Dictionary(108 unique tokens: [',', 'business', 'cain', 'insider', 'ã\\x81ine']...)\n",
      "Dictionary(43 unique tokens: ['and', 'benjamin', 'gmoser', 'justin', 'zhang']...)\n",
      "Dictionary(63 unique tokens: [',', '.', '1.5', '2016', 'against']...)\n",
      "Dictionary(175 unique tokens: ['?', 'can', 'did', 'hollywood', 'tell']...)\n",
      "Dictionary(191 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(169 unique tokens: [\"'s\", ',', '.', 'a', 'airport']...)\n",
      "Dictionary(465 unique tokens: [\"'\", \"'s\", ',', '.', '2016']...)\n",
      "Dictionary(383 unique tokens: [\"'d\", ',', '.', 'an', 'and']...)\n",
      "Dictionary(266 unique tokens: [\"'s\", ',', '--', '.', '1']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1191 unique tokens: ['.', 'an', 'dealing', 'epic', 'friday']...)\n",
      "Dictionary(301 unique tokens: ['-', '.', 'a', 'app', 'brief']...)\n",
      "Dictionary(125 unique tokens: ['(', ')', '.', 'at', 'colbert']...)\n",
      "Dictionary(212 unique tokens: ['.', ':', 'blatantly', 'clear', 'dan']...)\n",
      "Dictionary(284 unique tokens: [',', '.', 'a', 'actually', 'and']...)\n",
      "Dictionary(238 unique tokens: [',', 'alone', 'be', 'better', 'bread']...)\n",
      "Dictionary(225 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(86 unique tokens: [',', 'business', 'elena', 'holodny', 'insider']...)\n",
      "Dictionary(338 unique tokens: [\"'s\", ',', '.', '8,000', 'access']...)\n",
      "Dictionary(522 unique tokens: ['.', 'after', 'be', 'become', 'country']...)\n",
      "Dictionary(374 unique tokens: [',', '.', 'a', 'achy-breaky', 'acquainted']...)\n",
      "Dictionary(121 unique tokens: [\"''\", \"'s\", '(', ')', ',']...)\n",
      "Dictionary(632 unique tokens: [',', '.', '1996', 'a', 'accept']...)\n",
      "Dictionary(123 unique tokens: [\"''\", ',', '.', '``', 'albumâ']...)\n",
      "Dictionary(190 unique tokens: ['.', 'addition', 'andrew', 'angeles', 'free-agent']...)\n",
      "Dictionary(260 unique tokens: [',', '.', '1997', '2011', '9']...)\n",
      "Dictionary(189 unique tokens: ['.', 'arrived', 'battle', 'capital', 'defense']...)\n",
      "Dictionary(61 unique tokens: ['.', 'are', 'bill', 'diners', 'footing']...)\n",
      "Dictionary(332 unique tokens: ['$', ',', '.', '27', 'a']...)\n",
      "Dictionary(305 unique tokens: [\"'\", \"'s\", ',', '.', 'all']...)\n",
      "Dictionary(220 unique tokens: [',', '.', 'after', 'allowing', 'and']...)\n",
      "Dictionary(567 unique tokens: [',', '.', 'and', 'became', 'both']...)\n",
      "Dictionary(60 unique tokens: ['.', 'approach', 'as', 'back', 'beijing']...)\n",
      "Dictionary(75 unique tokens: ['.', 'bean', 'is', 'l.l', 'outdoors']...)\n",
      "Dictionary(185 unique tokens: [\"'\", \"'tea\", ',', '.', 'a']...)\n",
      "Dictionary(229 unique tokens: [',', '.', '2017', ';', 'admits']...)\n",
      "Dictionary(159 unique tokens: [\"'\", \"''\", \"'profoundly\", \"'s\", ',']...)\n",
      "Dictionary(418 unique tokens: [',', '.', 'almost', 'always', 'and']...)\n",
      "Dictionary(183 unique tokens: ['(', ')', ',', '.', 'announced']...)\n",
      "Dictionary(208 unique tokens: ['.', '2017', 'a', 'alex', 'analyst']...)\n",
      "Dictionary(46 unique tokens: [',', '.', '40', '?', 'a']...)\n",
      "Dictionary(169 unique tokens: [',', '.', 'a', 'are', 'happening']...)\n",
      "Dictionary(1198 unique tokens: ['entry', 'foreign', 'from', 'into', 'nation']...)\n",
      "Dictionary(191 unique tokens: ['.', 'abortion', 'abortions', 'allowed', 'already']...)\n",
      "Dictionary(208 unique tokens: [',', 'and', 'can', 'election', 'for']...)\n",
      "Dictionary(89 unique tokens: [',', '--', '.', '2000', '2013']...)\n",
      "Dictionary(1151 unique tokens: [\"''\", \"'s\", ',', '.', '2017']...)\n",
      "Dictionary(425 unique tokens: [',', '.', 'a', 'and', 'banning']...)\n",
      "Dictionary(246 unique tokens: [\"'\", \"'jojo\", '.', ':', 'across']...)\n",
      "Dictionary(150 unique tokens: ['.', 'a', 'accommodation', 'armed', 'bars']...)\n",
      "Dictionary(411 unique tokens: ['%', ',', '.', '47', 'a']...)\n",
      "Dictionary(103 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(383 unique tokens: ['$', ',', '.', '2017', 'a']...)\n",
      "Dictionary(206 unique tokens: ['!', '.', '1', 'a', 'are']...)\n",
      "Dictionary(479 unique tokens: [',', '.', 'abnormalities', 'about', 'also']...)\n",
      "Dictionary(319 unique tokens: [',', '?', 'but', 'did', 'how']...)\n",
      "Dictionary(916 unique tokens: [',', '.', 'a', 'administration', 'and']...)\n",
      "Dictionary(166 unique tokens: ['&', ',', '.', '3,000', 'a']...)\n",
      "Dictionary(79 unique tokens: [',', '.', 'and', 'annual', 'auto']...)\n",
      "Dictionary(263 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(99 unique tokens: [',', '.', 'about', 'adult', 'an']...)\n",
      "Dictionary(239 unique tokens: ['after', 'bizarre', 'by', 'ceremony', 'child']...)\n",
      "Dictionary(412 unique tokens: ['shutterstock', \"'s\", ',', '.', 'a']...)\n",
      "Dictionary(332 unique tokens: ['(', ')', '.', 'as', 'attempted']...)\n",
      "Dictionary(482 unique tokens: ['!', \"'s\", '(', ')', '--']...)\n",
      "Dictionary(643 unique tokens: ['.', 'before', 'ever', 'is', 'making']...)\n",
      "Dictionary(296 unique tokens: [',', '.', '35-year-old', '42-year-old', 'a']...)\n",
      "Dictionary(222 unique tokens: ['.', 'apprentice', 'arnold', 'back', 'be']...)\n",
      "Dictionary(1526 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(52 unique tokens: [',', '.', 'agencies', 'and', 'but']...)\n",
      "Dictionary(209 unique tokens: ['.', 'a', 'americans', 'anti-immigration', 'be']...)\n",
      "Dictionary(234 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(476 unique tokens: [',', '.', '10', '4,500', 'a']...)\n",
      "Dictionary(160 unique tokens: ['-', '.', 'accused', 'al-qaeda', 'an']...)\n",
      "Dictionary(111 unique tokens: ['#', \"'m\", '.', '//t.co/ebm5juszkm', ':']...)\n",
      "Dictionary(474 unique tokens: ['more', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(88 unique tokens: [\"'s\", ',', '.', '10', '9']...)\n",
      "Dictionary(454 unique tokens: ['-', ':', 'a', 'am', 'another']...)\n",
      "Dictionary(51 unique tokens: ['.', 'advising', 'attorney', 'campaign', 'confidence']...)\n",
      "Dictionary(674 unique tokens: [',', '.', 'a', 'an', 'and']...)\n",
      "Dictionary(971 unique tokens: [',', '.', 'about', 'an', 'are']...)\n",
      "Dictionary(1494 unique tokens: [',', '.', '1993', '6th', 'a']...)\n",
      "Dictionary(52 unique tokens: ['.', ':', 'a', 'and', 'business']...)\n",
      "Dictionary(250 unique tokens: ['.', 'a', 'acid', 'after', 'and']...)\n",
      "Dictionary(217 unique tokens: ['&', \"'\", \"''\", \"'idiot\", \"'it\"]...)\n",
      "Dictionary(300 unique tokens: [',', '.', 'abu', 'admitted', 'al-baghdadi']...)\n",
      "Dictionary(95 unique tokens: [',', '.', 'all', 'and', 'antoinette']...)\n",
      "Dictionary(325 unique tokens: [\"'s\", '.', ':', 'an', 'day']...)\n",
      "Dictionary(197 unique tokens: [\"''\", \"'m\", \"'s\", ',', '.']...)\n",
      "Dictionary(216 unique tokens: ['.', 'and', 'another', 'be', 'day']...)\n",
      "Dictionary(33 unique tokens: ['.', 'bit', 'not', 'one', ',']...)\n",
      "Dictionary(97 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(203 unique tokens: [\"'s\", ',', '.', '15', ':']...)\n",
      "Dictionary(1455 unique tokens: ['.', 'behind', 'fights', 'for', 'is']...)\n",
      "Dictionary(245 unique tokens: [\"''\", \"'s\", ',', '.', '``']...)\n",
      "Dictionary(212 unique tokens: [',', '.', 'a', 'academy', 'audience']...)\n",
      "Dictionary(166 unique tokens: ['&', '.', '2016', 'an', 'and']...)\n",
      "Dictionary(116 unique tokens: [',', 'be', 'for', 'future', 'if']...)\n",
      "Dictionary(172 unique tokens: [',', 'also', 'behind', 'he', 'i']...)\n",
      "Dictionary(272 unique tokens: [\"'s\", ',', '.', 'according', 'and']...)\n",
      "Dictionary(661 unique tokens: [',', '.', 'at', 'attest', 'be']...)\n",
      "Dictionary(134 unique tokens: ['!', \"''\", \"'s\", ',', '.']...)\n",
      "Dictionary(269 unique tokens: [',', '.', 'ahead', 'an', 'anticipated']...)\n",
      "Dictionary(506 unique tokens: ['.', 'a', 'for', 'in', 'now']...)\n",
      "Dictionary(63 unique tokens: [',', '.', 'a', 'addresses', 'attention']...)\n",
      "Dictionary(387 unique tokens: ['dufour/afp/getty', 'fred', 'images', ',', '.']...)\n",
      "Dictionary(54 unique tokens: ['.', 'a', 'academy', 'acceptance', 'and']...)\n",
      "Dictionary(1041 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(80 unique tokens: [',', '.', 'accepted', 'actor', 'affleck']...)\n",
      "Dictionary(188 unique tokens: ['$', \"'\", \"''\", \"'endorsement\", \"'failing\"]...)\n",
      "Dictionary(215 unique tokens: [\"'s\", '.', 'a', 'advantage', 'afghanistan']...)\n",
      "Dictionary(403 unique tokens: [\"'s\", \"'ve\", ',', '.', 'all']...)\n",
      "Dictionary(343 unique tokens: [',', 'alien', 'an', 'and', 'been']...)\n",
      "Dictionary(500 unique tokens: [',', '.', '/', '18', '2015']...)\n",
      "Dictionary(354 unique tokens: [',', '.', 'about', 'and', 'any']...)\n",
      "Dictionary(216 unique tokens: [',', '.', '28', 'a', 'according']...)\n",
      "Dictionary(543 unique tokens: [',', '.', ';', 'a', 'access']...)\n",
      "Dictionary(215 unique tokens: [',', '.', '500', 'ago', 'americas']...)\n",
      "Dictionary(446 unique tokens: [',', '.', 'a', 'and', 'as']...)\n",
      "Dictionary(218 unique tokens: ['.', 'a', 'administration', 'an', 'arguement']...)\n",
      "Dictionary(438 unique tokens: [',', '.', ':', 'blizzards', 'boston']...)\n",
      "Dictionary(202 unique tokens: [\"'ve\", ',', '.', ';', 'a']...)\n",
      "Dictionary(77 unique tokens: [',', '...', '?', 'a', 'an']...)\n",
      "Dictionary(53 unique tokens: [',', '.', 'a', 'acknowledged', 'after']...)\n",
      "Dictionary(286 unique tokens: ['(', ')', '.', '2013', 'are']...)\n",
      "Dictionary(335 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(326 unique tokens: [\"'s\", ',', '.', 'according', 'airport']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(235 unique tokens: [',', ':', 'and', 'at', 'bannon']...)\n",
      "Dictionary(286 unique tokens: [',', '.', 'a', 'accept', 'administration']...)\n",
      "Dictionary(115 unique tokens: ['.', 'a', 'an', 'attack', 'brain']...)\n",
      "Dictionary(123 unique tokens: ['(', ')', '.', 'a', 'and']...)\n",
      "Dictionary(224 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(448 unique tokens: [',', '.', 'about', 'according', 'among']...)\n",
      "Dictionary(164 unique tokens: [',', '--', '.', 'a', 'aaron']...)\n",
      "Dictionary(665 unique tokens: ['.', 'a', 'acoustic', 'album', 'christo']...)\n",
      "Dictionary(1061 unique tokens: [\"'re\", \"'s\", '(', ')', '.']...)\n",
      "Dictionary(200 unique tokens: [',', 'abby', 'business', 'insider', 'jackson']...)\n",
      "Dictionary(584 unique tokens: ['shutterstock', \"'s\", '(', ')', ',']...)\n",
      "Dictionary(97 unique tokens: [\"'s\", ',', '.', '3000', ':']...)\n",
      "Dictionary(207 unique tokens: ['.', 'a', 'after', 'allegedly', 'attacked']...)\n",
      "Dictionary(486 unique tokens: ['more', '(', ')', ',', '.']...)\n",
      "Dictionary(260 unique tokens: [',', 'a', 'and', 'away', 'bank']...)\n",
      "Dictionary(241 unique tokens: [\"'\", \"'lover\", \"'yuge\", ',', '.']...)\n",
      "Dictionary(185 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(196 unique tokens: ['&', ',', '.', ';', 'after']...)\n",
      "Dictionary(241 unique tokens: [\"''\", '.', '?', '``', 'a']...)\n",
      "Dictionary(220 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(19 unique tokens: [\"'\", \"'anna\", '.', 'albania', 'be']...)\n",
      "Dictionary(240 unique tokens: ['(', ')', ',', '-', '.']...)\n",
      "Dictionary(129 unique tokens: ['!', \"''\", \"'s\", '(', ')']...)\n",
      "Dictionary(265 unique tokens: ['.', 'breaches', 'buy', 'company', 'despite']...)\n",
      "Dictionary(229 unique tokens: ['-', '.', 'a', 'american', 'an']...)\n",
      "Dictionary(155 unique tokens: ['and', 'bolstering', 'eastern', 'flank', 'games']...)\n",
      "Dictionary(146 unique tokens: [',', '.', 'a', 'after', 'and']...)\n",
      "Dictionary(340 unique tokens: [',', '.', 'a', 'assault', 'been']...)\n",
      "Dictionary(71 unique tokens: ['.', ':', 'a', 'actually', 'agreed']...)\n",
      "Dictionary(541 unique tokens: [',', '.', 'and', 'aren', 'arrived']...)\n",
      "Dictionary(266 unique tokens: [\"'s\", '.', 'alleged', 'an', 'as']...)\n",
      "Dictionary(347 unique tokens: [',', '.', 'a', 'and', 'as']...)\n",
      "Dictionary(137 unique tokens: ['.', 'action', 'appear', 'at', 'conference']...)\n",
      "Dictionary(828 unique tokens: [',', '.', '1973', '69', 'abortion']...)\n",
      "Dictionary(464 unique tokens: [\"''\", ',', '``', 'a', 'biggest']...)\n",
      "Dictionary(465 unique tokens: [',', '.', '?', 'a', 'agency']...)\n",
      "Dictionary(213 unique tokens: [',', 'business', 'guerrasio', 'insider', 'jason']...)\n",
      "Dictionary(78 unique tokens: ['rega', 'sam', ',', '10:07', '19']...)\n",
      "Dictionary(155 unique tokens: [',', '.', 'a', 'and', 'are']...)\n",
      "Dictionary(606 unique tokens: ['(', ')', '.', 'a', 'adviser']...)\n",
      "Dictionary(93 unique tokens: ['!', 'boy', 'boys…oh', 'three', '“']...)\n",
      "Dictionary(399 unique tokens: [',', '.', 'a', 'advocates', 'amnesty']...)\n",
      "Dictionary(57 unique tokens: [',', '.', '30m', 'a', 'and']...)\n",
      "Dictionary(252 unique tokens: [\"'s\", ',', '.', '2016-17', '76ers']...)\n",
      "Dictionary(123 unique tokens: ['.', 'a', 'battered', 'california', 'fell']...)\n",
      "Dictionary(144 unique tokens: ['.', '24', 'a', 'almost', 'boyfriend']...)\n",
      "Dictionary(265 unique tokens: [',', '.', 'a', 'and', 'as']...)\n",
      "Dictionary(49 unique tokens: ['.', 'art', 'digital', 'doors', 'is']...)\n",
      "Dictionary(127 unique tokens: ['--', '.', '14-year-old', 'a', 'cleveland']...)\n",
      "Dictionary(22 unique tokens: ['.', 'a', 'after', 'alive', 'away']...)\n",
      "Dictionary(369 unique tokens: [',', '.', 'always', 'another', 'be']...)\n",
      "Dictionary(304 unique tokens: [',', '.', 'a', 'best', 'british']...)\n",
      "Dictionary(415 unique tokens: ['(', ')', ',', '--', '.']...)\n",
      "Dictionary(369 unique tokens: ['(', ')', ',', '.', '13']...)\n",
      "Dictionary(144 unique tokens: ['.', '24', 'a', 'almost', 'boyfriend']...)\n",
      "Dictionary(182 unique tokens: ['.', 'admitted', 'company', 'every', 'inappropriate']...)\n",
      "Dictionary(14 unique tokens: [',', '?', 'but', 'can', 'pressure']...)\n",
      "Dictionary(94 unique tokens: [\"'s\", ',', '.', 'are', 'but']...)\n",
      "Dictionary(114 unique tokens: ['.', 'at', 'chris', 'christie', 'dined']...)\n",
      "Dictionary(377 unique tokens: ['--', '.', 'a', 'across', 'affect']...)\n",
      "Dictionary(101 unique tokens: [\"'\", \"''\", \"'s\", '(', ')']...)\n",
      "Dictionary(453 unique tokens: ['(', ')', ':', 'images', 'photo']...)\n",
      "Dictionary(334 unique tokens: [',', '.', 'a', 'adhd', 'and']...)\n",
      "Dictionary(275 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(368 unique tokens: ['(', ')', '.', ':', 'about']...)\n",
      "Dictionary(465 unique tokens: [',', '--', '.', '2,000', 'a']...)\n",
      "Dictionary(537 unique tokens: [\"'\", ',', '--', '.', '2017']...)\n",
      "Dictionary(171 unique tokens: ['.', 'can', 'help', 'it', 't']...)\n",
      "Dictionary(343 unique tokens: ['.', 'administration', 'after', 'benjamin', 'first']...)\n",
      "Dictionary(221 unique tokens: [',', '.', 'a', 'again', 'as']...)\n",
      "Dictionary(158 unique tokens: [',', '.', 'a', 'able', 'again']...)\n",
      "Dictionary(101 unique tokens: [',', 'business', 'garber', 'insider', 'jonathan']...)\n",
      "Dictionary(130 unique tokens: [',', 'biz', 'business', 'carson', 'insider']...)\n",
      "Dictionary(192 unique tokens: ['deal', 'disney', 'ended', 'its', 'recently']...)\n",
      "Dictionary(370 unique tokens: ['at', 'be', 'because', 'being', 'bloc']...)\n",
      "Dictionary(164 unique tokens: ['coming', 'countdown', 'countdownlbl', 'in', 'next']...)\n",
      "Dictionary(98 unique tokens: [',', '.', 'a', 'and', 'collided']...)\n",
      "Dictionary(268 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(230 unique tokens: ['(', ')', '.', 'a', 'abc']...)\n",
      "Dictionary(218 unique tokens: [',', '.', 'allow', 'and', 'as']...)\n",
      "Dictionary(139 unique tokens: [',', '-', '.', 'a', 'about']...)\n",
      "Dictionary(454 unique tokens: ['more', '.', 'anymore', 'chachi', 'doesn']...)\n",
      "Dictionary(56 unique tokens: [',', '.', '[', ']', 'a']...)\n",
      "Dictionary(884 unique tokens: [',', '.', ':', 'a', 'about']...)\n",
      "Dictionary(216 unique tokens: [\"''\", \"'s\", \"'ve\", '(', ')']...)\n",
      "Dictionary(270 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(475 unique tokens: ['a', 'at', 'been', 'boost', 'budgets']...)\n",
      "Dictionary(145 unique tokens: ['.', 'always', 'has', 'her', 'katy']...)\n",
      "Dictionary(188 unique tokens: ['.', '59th', 'are', 'awards', 'finally']...)\n",
      "Dictionary(103 unique tokens: [',', '2017', 'a', 'after', 'ahead']...)\n",
      "Dictionary(77 unique tokens: ['-', '.', 'a', 'at', 'be']...)\n",
      "Dictionary(53 unique tokens: ['and', 'eames', 'lebowitz', 'shana', 'yates']...)\n",
      "Dictionary(199 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(108 unique tokens: ['!', ',', 'and', 'are', 'child']...)\n",
      "Dictionary(871 unique tokens: ['(', ')', ',', '.', '1867']...)\n",
      "Dictionary(269 unique tokens: ['*gestures', '?', 'around*', 'why', 'wildly']...)\n",
      "Dictionary(115 unique tokens: [',', '.', '16', 'actresses', 'and']...)\n",
      "Dictionary(1587 unique tokens: [',', '.', 'a', 'be', 'for']...)\n",
      "Dictionary(187 unique tokens: [\"'s\", '.', 'a', 'been', 'down']...)\n",
      "Dictionary(231 unique tokens: [\"'\", \"''\", '(', ')', ',']...)\n",
      "Dictionary(435 unique tokens: [\"'s\", '.', 'a', 'as', 'beckham']...)\n",
      "Dictionary(265 unique tokens: [',', '.', 'a', 'an', 'appeal']...)\n",
      "Dictionary(204 unique tokens: ['.', 'across', 'are', 'being', 'congressional']...)\n",
      "Dictionary(94 unique tokens: ['.', 'a', 'airplane', 'an', 'and']...)\n",
      "Dictionary(207 unique tokens: [',', '.', '70s', '80s', 'and']...)\n",
      "Dictionary(162 unique tokens: ['.', '82-79', 'a', 'and', 'are']...)\n",
      "Dictionary(212 unique tokens: [',', '.', '2014', 'according', 'airbus']...)\n",
      "Dictionary(99 unique tokens: [\"'s\", ',', '.', 'addressed', 'colbert']...)\n",
      "Dictionary(305 unique tokens: [',', '.', '1945', 'a', 'army']...)\n",
      "Dictionary(213 unique tokens: ['--', '.', 'a', 'angeles', 'be']...)\n",
      "Dictionary(233 unique tokens: [\"'s\", '(', ')', ',', '.']...)\n",
      "Dictionary(456 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(377 unique tokens: [\"'s\", ',', '.', 'a', 'according']...)\n",
      "Dictionary(314 unique tokens: [',', '.', 'a', 'abilities', 'according']...)\n",
      "Dictionary(183 unique tokens: ['(', ')', ',', '.', 'a']...)\n",
      "Dictionary(455 unique tokens: [':', 'bocsi/bloomberg', 'krisztian', 'photographer', \"'s\"]...)\n",
      "Dictionary(152 unique tokens: [\"'s\", ',', '.', 'all', 'all-star']...)\n",
      "Dictionary(312 unique tokens: [',', '.', 'across', 'after', 'almost']...)\n",
      "Dictionary(345 unique tokens: [',', '.', 'a', 'addressed', 'after']...)\n",
      "Dictionary(399 unique tokens: ['more', ',', '.', 'a', 'actually']...)\n",
      "Dictionary(242 unique tokens: [',', '.', 'a', 'buddy', 'caught']...)\n",
      "Dictionary(25 unique tokens: ['.', 'a', 'been', 'china', 'cliff-edge']...)\n",
      "Dictionary(218 unique tokens: ['.', 'a', 'daily', 'first', 'france']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(289 unique tokens: [',', '.', 'about', 'app', 'flipboard']...)\n",
      "Dictionary(404 unique tokens: [',', '.', 'a', 'ark.', 'at']...)\n",
      "Dictionary(252 unique tokens: [\"''\", \"'s\", '.', '1', ':']...)\n",
      "Dictionary(351 unique tokens: ['.', 'a', 'accused', 'attorney', 'condemned']...)\n",
      "Dictionary(63 unique tokens: [\"'s\", ',', '.', 'activist', 'bbc']...)\n",
      "Dictionary(54 unique tokens: ['$', ',', '.', '1.8', 'a']...)\n",
      "Dictionary(327 unique tokens: [',', '.', 'a', 'about', 'after']...)\n",
      "Dictionary(295 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(342 unique tokens: [',', '.', 'a', 'about', 'book']...)\n",
      "Dictionary(91 unique tokens: ['.', 'about', 'ending', 'health', 'is']...)\n",
      "Dictionary(356 unique tokens: [',', 'advances', 'an', 'books', 'children']...)\n",
      "Dictionary(212 unique tokens: [',', '.', 'a', 'allow', 'american']...)\n",
      "Dictionary(42 unique tokens: [',', 'big', 'journalism', 'milo', \"'s\"]...)\n",
      "Dictionary(422 unique tokens: [',', '.', 'a', 'according', 'after']...)\n",
      "Dictionary(368 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(234 unique tokens: ['.', 'allegedly', 'andrew', 'barrister', 'beatings']...)\n",
      "Dictionary(157 unique tokens: ['.', 'around', 'doesn', 'fuck', 'sansa']...)\n",
      "Dictionary(252 unique tokens: ['share', 'this', 'with', 'email', 'facebook']...)\n",
      "Dictionary(271 unique tokens: ['$', '%', \"'d\", \"'s\", '(']...)\n",
      "Dictionary(210 unique tokens: ['$', ',', '.', '5', '50']...)\n",
      "Dictionary(613 unique tokens: [',', '.', 'administration', 'and', 'as']...)\n",
      "Dictionary(153 unique tokens: [',', '.', '21-3', 'a', 'against']...)\n",
      "Dictionary(193 unique tokens: ['.', '12-year-old', 'a', 'again', 'america']...)\n",
      "Dictionary(438 unique tokens: ['.', 'bowl', 'coverage', 'follow', 'here']...)\n",
      "Dictionary(172 unique tokens: [',', '.', 'and', 'as', 'broadcast']...)\n",
      "Dictionary(243 unique tokens: ['(', ')', '.', 'and', 'between']...)\n",
      "Dictionary(255 unique tokens: [',', '.', 'addition', 'also', 'arrival']...)\n",
      "Dictionary(993 unique tokens: [\"'s\", ',', '.', 'a', 'but']...)\n",
      "Dictionary(319 unique tokens: [\"'\", \"'becoming\", '(', ')', 'attends']...)\n",
      "Dictionary(1587 unique tokens: [',', '.', 'a', 'be', 'for']...)\n",
      "Dictionary(618 unique tokens: [',', '.', '2017', 'about', 'bessemer']...)\n",
      "Dictionary(390 unique tokens: ['1', 'related', \"'s\", ',', '--']...)\n",
      "Dictionary(54 unique tokens: [':', 'aly', 'credit', 'song/reuters', 'dunand/afp/getty']...)\n",
      "Dictionary(5 unique tokens: ['?', 'still', 'watching', 'device', 'rotate'])\n",
      "Dictionary(188 unique tokens: ['.', 'days', 'daytime', 'donald', 'done']...)\n",
      "Dictionary(234 unique tokens: ['.', 'a', 'and', 'attack', 'been']...)\n",
      "Dictionary(425 unique tokens: [\"'s\", ',', '.', 'a', 'app']...)\n",
      "Dictionary(285 unique tokens: [',', '.', 'are', 'away', 'from']...)\n",
      "Dictionary(373 unique tokens: [',', '?', 'in', 'is', 'of']...)\n",
      "Dictionary(274 unique tokens: ['$', '.', '17', 'a', 'administration']...)\n",
      "Dictionary(271 unique tokens: ['(', ')', ',', '.', 'and']...)\n",
      "Dictionary(323 unique tokens: [',', '--', '.', '12', '8-hour']...)\n",
      "Dictionary(164 unique tokens: ['.', 'a', 'canberra', 'chauffeur-driven', 'club']...)\n",
      "Dictionary(215 unique tokens: ['#', ',', '.', '?', 'a']...)\n",
      "Dictionary(22 unique tokens: [':', 'and', 'belichick', 'bill', 'bowl']...)\n",
      "Dictionary(202 unique tokens: [\"''\", '.', '21st-century', '``', 'a']...)\n",
      "Dictionary(335 unique tokens: ['%', \"'s\", ',', '.', '20']...)\n",
      "Dictionary(207 unique tokens: [\"'\", \"'ugliness\", '.', 'a', 'according']...)\n",
      "Dictionary(55 unique tokens: [',', 'dan', 'dear', '.', '?']...)\n",
      "Dictionary(414 unique tokens: [',', '.', 'a', 'achieve', 'agreement']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.078901097"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbb = []\n",
    "for x in range (2000):\n",
    "    sim_val = gensim_sim(non_clickbait_train[x]['targetTitle'],non_clickbait_train[x]['targetParagraphs'])\n",
    "    if sim_val > 0:\n",
    "        bbb.append(sim_val)\n",
    "np.mean(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show\n",
      "4\n",
      "Number of words in dictionary: 36\n",
      "0 'm\n",
      "1 .\n",
      "2 i\n",
      "3 on\n",
      "4 road\n",
      "5 show\n",
      "6 taking\n",
      "7 the\n",
      "8 a\n",
      "9 are\n",
      "10 force\n",
      "11 multiplier\n",
      "12 my\n",
      "13 socks\n",
      "14 's\n",
      "15 am\n",
      "16 barber\n",
      "17 cut\n",
      "18 cuts\n",
      "19 does\n",
      "20 everyone\n",
      "21 hair\n",
      "22 n't\n",
      "23 own\n",
      "24 their\n",
      "25 who\n",
      "26 has\n",
      "27 is\n",
      "28 it\n",
      "29 legend\n",
      "30 mad\n",
      "31 mind\n",
      "32 monkey\n",
      "33 that\n",
      "34 fun\n",
      "35 make\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary[5])\n",
    "print(dictionary.token2id['road'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(1, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(1, 1), (2, 1), (7, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2)], [(1, 1), (7, 1), (8, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(1, 1), (2, 1), (12, 1), (23, 1), (34, 1), (35, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=5, num_nnz=47)\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 5 documents in 0 shards (stored under /var/folders/0f/qdj5sql91zd4g4xf9scp_jx80000gq/T/simserver1a8bc5)\n",
      "<class 'gensim.similarities.docsim.Similarity'>\n"
     ]
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity(None,tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['socks', 'are', 'a', 'force', 'for', 'good', '.']\n",
      "[(1, 1), (8, 1), (9, 1), (10, 1), (13, 1)]\n",
      "[(8, 0.31226270667960454), (9, 0.54848032538919966), (10, 0.54848032538919966), (13, 0.54848032538919966)]\n"
     ]
    }
   ],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(\"Socks are a force for good.\")]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.84565616,  0.        ,  0.06124881,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clickbait_train[0]['targetTitle']\n",
    "clickbait_train[0]['targetParagraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WINNIPEG, Manitoba – The bubble U.S. Soccer is putting around Hope Solo isn't working to calm anyone's concerns about the star goalkeeper.\""
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickbait_train[0]['targetParagraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['winnipeg', ',', 'manitoba', '–', 'the', 'bubble', 'u.s.', 'soccer', 'is', 'putting', 'around', 'hope', 'solo', 'is', \"n't\", 'working', 'to', 'calm', 'anyone', \"'s\", 'concerns', 'about', 'the', 'star', 'goalkeeper', '.']\n",
      "[['winnipeg', 'manitoba', '–', 'the', 'bubble', 'u.s.', 'soccer', 'is', 'putting', 'around', 'hope', 'solo', 'is', \"n't\", 'working', 'to', 'calm', 'anyone', \"'s\", 'concerns', 'about', 'the', 'star', 'goalkeeper'], ['the', 'latest', 'lament', 'comes', 'from', 'no', 'less', 'than', 'a', 'u.s', 'senator', 'who', 'into', 'solo', \"'s\", 'domestic', 'violence', 'incident', 'of', 'last', 'year', 'and', 'offer', 'a', 'detailed', 'explanation', 'of', 'why', 'solo', 'is', 'on', 'the', 'field', 'she', 'is', 'expected', 'to', 'be', 'the', 'starting', 'goalkeeper', 'when', 'the', 'usa', 'plays', 'sweden', 'in', 'its', 'second', 'group', 'game', 'at', 'the', 'women', \"'s\", 'world', 'cup', 'on', 'friday'], ['fc', 'yahoo'], ['u.s', 'senator', 'richard', 'blumenthal', 'of', 'connecticut', 'penned', 'a', 'lengthy', 'complaint', 'about', 'the', 'near-silence', 'the', 'organization', 'has', 'given', 'on', 'solo', 'especially', 'in', 'the', 'wake', 'of', 'espn', \"'s\", '``', 'outside', 'the', 'lines', \"''\", 'report', 'on', 'sunday', 'blumenthal', 'wrote', 'that', 'if', 'the', 'report', 'is', 'accurate', '``', 'u.s.', 'soccer', \"'s\", 'approach', 'to', 'domestic', 'violence', 'and', 'family', 'violence', 'is', 'at', 'best', 'superficial', 'and', 'at', 'worst', 'dangerously', 'neglectful', 'and', 'self-serving', \"''\"], ['this', 'situation', 'is', 'well', 'beyond', 'solo', 'now', 'u.s.', 'soccer', 'has', 'made', 'this', 'a', 'referendum', 'about', 'its', 'own', 'ability', 'to', 'represent', 'the', 'values', 'of', 'the', 'nation', '``', 'as', 'boys', 'and', 'girls', 'tune', 'into', 'friday', \"'s\", 'game', 'watching', 'the', 'women', 'on', 'tv', 'as', 'role', 'models', \"''\", 'blumenthal', 'wrote', '``', 'what', 'is', 'the', 'message', 'of', 'starting', 'hope', 'solo', 'at', 'goal', \"''\"], ['women', \"'s\", 'world', 'cup'], ['u.s.', 'soccer', 'is', 'not', 'only', 'avoiding', 'difficult', 'questions', 'it', 'is', 'also', 'avoiding', 'an', 'account', 'of', 'all', 'its', 'actions', 'even', 'nfl', 'commissioner', 'roger', 'goodell', 'has', 'to', 'some', 'extent', 'owned', 'up', 'to', 'his', 'failures', 'on', 'the', 'ray', 'rice', 'case', 'yet', 'gulati', 'has', 'not', 'even', 'decried', 'solo', \"'s\", 'poor', 'decisions'], ['last', 'september', 'three', 'months', 'after', 'solo', \"'s\", 'domestic', 'violence', 'charges', 'which', 'were', 'later', 'gulati', 'released', 'this', 'vague', 'statement', 'on', 'the', 'matter'], ['``', 'u.s.', 'soccer', 'takes', 'the', 'issue', 'of', 'domestic', 'violence', 'very', 'seriously', 'from', 'the', 'beginning', 'we', 'considered', 'the', 'information', 'available', 'and', 'have', 'taken', 'a', 'deliberate', 'and', 'thoughtful', 'approach', 'regarding', 'hope', 'solo', \"'s\", 'status', 'with', 'the', 'national', 'team', 'based', 'on', 'that', 'information', 'u.s.', 'soccer', 'stands', 'by', 'our', 'decision', 'to', 'allow', 'her', 'to', 'participate', 'with', 'the', 'team', 'as', 'the', 'legal', 'process', 'unfolds', 'if', 'new', 'information', 'becomes', 'available', 'we', 'will', 'carefully', 'consider', 'it', \"''\"], ['a', 'lot', 'of', 'this', 'would', 'be', 'solved', 'if', 'gulati', 'and', 'solo', 'held', 'a', 'press', 'conference', 'and', 'claimed', 'some', 'accountability', 'it', \"'s\", 'clear', 'from', 'monday', \"'s\", 'dominant', 'performance', 'in', 'a', '3-1', 'tournament-opening', 'win', 'over', 'australia', 'that', 'solo', 'is', 'not', 'distracted', 'by', 'the', 'national', 'discussion', 'of', 'her', 'past', 'so', 'a', 'short', 'appearance', '–', 'even', 'without', 'reporters', 'questions', '–', 'probably', 'wo', \"n't\", 'ruin', 'the', 'u.s.', \"'s\", 'chances', 'for', 'a', 'trophy', 'and', 'claiming', 'that', 'solo', 'has', 'a', 'match', 'to', 'focus', 'on', 'is', \"n't\", 'credible', 'as', 'it', \"'s\", 'basically', 'an', 'admission', 'that', 'a', 'single', 'game', 'is', 'more', 'important', 'than', 'a', 'discussion', 'of', 'domestic', 'violence'], ['for', 'gulati', 'there', 'is', 'little', 'excuse', 'the', 'silence', 'the', 'lack', 'of', 'punishment', 'and', 'then', 'the', 'decision', 'to', 'allow', 'head', 'coach', 'jill', 'ellis', 'to', 'discuss', 'or', 'not', 'discuss', 'the', 'situation', 'here', 'combines', 'to', 'make', 'the', 'top', 'official', 'of', 'american', 'soccer', 'look', 'like', 'he', 'does', \"n't\", 'prioritize', 'this', 'issue'], ['``', 'in', 'the', 'wake', 'of', 'this', 'violent', 'incident', 'u.s.', 'soccer', 'offered', 'no', 'comment', 'publicly', 'for', 'three', 'months', \"''\", 'blumenthal', 'wrote', '``', 'it', 'finally', 'issued', 'a', 'statement', 'that', 'was', 'purportedly', 'the', 'result', 'of', 'a', \"'deliberate\", 'and', 'thoughtful', 'approach', 'to', 'consider', 'the', 'incident', 'and', 'determine', 'hope', 'solo', \"'s\", 'status', 'with', 'the', 'team', 'but', 'it', 'neglected', 'to', 'include', 'an', 'effort', 'to', 'contact', 'the', 'alleged', 'victims', \"''\"], ['the', 'more', 'u.s.', 'soccer', 'tries', 'to', 'shift', 'focus', 'to', 'the', 'field', 'the', 'less', 'it', 'accomplishes', 'that', 'this', 'is', 'the', 'super', 'bowl', 'of', 'women', \"'s\", 'soccer', 'and', 'decrying', 'this', 'as', '``', 'old', 'news', \"''\", 'does', \"n't\", 'work', 'because', 'the', 'entire', 'country', 'is', 'watching', 'now', 'countless', 'americans', 'are', 'debating', 'whether', 'to', 'root', 'for', 'solo', 'or', 'not', 'and', 'her', 'protectors', 'are', 'effectively', 'convincing', 'a', 'lot', 'of', 'people', 'to', 'remain', 'skeptical', 'of', 'her'], ['it', 'does', \"n't\", 'have', 'to', 'be', 'this', 'way', 'a', 'better', 'explanation', 'of', 'what', 'gulati', 'has', 'done', 'on', 'this', 'topic', '–', 'and', 'a', 'better', 'explanation', 'of', 'what', 'solo', 'has', 'done', 'to', 'work', 'on', 'her', 'problems', '–', 'would', 'go', 'a', 'long', 'way', 'toward', 'moving', 'on', 'especially', 'the', 'way', 'u.s.', 'soccer', 'clearly', 'wants', 'instead', 'there', 'is', 'opacity', 'where', 'there', 'should', 'be', 'transparency'], ['the', 'nfl', 'has', 'come', 'under', 'a', 'lot', 'of', 'scrutiny', 'for', 'its', 'efforts', 'to', '``', 'protect', 'the', 'shield', \"''\", 'but', 'u.s.', 'soccer', \"'s\", 'shield', 'stands', 'for', 'a', 'lot', 'more', 'than', 'just', 'a', 'sport', 'that', 'shield', 'should', \"n't\", 'only', 'be', 'used', 'to', 'defend', 'a', 'player'], ['735']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in clickbait_train[0]['targetParagraphs'] ]\n",
    "print((gen_docs[0]))\n",
    "gen_docs_ = [[a for a in c if a not in punctuation] for c in gen_docs ]\n",
    "print(gen_docs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "around\n",
      "3\n",
      "Number of words in dictionary: 341\n",
      "0 's\n",
      "1 ,\n",
      "2 .\n",
      "3 about\n",
      "4 anyone\n",
      "5 around\n",
      "6 bubble\n",
      "7 calm\n",
      "8 concerns\n",
      "9 goalkeeper\n",
      "10 hope\n",
      "11 is\n",
      "12 manitoba\n",
      "13 n't\n",
      "14 putting\n",
      "15 soccer\n",
      "16 solo\n",
      "17 star\n",
      "18 the\n",
      "19 to\n",
      "20 u.s.\n",
      "21 winnipeg\n",
      "22 working\n",
      "23 –\n",
      "24 a\n",
      "25 and\n",
      "26 at\n",
      "27 be\n",
      "28 comes\n",
      "29 cup\n",
      "30 detailed\n",
      "31 domestic\n",
      "32 expected\n",
      "33 explanation\n",
      "34 field\n",
      "35 friday\n",
      "36 from\n",
      "37 game\n",
      "38 group\n",
      "39 in\n",
      "40 incident\n",
      "41 into\n",
      "42 its\n",
      "43 lament\n",
      "44 last\n",
      "45 latest\n",
      "46 less\n",
      "47 no\n",
      "48 of\n",
      "49 offer\n",
      "50 on\n",
      "51 plays\n",
      "52 second\n",
      "53 senator\n",
      "54 she\n",
      "55 starting\n",
      "56 sweden\n",
      "57 than\n",
      "58 u.s\n",
      "59 usa\n",
      "60 violence\n",
      "61 when\n",
      "62 who\n",
      "63 why\n",
      "64 women\n",
      "65 world\n",
      "66 year\n",
      "67 :\n",
      "68 [\n",
      "69 ]\n",
      "70 fc\n",
      "71 yahoo\n",
      "72 ''\n",
      "73 ``\n",
      "74 accurate\n",
      "75 approach\n",
      "76 best\n",
      "77 blumenthal\n",
      "78 complaint\n",
      "79 connecticut\n",
      "80 dangerously\n",
      "81 especially\n",
      "82 espn\n",
      "83 family\n",
      "84 given\n",
      "85 has\n",
      "86 if\n",
      "87 lengthy\n",
      "88 lines\n",
      "89 near-silence\n",
      "90 neglectful\n",
      "91 organization\n",
      "92 outside\n",
      "93 penned\n",
      "94 report\n",
      "95 richard\n",
      "96 self-serving\n",
      "97 sunday\n",
      "98 superficial\n",
      "99 that\n",
      "100 wake\n",
      "101 worst\n",
      "102 wrote\n",
      "103 ?\n",
      "104 ability\n",
      "105 as\n",
      "106 beyond\n",
      "107 boys\n",
      "108 girls\n",
      "109 goal\n",
      "110 made\n",
      "111 message\n",
      "112 models\n",
      "113 nation\n",
      "114 now\n",
      "115 own\n",
      "116 referendum\n",
      "117 represent\n",
      "118 role\n",
      "119 situation\n",
      "120 this\n",
      "121 tune\n",
      "122 tv\n",
      "123 values\n",
      "124 watching\n",
      "125 well\n",
      "126 what\n",
      "127 |\n",
      "128 account\n",
      "129 actions\n",
      "130 all\n",
      "131 also\n",
      "132 an\n",
      "133 avoiding\n",
      "134 case\n",
      "135 commissioner\n",
      "136 decisions\n",
      "137 decried\n",
      "138 difficult\n",
      "139 even\n",
      "140 extent\n",
      "141 failures\n",
      "142 goodell\n",
      "143 gulati\n",
      "144 his\n",
      "145 it\n",
      "146 nfl\n",
      "147 not\n",
      "148 only\n",
      "149 owned\n",
      "150 poor\n",
      "151 questions\n",
      "152 ray\n",
      "153 rice\n",
      "154 roger\n",
      "155 some\n",
      "156 up\n",
      "157 yet\n",
      "158 (\n",
      "159 )\n",
      "160 after\n",
      "161 charges\n",
      "162 later\n",
      "163 matter\n",
      "164 months\n",
      "165 released\n",
      "166 september\n",
      "167 statement\n",
      "168 three\n",
      "169 vague\n",
      "170 were\n",
      "171 which\n",
      "172 allow\n",
      "173 available\n",
      "174 based\n",
      "175 becomes\n",
      "176 beginning\n",
      "177 by\n",
      "178 carefully\n",
      "179 consider\n",
      "180 considered\n",
      "181 decision\n",
      "182 deliberate\n",
      "183 have\n",
      "184 her\n",
      "185 information\n",
      "186 issue\n",
      "187 legal\n",
      "188 national\n",
      "189 new\n",
      "190 our\n",
      "191 participate\n",
      "192 process\n",
      "193 regarding\n",
      "194 seriously\n",
      "195 stands\n",
      "196 status\n",
      "197 taken\n",
      "198 takes\n",
      "199 team\n",
      "200 thoughtful\n",
      "201 unfolds\n",
      "202 very\n",
      "203 we\n",
      "204 will\n",
      "205 with\n",
      "206 '\n",
      "207 3-1\n",
      "208 accountability\n",
      "209 admission\n",
      "210 appearance\n",
      "211 australia\n",
      "212 basically\n",
      "213 chances\n",
      "214 claimed\n",
      "215 claiming\n",
      "216 clear\n",
      "217 conference\n",
      "218 credible\n",
      "219 discussion\n",
      "220 distracted\n",
      "221 dominant\n",
      "222 focus\n",
      "223 for\n",
      "224 held\n",
      "225 important\n",
      "226 lot\n",
      "227 match\n",
      "228 monday\n",
      "229 more\n",
      "230 over\n",
      "231 past\n",
      "232 performance\n",
      "233 press\n",
      "234 probably\n",
      "235 reporters\n",
      "236 ruin\n",
      "237 short\n",
      "238 single\n",
      "239 so\n",
      "240 solved\n",
      "241 tournament-opening\n",
      "242 trophy\n",
      "243 win\n",
      "244 without\n",
      "245 wo\n",
      "246 would\n",
      "247 american\n",
      "248 coach\n",
      "249 combines\n",
      "250 discuss\n",
      "251 does\n",
      "252 ellis\n",
      "253 excuse\n",
      "254 he\n",
      "255 head\n",
      "256 here\n",
      "257 jill\n",
      "258 lack\n",
      "259 like\n",
      "260 little\n",
      "261 look\n",
      "262 make\n",
      "263 official\n",
      "264 or\n",
      "265 prioritize\n",
      "266 punishment\n",
      "267 silence\n",
      "268 then\n",
      "269 there\n",
      "270 top\n",
      "271 'deliberate\n",
      "272 alleged\n",
      "273 but\n",
      "274 comment\n",
      "275 contact\n",
      "276 determine\n",
      "277 effort\n",
      "278 finally\n",
      "279 include\n",
      "280 issued\n",
      "281 neglected\n",
      "282 offered\n",
      "283 publicly\n",
      "284 purportedly\n",
      "285 result\n",
      "286 victims\n",
      "287 violent\n",
      "288 was\n",
      "289 accomplishes\n",
      "290 americans\n",
      "291 are\n",
      "292 because\n",
      "293 bowl\n",
      "294 convincing\n",
      "295 countless\n",
      "296 country\n",
      "297 debating\n",
      "298 decrying\n",
      "299 effectively\n",
      "300 entire\n",
      "301 news\n",
      "302 old\n",
      "303 people\n",
      "304 protectors\n",
      "305 remain\n",
      "306 root\n",
      "307 shift\n",
      "308 skeptical\n",
      "309 super\n",
      "310 tries\n",
      "311 whether\n",
      "312 work\n",
      "313 better\n",
      "314 clearly\n",
      "315 done\n",
      "316 go\n",
      "317 instead\n",
      "318 long\n",
      "319 moving\n",
      "320 opacity\n",
      "321 problems\n",
      "322 should\n",
      "323 topic\n",
      "324 toward\n",
      "325 transparency\n",
      "326 wants\n",
      "327 way\n",
      "328 where\n",
      "329 come\n",
      "330 defend\n",
      "331 efforts\n",
      "332 just\n",
      "333 player\n",
      "334 protect\n",
      "335 scrutiny\n",
      "336 shield\n",
      "337 sport\n",
      "338 under\n",
      "339 used\n",
      "340 735\n"
     ]
    }
   ],
   "source": [
    "dictionary_ = gensim.corpora.Dictionary(gen_docs_)\n",
    "print(dictionary_[5])\n",
    "print(dictionary_.token2id['about'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary_))\n",
    "for i in range(len(dictionary_)):\n",
    "    print(i, dictionary_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(0, 2), (1, 1), (2, 3), (9, 1), (11, 2), (16, 2), (18, 5), (19, 1), (24, 2), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 2), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)], [(67, 1), (68, 1), (69, 1), (70, 1), (71, 1)], [(0, 2), (1, 1), (2, 3), (3, 1), (11, 2), (15, 1), (16, 1), (18, 5), (19, 1), (20, 1), (24, 1), (25, 3), (26, 2), (31, 1), (39, 1), (48, 2), (50, 2), (53, 1), (58, 1), (60, 2), (72, 2), (73, 2), (74, 1), (75, 1), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1)], [(0, 1), (1, 3), (2, 2), (3, 1), (10, 1), (11, 2), (15, 1), (16, 2), (18, 4), (19, 1), (20, 1), (24, 1), (25, 1), (26, 1), (35, 1), (37, 1), (41, 1), (42, 1), (48, 2), (50, 1), (55, 1), (64, 1), (72, 2), (73, 2), (77, 1), (85, 1), (102, 1), (103, 1), (104, 1), (105, 2), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1)], [(0, 1), (29, 1), (64, 1), (65, 1), (67, 1), (68, 1), (69, 1), (127, 3)], [(0, 1), (1, 4), (2, 2), (11, 2), (15, 1), (16, 1), (18, 1), (19, 2), (20, 1), (42, 1), (48, 1), (50, 1), (85, 2), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 2), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 2), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 2), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1)], [(0, 1), (1, 2), (16, 1), (18, 1), (31, 1), (44, 1), (50, 1), (60, 1), (67, 1), (120, 1), (143, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1)], [(0, 1), (1, 2), (2, 4), (10, 1), (15, 2), (16, 1), (18, 6), (19, 2), (20, 2), (24, 1), (25, 2), (31, 1), (36, 1), (48, 1), (50, 1), (60, 1), (72, 1), (73, 1), (75, 1), (86, 1), (99, 1), (105, 1), (145, 1), (172, 1), (173, 2), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 3), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 1), (202, 1), (203, 2), (204, 1), (205, 2)], [(0, 4), (1, 1), (2, 3), (11, 3), (13, 2), (16, 3), (18, 2), (19, 1), (20, 1), (23, 2), (24, 8), (25, 3), (27, 1), (31, 1), (36, 1), (37, 1), (39, 1), (48, 3), (50, 1), (57, 1), (60, 1), (85, 1), (86, 1), (99, 3), (105, 1), (120, 1), (132, 1), (139, 1), (143, 1), (145, 2), (147, 1), (151, 1), (155, 1), (177, 1), (184, 1), (188, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 2), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1)], [(1, 3), (2, 2), (11, 1), (13, 1), (15, 1), (18, 5), (19, 3), (25, 1), (48, 2), (119, 1), (120, 1), (143, 1), (147, 1), (158, 1), (159, 1), (172, 1), (181, 1), (186, 1), (223, 1), (247, 1), (248, 1), (249, 1), (250, 2), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1)], [(0, 1), (1, 3), (2, 2), (10, 1), (15, 1), (16, 1), (18, 5), (19, 3), (20, 1), (24, 2), (25, 2), (39, 1), (40, 2), (47, 1), (48, 2), (72, 2), (73, 2), (75, 1), (77, 1), (99, 1), (100, 1), (102, 1), (120, 1), (132, 1), (145, 2), (164, 1), (167, 1), (168, 1), (179, 1), (196, 1), (199, 1), (200, 1), (205, 1), (206, 1), (223, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1)], [(0, 1), (1, 3), (2, 3), (11, 2), (13, 1), (15, 2), (16, 1), (18, 5), (19, 4), (20, 1), (24, 1), (25, 2), (34, 1), (46, 1), (48, 3), (64, 1), (72, 1), (73, 1), (99, 1), (105, 1), (114, 1), (120, 2), (124, 1), (145, 1), (147, 1), (184, 2), (222, 1), (223, 1), (226, 1), (229, 1), (251, 1), (264, 1), (289, 1), (290, 1), (291, 2), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1)], [(1, 2), (2, 3), (11, 1), (13, 1), (15, 1), (16, 1), (18, 1), (19, 2), (20, 1), (23, 2), (24, 3), (25, 1), (27, 2), (33, 2), (48, 2), (50, 3), (81, 1), (85, 2), (120, 2), (126, 2), (143, 1), (145, 1), (183, 1), (184, 1), (246, 1), (251, 1), (269, 2), (312, 1), (313, 2), (314, 1), (315, 2), (316, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 1), (327, 3), (328, 1)], [(0, 1), (1, 1), (2, 2), (13, 1), (15, 1), (18, 2), (19, 2), (20, 1), (24, 4), (27, 1), (42, 1), (48, 1), (57, 1), (72, 1), (73, 1), (85, 1), (99, 1), (146, 1), (148, 1), (195, 1), (223, 2), (226, 2), (229, 1), (273, 1), (322, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1), (335, 1), (336, 3), (337, 1), (338, 1), (339, 1)], [(340, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=16, num_nnz=625)\n",
      "625\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 16 documents in 0 shards (stored under /var/folders/0f/qdj5sql91zd4g4xf9scp_jx80000gq/T/simservercc1217)\n",
      "<class 'gensim.similarities.docsim.Similarity'>\n"
     ]
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity(None,tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u.s.', 'soccer', 'should', 'start', 'answering', 'tough', 'questions', 'about', 'hope', 'solo']\n",
      "[(3, 1), (10, 1), (15, 1), (16, 1), (20, 1), (151, 1), (322, 1)]\n",
      "[(3, 0.4481199656584941), (10, 0.37110808075842094), (15, 0.12581898166127858), (16, 0.1003046472630994), (20, 0.12581898166127858), (151, 0.55666212113763147), (322, 0.55666212113763147)]\n"
     ]
    }
   ],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(clickbait_train[0]['targetTitle'])]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14804909,  0.00514506,  0.        ,  0.05579161,  0.09988002,\n",
       "        0.        ,  0.08448693,  0.00369259,  0.04070854,  0.06461674,\n",
       "        0.00388119,  0.04364515,  0.01308389,  0.07119576,  0.08902098,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045199849"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sims[query_doc_tf_idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
